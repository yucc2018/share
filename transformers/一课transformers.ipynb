{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>ä¸€è¯¾Transformers</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€èŠ‚ ä»‹ç»ä¸å‡†å¤‡å·¥ä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 transformersç®€ä»‹åŠæœ¬æ•™ç¨‹ç›®çš„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éšç€NLPé¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹çš„ç››è¡Œï¼Œä»BERTã€GPTåˆ°T5ã€ELECTRAã€Longformerã€MobileBERTç­‰è¶Šæ¥è¶Šå¤šçš„æ¨¡å‹æ¶Œç°äº†å‡ºæ¥ã€‚æ¯ä¸ªæ¨¡å‹çš„ä½œè€…å¯èƒ½ç”¨tfï¼Œä¹Ÿå¯èƒ½æ˜¯pytorchï¼Œè€Œä¸”å¾ˆå¯èƒ½ä¸åŒçš„ç¯å¢ƒç‰ˆæœ¬ï¼Œè¿™å¯¹äºå­¦æœ¯ç•Œã€å·¥ä¸šç•Œçš„å­¦ä¹ ã€å¤ç°ã€ä½¿ç”¨éƒ½å¸¦æ¥äº†ä¸€å®šå›°éš¾ã€‚å¹¸å¥½ï¼Œhuggingfaceå…¬å¸ä¸‹çš„transformersåº“å¸®æˆ‘ä»¬è§£å†³äº†è¿™ä¸ªéš¾é¢˜ã€‚\n",
    "\n",
    "huggingfaceæ˜¯ä¸€å®¶å…¬å¸ï¼Œtransformers[1][2][3]æ˜¯å…¶å…¬å¸å¼€å‘çš„å¼€æºåº“ï¼Œå·²æœ‰30k+ä¸ªstarã€‚è¯¥åº“è¡¨ç°ä¸ºï¼šç®€å•æ˜“ç”¨ï¼›åŒæ—¶æ”¯æŒtf2å’Œpytorchï¼›æ”¯æŒå¾ˆå¤šé¢„è®­ç»ƒæ¨¡å‹å¦‚BERT, GPT, ALBERT, T5, DialoGPT, ELECTRAç­‰ï¼Œè€Œä¸”éšæ—¶ç»´æŠ¤ï¼›æä¾›ç»Ÿä¸€çš„ã€æ ‡å‡†çš„Configã€Modelã€Tokenizerã€Traineræ¥å£ï¼ŒåŒæ—¶æä¾›æ ‡å‡†åŒ–çš„æ¨¡å‹æ–¹å¼ï¼Œæ–¹ä¾¿å¤ç°ã€æ‹“å±•å’Œå®éªŒã€‚\n",
    "\n",
    "ç›®å‰ä¸€äº›å¼€æºåº“ä½¿ç”¨huggingface/transformersä¸ºåŸºç¡€è¿›è¡Œå¼€å‘ï¼Œæä¾›ä¸€äº›åˆ†ç±»ã€ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚åŸºäºtransformersåº“å¼€å‘çš„ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆ[4]ï¼›åŸºäºtransformersåº“è¿›è¡Œçš„ç§‘ç ”ï¼Œå¦‚DialoGPT[5]ã€‚\n",
    "\n",
    "ä¸¤ç§æ–¹å¼å»ä½¿ç”¨ï¼Œç¬¬ä¸€ç§æ˜¯pipelineæ–¹å¼ï¼Œé«˜åº¦é›†æˆï¼Œç›´æ¥ä½¿ç”¨ï¼Œå¯ä»¥ç”¨äºæƒ…æ„Ÿåˆ†ç±»ã€NERæ ‡æ³¨ç­‰ï¼›ç¬¬äºŒç§æ˜¯æä¾›æ ‡æ³¨çš„æ¨¡å‹ï¼Œå»è®­ç»ƒï¼Œæ›´ç¬¦åˆæ ‡å‡†çš„ä½¿ç”¨ï¼Œæ›´èƒ½ä½¿ç”¨æˆ‘ä»¬æ—¥å¸¸å­¦ä¹ ã€å·¥ä½œä¸­çš„ä»»åŠ¡ã€‚\n",
    "\n",
    "æœ¬ç¯‡è®²è§£ç¬¬äºŒç§ä½¿ç”¨æ–¹å¼ï¼Œä»¥pytorchç‰ˆçš„æ¨¡å‹ä½¿ç”¨ä¸ºä¾‹ï¼Œå¸Œæœ›èƒ½é€šè¿‡ä¸€èŠ‚è¯¾çš„æ—¶é—´å¸®åŠ©å¤§å®¶å…¥é—¨transformersåº“çš„ä½¿ç”¨ã€‚\n",
    "\n",
    "æ³¨ï¼šæœ¬ä»£ç å‚è€ƒå’Œä½¿ç”¨äº†å¤§é‡çš„å®˜æ–¹æ ·ä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æœ¬æ–‡ç»“æ„\n",
    "\n",
    "#### ç¬¬ä¸€èŠ‚ ä»‹ç»ä¸å‡†å¤‡å·¥ä½œ\n",
    "\n",
    "æœ¬èŠ‚å¯¹huggingface/transformersæ˜¯ä»€ä¹ˆè¿›è¡Œäº†è¯´æ˜ï¼Œè¯´æ˜æœ¬æ–‡çš„ç›®çš„ï¼Œç« èŠ‚ç»“æ„ï¼Œå‡†å¤‡å·¥ä½œç­‰å†…å®¹ã€‚\n",
    "\n",
    "#### ç¬¬äºŒèŠ‚ å¿«é€Ÿå…¥é—¨\n",
    "\n",
    "ä¸€ä¸ªå¿«é€Ÿå…¥é—¨çš„ä¾‹å­ï¼Œtokenizerã€modelåŠ è½½ä¸ä¿å­˜çš„æ¦‚å¿µï¼Œè®²è§£å¦‚ä½•åˆ©ç”¨tokenizerå°†æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹çš„è¾“å…¥ï¼Œé€šè¿‡modelå¾—åˆ°logitsä¸lossç­‰ç»“æœã€‚é€šè¿‡è¿™ä¹ˆç®€å•åœ°å‡ æ­¥ï¼Œè¿™æ˜¯æ ‡å‡†çš„pytorchä¸€ä¸ªbatchçš„æŸå¤±è®¡ç®—è¿‡ç¨‹ã€‚åŸºæœ¬ä¸Šå®ç°äº†ä¸€ä¸ªå®Œæ•´çš„å‘¨æœŸè¿­ä»£ã€‚\n",
    "\n",
    "#### ç¬¬ä¸‰èŠ‚ æ¦‚å¿µä¸è¯´æ˜\n",
    "\n",
    "ä»‹ç»transformersçš„æœåŠ¡äººç¾¤ã€ç›®æ ‡ã€Configurationã€Tokenizerã€Modelã€AutoModelsã€Trainerè¿™å‡ ä¸ªç±»çš„è®¾è®¡ç›®çš„ï¼Œä»é«˜å±‚æ¬¡ä¸Šç†è§£è¯¥åº“çš„è®¾è®¡æ€è·¯ã€‚\n",
    "\n",
    "#### ç¬¬å››èŠ‚ GLUE/MRPCæ•°æ®é›†è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„ç¤ºä¾‹\n",
    "\n",
    "åŸºäºä¸Šé¢çš„çŸ¥è¯†ï¼Œè¿›è¡Œå®Œæ•´çš„å®æˆ˜ä»£ç æ¼”ç¤ºã€‚è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ–‡æœ¬åˆ†ç±»fine-tuneä¸çº¿ä¸Šéƒ¨ç½²çš„ä»£ç ï¼Œå…±åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š\n",
    "\n",
    "ç¬¬ä¸€éƒ¨åˆ†ï¼Œä½¿ç”¨transformersåº“çš„examplesä¸­çš„ä»£ç ï¼Œè¿›è¡Œåˆ†ç±»æ¨¡å‹çš„è®­ç»ƒã€‚è¿™æ˜¯ç®€å•çš„èƒ½è·‘çš„ä¾‹å­ã€‚\n",
    "\n",
    "ç¬¬äºŒéƒ¨åˆ†ï¼ŒåŠ è½½ç¬¬ä¸€éƒ¨åˆ†å¾—åˆ°çš„æ¨¡å‹ç»“æœï¼Œè¿›è¡Œé¢„æµ‹ç»“æœã€‚\n",
    "\n",
    "ç¬¬ä¸‰éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†çš„è¯¦ç»†ä»£ç ï¼Œè®²è§£å¦‚ä½•åŠ è½½æ•°æ®ã€æ¨¡å‹ã€tokenizerç­‰ï¼Œå»åˆå§‹åŒ–Trainerï¼Œå¹¶è®­ç»ƒã€‚\n",
    "\n",
    "#### ç¬¬äº”èŠ‚ GPT2è®­ç»ƒä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### ç¬¬å…­èŠ‚\n",
    "\n",
    "#### ç¬¬ä¸ƒèŠ‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 å‡†å¤‡å·¥ä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†\n",
    "\n",
    "ä¸‹é¢çš„è®²è§£ä½¿ç”¨åˆ°bert-base-uncasedã€gpt2ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼›åŒæ—¶ä¼šä½¿ç”¨åˆ°glueä¸‹çš„MRPCæ•°æ®é›†ã€wikitext-2-rawæ•°æ®é›†ã€‚å¯ä»¥ç›´æ¥é€šè¿‡ä¸‹é¢çš„ç™¾åº¦äº‘é“¾æ¥å’Œå¯†ç å»ä¸‹è½½ã€‚å¯ä»¥å°†ä¸‹è½½å¥½çš„æ•°æ®æ”¾åœ¨ä¸æœ¬notebookåŒçº§ç›®å½•ä¸‹ï¼Œæ–¹ä¾¿ä½¿ç”¨ã€‚\n",
    "\n",
    "é“¾æ¥ï¼š\n",
    "\n",
    "å¯†ç ï¼š\n",
    "\n",
    "\n",
    "#### 1.3.2 å…‹éš†transformersåº“è‡³æœ¬åœ°\n",
    "\n",
    "å› ä¸ºéœ€è¦ç”¨åˆ°transformersåº“ä¸­çš„æ ·ä¾‹ï¼Œæ‰€ä»¥éœ€è¦å°†ç›¸åº”çš„åº“å…‹éš†ä¸‹æ¥ï¼Œå¯ä»¥å°†åº“æ”¾åœ¨æœ¬notebookåŒçº§ç›®å½•ä¸‹ã€‚åœ¨shellä¸­æ‰§è¡Œä¸‹é¢ä»£ç è¿›è¡Œå…‹éš†ï¼š\n",
    "\n",
    "```\n",
    "git clone git@github.com:huggingface/transformers.git\n",
    "```\n",
    "\n",
    "\n",
    "#### 1.3.3 å®‰è£…ç›¸åº”çš„ç¯å¢ƒ\n",
    "\n",
    "æœ¬æ•™ç¨‹ä»¥pytorchä¸ºåŸºç¡€ï¼Œéœ€è¦å®‰è£…pytorchï¼Œå»ºè®®pytorch>=1.4.0ã€‚\n",
    "\n",
    "transformerså‡çº§è‡³æœ€æ–°ç‰ˆæœ¬ï¼Œå‡çº§æ–¹å¼ï¼š\n",
    "\n",
    "```\n",
    "pip install --upgrade transformers\n",
    "```\n",
    "\n",
    "å¯èƒ½ä¼šéœ€è¦å…¶ä»–ç¯å¢ƒï¼Œå¦‚pandasã€xlrdã€sklearnç­‰ï¼Œæç¤ºç¼ºå°‘ä»€ä¹ˆåŒ…ï¼Œç›´æ¥å®‰è£…å³å¯ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äºŒèŠ‚ å¿«é€Ÿå…¥é—¨\n",
    "\n",
    "é€šè¿‡ç®€å•çš„ä¾‹å­ï¼Œæ„Ÿå—ä»æ–‡æœ¬åˆ°æ¨¡å‹è¾“å‡ºçš„å¿«æ·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.4.0+cu100\n",
      "transformers: 3.0.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f'torch: {torch.__version__}')\n",
    "print(f'transformers: {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰€æœ‰å¤§å†™çš„å†…å®¹ï¼Œéœ€è¦æ”¹ä¸ºè‡ªå·±çš„å®é™…è·¯å¾„\n",
    "BERT_MODEL_NAME_OR_PATH = 'transformers_data_and_model/bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers_data_and_model/bert-base-uncased\n",
      "â”œâ”€â”€ config.json\n",
      "â”œâ”€â”€ modelcard.json\n",
      "â”œâ”€â”€ pytorch_model.bin\n",
      "â””â”€â”€ vocab.txt\n",
      "\n",
      "0 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "# å±•ç¤ºæ–‡ä»¶å¤¹ä¸­çš„å†…å®¹\n",
    "!tree {BERT_MODEL_NAME_OR_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at transformers_data_and_model/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at transformers_data_and_model/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–modelå’Œtokenizer\n",
    "#ã€€æ‰€æœ‰modelå’Œtokenizerçš„åˆå§‹åŒ–éƒ½ä½¿ç”¨from_pretrainedçš„æ–¹æ³•ï¼Œä¿å­˜éƒ½ä½¿ç”¨save_pretrainedçš„æ–¹æ³•\n",
    "# å¯¹from_pretrained:ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æ–‡ä»¶å¤¹çš„è·¯å¾„/æ–‡ä»¶çš„è·¯å¾„/æ¨¡å‹çš„short nameç­‰å‡ ç§æ–¹æ³•ï¼Œè¿™é‡Œæ¨èä½¿ç”¨æ–‡ä»¶å¤¹çš„æ–¹å¼\n",
    "# modelåˆå§‹åŒ–é»˜è®¤æ˜¯evalæ¨¡å¼ï¼Œè¿™é‡ŒåŠ è½½çš„æ˜¯BERTçš„tokenizerå’Œåˆ†ç±»æ¨¡å‹model\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME_OR_PATH)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BERT_MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# tokenizerçš„ä½œç”¨ï¼šå¯¹äºç»™å®šçš„æ–‡æœ¬ï¼Œç»è¿‡tokenizerå¤„ç†æˆmodelå¯ä»¥æ¥å—çš„æ ¼å¼\n",
    "# tokenizeræœ€é‡è¦çš„ç”¨æ³•æ˜¯__call__ï¼Œè¿™ä¸ªæ–¹æ³•å¯ä»¥å°†æ–‡æœ¬è¾“å‡ºä¸ºæ¨¡å‹çš„è¦çš„æ ¼å¼\n",
    "# tokenizerè¿˜æœ‰å…¶ä»–æ–¹æ³•ï¼Œencode/decodeï¼Œé¡¾åæ€ä¹‰ï¼Œå°±æ˜¯å°†æ–‡æœ¬è½¬æ¢æˆinput_idsåŠå°†input_idsè½¬æ¢ä¸ºæ–‡æœ¬\n",
    "# encode/decodeä¸__call__å…¶å®æ— æœ¬è´¨åŒºåˆ«ï¼Œåªæ˜¯__call__ä¸ºäº†æä¾›ç»Ÿä¸€çš„å¤„ç†æ¥å£\n",
    "inputs = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
    "# input_idsæ˜¯æ–‡æœ¬æ¯ä¸ªè¯çš„indexï¼›token_type_idæ˜¯è¡¨ç¤ºæ–‡æœ¬æ˜¯ç¬¬ä¸€å¥/ç¬¬äºŒå¥ï¼› attention_maskæ˜¯å¤„ç†maskç”¨çš„\n",
    "# è¿™é‡Œå¤„ç†çš„æ˜¯ä¸€æ¡æ ·æœ¬ä¸”åªæœ‰ä¸€å¥è¯çš„ä¾‹å­ï¼Œå¦‚æœæ˜¯å¤šæ¡å•å¥æ ·æœ¬ï¼Œè¾“å…¥ä¸ºä¸€ä¸ªæ–‡æœ¬listå³å¯ã€‚\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 102, 2204, 2851, 102], 'token_type_ids': [0, 0, 0, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# ä¸€æ¡æ ·æœ¬ä¸”æœ‰ä¸¤å¥è¯çš„ä¾‹å­ï¼Œåˆ†åˆ«ä½œä¸ºç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå‚æ•°è¾“å…¥\n",
    "# å¦‚æœæ˜¯å¤šä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯ä¸¤å¥è¯ï¼Œåˆ™ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ç¬¬ä¸€å¥è¯çš„æ–‡æœ¬listï¼Œç¬¬äºŒä¸ªå‚æ•°ä¸ºç¬¬äºŒå¥è¯çš„æ–‡æœ¬list\n",
    "inputs2 = tokenizer('hello', 'good morning')\n",
    "print(inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# æ›´å¤šæ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦çš„modelè¾“å…¥æ˜¯æˆbatchæ ¼å¼çš„\n",
    "# ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯æ–‡æœ¬listï¼Œpaddingè®¾ç½®ä¸ºTrueï¼Œtruncationè®¾ç½®ä¸ºTrueå¯ä»¥è¿›è¡Œpaddingå’Œtruncation\n",
    "# return_tensorså†™æ˜äº†è¿”å›çš„æ ¼å¼ï¼Œæ˜¯ä¸€ä¸ªpytorchçš„tensor\n",
    "pt_batch = tokenizer(\n",
    "     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "     padding=True,\n",
    "     truncation=True,\n",
    "     return_tensors=\"pt\"\n",
    ")\n",
    "for key, value in pt_batch.items():\n",
    "     print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:tensor([[0.2274, 0.1681],\n",
      "        [0.1150, 0.2867]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "loss: 0.7529614567756653\n",
      "logits: tensor([[0.2274, 0.1681],\n",
      "        [0.1150, 0.2867]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# å¯¹äºtokenizerå¤„ç†åçš„æ–‡æœ¬ï¼Œç›®æ ‡æ˜¯é€å…¥modelï¼Œç”¨äºåˆ†ç±»ã€é¢„æµ‹ç­‰ä»»åŠ¡\n",
    "# ä¸Šé¢çš„pt_batchå°±æ˜¯ä¸€ä¸ªbatchï¼Œå¢åŠ **ç›´æ¥è¾“å…¥æ¨¡å‹\n",
    "# æ ¹æ®transformersåº“çš„è§„åˆ™ï¼Œ æ‰€æœ‰modelçš„è¾“å‡ºéƒ½æ˜¯å…ƒç»„\n",
    "# å¦‚æœåªæœ‰æ¯ä¸ªbatchçš„è¾“å…¥ï¼Œå…ƒç»„è¾“å‡ºçš„ç¬¬ä¸€ä¸ªæ˜¯logits\n",
    "# å¦‚æœåŒæ—¶ä¼ å…¥äº†labelsçš„å‚æ•°ï¼Œåˆ™å…ƒç»„è¾“å‡ºçš„ç¬¬ä¸€ä¸ªæ˜¯lossï¼Œç¬¬äºŒä¸ªæ˜¯logits\n",
    "# ä¸€èˆ¬å›å½’ä»»åŠ¡lossç”¨çš„Mean-Square lossï¼Œåˆ†ç±»ä»»åŠ¡åˆ™æ˜¯Cross-Entroy\n",
    "pt_outputs = model(**pt_batch)\n",
    "print(f'logits:{pt_outputs[0]}\\n')\n",
    "# ä¼ å…¥labelså‚æ•°çš„æƒ…å†µ\n",
    "pt_outputs = model(**pt_batch, labels=torch.LongTensor([1, 0]))\n",
    "print(f'loss: {pt_outputs[0]}\\nlogits: {pt_outputs[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢å°±æ˜¯pytorchç‰ˆtransformersçš„åŸºæœ¬è¾“å…¥é€»è¾‘ï¼š\n",
    "\n",
    "transformersä¸­çš„æ‰€æœ‰modeléƒ½æ˜¯pytorchçš„æ ‡å‡†æ¨¡å‹ç±»torch.nn.Moduleã€‚æ–‡æœ¬å¯ä»¥é€šè¿‡tokenizerè°ƒç”¨è½¬æ¢ä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œæ¨¡å‹è¾“å…¥è¿™äº›ä¿¡æ¯ï¼Œå¾—åˆ°logitsï¼Œè®¡ç®—lossï¼Œè¿›è¡Œè¯¯å·®å›ä¼ backwardï¼Œè¿›è¡Œè¿­ä»£ï¼Œå°±å®Œæˆäº†è®­ç»ƒ/fine-tuneã€‚æ¨¡å‹è¾“å…¥çš„æ—¶å€™ï¼Œå¦‚æœä¼ å…¥labelsçš„å‚æ•°ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å¾—åˆ°ç›¸åº”çš„lossï¼Œä¸€æ ·backwardï¼Œå¤šæ¬¡è¿­ä»£ï¼Œå®Œæˆè®­ç»ƒã€‚\n",
    "\n",
    "é™¤äº†æ ‡å‡†çš„pytorchæ–¹å¼ï¼Œtransformersè¿˜å°è£…äº†Trainerç±»æ¥å¸®åŠ©æˆ‘ä»¬ç®€åŒ–pytorchä»£ç ï¼Œåé¢å†è®²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæˆè®­ç»ƒ/å¾®è°ƒåï¼Œå¯ä»¥å°†tokenizerå’Œmodelä¿å­˜è‡³ç›¸åŒçš„æ–‡ä»¶å¤¹ã€‚\n",
    "# åœ¨transformersæ¡†æ¶é‡Œï¼Œä¸€ä¸ªå¾ˆå¥½çš„ä¹ æƒ¯ï¼Œå°†modelã€tokenizerå‚æ•°ã€è®­ç»ƒå‚æ•°ç­‰æ‰€æœ‰å­˜æ”¾åœ¨åŒä¸€æ–‡ä»¶å¤¹ã€‚\n",
    "# è¿™é‡Œçš„model/tokenizer/configçš„åˆå§‹åŒ–ä½¿ç”¨from_pretrainedï¼Œä¿å­˜ä½¿ç”¨save_pretrained\n",
    "# save_pretrainedä¼ å…¥å…·ä½“çš„æ–‡ä»¶å¤¹åå³å¯\n",
    "SAVE_DIRECTORY = 'transformers_data_and_model/bert_save_example'\n",
    "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
    "model.save_pretrained(SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers_data_and_model/bert_save_example\n",
      "â”œâ”€â”€ config.json\n",
      "â”œâ”€â”€ pytorch_model.bin\n",
      "â”œâ”€â”€ special_tokens_map.json\n",
      "â”œâ”€â”€ tokenizer_config.json\n",
      "â””â”€â”€ vocab.txt\n",
      "\n",
      "0 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜çš„ç»“æœå±•ç¤º\n",
    "!tree {SAVE_DIRECTORY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ€»ç»“ï¼š\n",
    "\n",
    "1. åœ¨transformersæ¡†æ¶é‡Œï¼Œæä¾›äº†model/tokenizer/configé€šç”¨åŒ–çš„åŠ è½½å’Œä¿å­˜ï¼Œä¹Ÿå°±æ˜¯from_pretraiend/save_pretrainedï¼›\n",
    "2. tokenizerçš„ä½œç”¨åœ¨äºï¼Œé€šè¿‡\\_\\_call__å°†æ–‡æœ¬è¿›è¡Œè½¬æ¢æˆæ¨¡å‹æ¥å—çš„æ ¼å¼ï¼Œmodelçš„è¾“å‡ºéƒ½æ˜¯å…ƒç»„ï¼Œä¾æ®è¿™äº›å…ƒç»„çš„å†…å®¹è¿›è¡Œè®¡ç®—lossï¼›\n",
    "3. tokenizeråŒ…è£…äº†Byte-Pair Encodingã€WordPieceã€SentencePieceç­‰ä¸åŒçš„æ–¹å¼ï¼›\n",
    "4. modelæ˜¯åŒ…è£…äº†BERTã€GPT2ã€ALBERTç­‰ä¸åŒçš„æ¨¡å‹ï¼Œå¹¶ä¸”æä¾›æ ‡å‡†åŒ–çš„ç±»ã€‚ç­‰ä¸‹ç»†è¯´ã€‚\n",
    "5. å¯¹äºæˆ‘ä»¬æ¥è®²ï¼Œå»å¤ç°ã€å®éªŒã€ç ”ç©¶æ›´å®¹æ˜“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸‰èŠ‚ æ¦‚å¿µä¸è¯´æ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬èŠ‚è®²è§£transformersæ¶‰åŠçš„æœåŠ¡ç¾¤ä½“ã€ç›®æ ‡ã€ä¸»è¦æ¦‚å¿µã€AutoModelsã€Trainerç±»ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 transformersçš„æœåŠ¡äººç¾¤ï¼š\n",
    "- å¯»æ‰¾ç”¨äºä½¿ç”¨ã€å­¦ä¹ ã€æ‰©å±•å¤§å‹transformersæ¨¡å‹çš„NLPç ”ç©¶è€…å’Œæ•™è‚²è€…\n",
    "- å¸Œæœ›å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–åœ¨ç”Ÿäº§ç”Ÿæä¾›æœåŠ¡çš„å®è·µè€…\n",
    "- åªæƒ³ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å°†å…¶ç”¨äºè§£å†³NLPä»»åŠ¡çš„å·¥ç¨‹å¸ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 transformersçš„ä¸¤ä¸ªå¼ºç›®æ ‡ï¼š\n",
    "- å°½å¯èƒ½çš„ç®€å•ï¼Œå¿«é€Ÿçš„ä½¿ç”¨\n",
    "- ä¸ºæœ€æ–°æ¨¡å‹æä¾›ä¸åŸå§‹æ¨¡å‹å°½å¯èƒ½æ¥è¿‘çš„æ€§èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 transformersçš„å°ç›®æ ‡:\n",
    "- å°½å¯èƒ½çš„å†…éƒ¨æ¥å£ä¸€è‡´\n",
    "- çº³å…¥ä¸»è§‚é€‰æ‹©çš„æœ‰å‰é€”çš„å·¥å…·ï¼Œä»¥å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒ/ç ”ç©¶\n",
    "- åœ¨PyTorchå’ŒTensorFlow 2.0ä¹‹é—´è½»æ¾åˆ‡æ¢ï¼Œä»è€Œå…è®¸ä½¿ç”¨ä¸€ç§æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œè€Œä½¿ç”¨å¦ä¸€ç§æ¡†æ¶è¿›è¡Œæ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 ä¸»è¦æ¦‚å¿µï¼š\n",
    "\n",
    "ä¸»è¦çš„ç±»æœ‰Modelã€Configurationã€Tokenizerè¿™ä¸‰ä¸ªç±»ï¼Œä¸‹é¢åˆ†åˆ«ä»‹ç»ã€‚\n",
    "\n",
    "Modelç±»ï¼Œæ¯”å¦‚BertModelï¼Œå‡ä»pytorch models(torch.nn.Module)æˆ–è€…keras models(tf.keras.Model)ç»§æ‰¿è€Œæ¥ï¼Œç”¨äºå¤„ç†é¢„è®­ç»ƒæƒé‡ã€‚\n",
    "\n",
    "Configurationç±»ï¼Œæ¯”å¦‚BertConfigï¼Œé‡Œé¢ä¿å­˜ç€å»ºç«‹æ¨¡å‹æ‰€éœ€è¦çš„æ‰€æœ‰å‚æ•°ã€‚å¹¶ä¸æ˜¯æ€»æ˜¯æˆ‘ä»¬æ‰‹åŠ¨å»åˆå§‹åŒ–è¿™ä¸ªç±»ï¼Œå°¤å…¶æ˜¯å½“ä½ ä½¿ç”¨æ²¡æœ‰åšä»»ä½•æ›´æ”¹çš„é¢„è®­ç»ƒæ—¶ï¼Œmodelä¼šè‡ªåŠ¨å¤„ç†å¥½è¿™ä¸ªç±»ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœè‡ªå·±é‡æ–°é¢„è®­ç»ƒçš„æ¨¡å‹ä¸”æ¶æ„ä¸ä¸€è‡´æ—¶ï¼Œæ˜¯è®¸æˆ‘ä»¬æˆ‘ä»¬å»åˆå§‹åŒ–è¿™ä¸ªç±»çš„ã€‚\n",
    "\n",
    "Tokenizerç±»ï¼Œæ¯”å¦‚BERTTokenizerï¼Œä¸ºæ¯ä¸ªæ¨¡å‹ä¿å­˜è¯å…¸ï¼Œå¹¶å°†æ–‡æœ¬è¿›è¡Œç¼–ç /è§£ç æˆæ¨¡å‹éœ€è¦çš„æ ¼å¼â€”â€”tokenåµŒå…¥çš„ç´¢å¼•ã€‚\n",
    "\n",
    "ä¸Šé¢çš„ç±»ï¼Œéƒ½æœ‰ä¸‹é¢ä¸¤ä¸ªæ–¹æ³•å»å®ä¾‹åŒ–ç±»å’Œä¿å­˜è‡³æœ¬åœ°ï¼š\n",
    "\n",
    "`from_pretrained()` å…è®¸æˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨short_nameï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æœ¬åœ°çš„æ¨¡å‹ï¼Œä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°model_name_or_pathä¼ å…¥ï¼Œå¯ä»¥æ˜¯æ–‡ä»¶å¤¹ã€æ–‡ä»¶ã€short_nameç­‰ã€‚å…¶ä¸­æ–‡ä»¶å¤¹çš„è¯ï¼Œä¼šé»˜è®¤å¯»æ‰¾æ–‡ä»¶å¤¹ä¸­çš„pytorh_model.binã€‚\n",
    "\n",
    "`save_pretrained()` å…è®¸æˆ‘ä»¬å°†æ¨¡å‹ä¿å­˜è‡³æœ¬åœ°ï¼Œä¿å­˜çš„å‚æ•°æ˜¯å¯ä»¥æ˜¯æ–‡ä»¶å¤¹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 AutoModels\n",
    "\n",
    "ä»¥BERTä¸ºä¾‹ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªConfigï¼ˆBertConfigï¼‰ï¼›æœ‰1-2ä¸ªtokenizerï¼Œåˆ†åˆ«æ˜¯åŸºäºrustçš„å¿«é€Ÿtokenizerï¼ˆBertTokenizerFastï¼‰ï¼Œä¸€ä¸ªæ˜¯åŸºäºpythonåŸç‰ˆçš„tokenizerï¼ˆBertTokenizerï¼‰ï¼Œéƒ¨åˆ†æ²¡æœ‰æä¾›rustçš„å¿«é€Ÿtokenizerï¼›æœ‰å¤šä¸ªçš†æœ‰ä¸åŒheadçš„Modelï¼Œæ¯”å¦‚æœ€åŸå§‹çš„æ¨¡å‹ï¼Œä¸å«headçš„BertModelã€é¢„è®­ç»ƒMLMå’ŒNSPçš„BertForPreTrainingã€MLM headçš„BertForMaskedLMï¼ŒNSPçš„BertForNextSentencePredictionï¼Œç”¨äºå¥å­åˆ†ç±»çš„BertForSequenceClassificationï¼Œç”¨äºå¤šé€‰çš„BertForMultipleChoiceï¼Œç”¨å•è¯åˆ†ç±»çš„BertForTokenClassificationï¼Œç”¨äºé—®ç­”çš„BertForQuestionAnsweringã€‚\n",
    "\n",
    "ä¸åŒçš„æ¨¡å‹ï¼Œä¼šç¨æœ‰ä¸åŒã€‚ä½†æ˜¯configç±»éƒ½ç»§æ‰¿è‡ªPretrainedConfigï¼›tokenizeréƒ½ç»§æ‰¿è‡ªPreTrainedTokenizeræˆ–PreTrainedTokenizerFastï¼›modeléƒ½ç»§æ‰¿è‡ªPreTrainedModelã€‚\n",
    "\n",
    "ä¸ºäº†ä½¿ç”¨æ–¹ä¾¿ï¼ŒAutoConfigã€AutoTokenizerã€AutoModelã€AutoModelForPreTrainingã€AutoModelWithLMHeadã€AutoModelForSequenceClassificationã€AutoModelForQuestionAnsweringã€AutoModelForTokenClassificationç­‰å¯ä»¥ç”¨äºè‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Trainerç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainerç±»æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„æ ‡å‡†è®­ç»ƒçš„APIï¼Œç›®å‰æ”¯æŒè¯­è¨€æ¨¡å‹ã€æ–‡æœ¬åˆ†ç±»ã€å•è¯åˆ†ç±»ï¼ˆNERï¼‰ç­‰ä»»åŠ¡ã€‚å¯¹äºå‰é¢çš„configã€tokenizerã€modelï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºï¼Œå¸®åŠ©æˆ‘ä»¬ç®€åŒ–çš„æ˜¯å†™æ¨¡å‹çš„è¿™ä¸€æ­¥ï¼Œæ­£å¸¸ç”Ÿæˆdatasetã€dataloaderï¼Œç„¶åå†æ¯ä¸ªepochã€batchè¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°æœ€ç»ˆçš„ç»“æœã€‚æ­£å¸¸å†™çš„è¯ï¼ŒTrainerç±»å¯ä»¥ä¸ç”¨ï¼ŒTrainerå…¶å®æ˜¯ç®€åŒ–çš„æ˜¯æˆ‘ä»¬è®­ç»ƒçš„è¿™ä¸€æ­¥ã€‚\n",
    "\n",
    "å¯¹äºé€šå¸¸çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå†™æ³•å¤§è‡´æ˜¯è¿™æ ·çš„ï¼ˆä¸‹é¢æ˜¯ä¼ªä»£ç ï¼‰:\n",
    "\n",
    "```\n",
    "# åŠ è½½æ•°æ®\n",
    "train_data, test_data = get_data()\n",
    "# è½¬æ¢æˆfeaturesï¼Œè·å¾—dataset\n",
    "train_dataset = MyDataset(train_data, args)\n",
    "test_dataset = MyDataset(test_data, args)\n",
    "\n",
    "# è½¬æ¢æˆdataloaderï¼Œç”¨äºç”Ÿæˆbatch\n",
    "train_sampler, test_sampler = \n",
    "train_dataloader = Dataloader(train_dataset, sampler=train_sampler, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = Dataloader(train_dataset, sampler=test_sampler, batch_size=batch_size, collate_fn=collate_fn)\n",
    "# åˆå§‹åŒ–tensorboard\n",
    "tb_writer = SummaryWriter(log_dir=None)\n",
    "# åŠ è½½optimizer\n",
    "optimizer = \n",
    "mode.to(GPU)\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # åˆ‡æ¢ä¸ºtrain\n",
    "        model.train()\n",
    "        # ä¼ å…¥ï¼§ï¼°ï¼µ\n",
    "        batch.to(GPU)\n",
    "        # è®¡ç®—æ¯ä¸€æ­¥çš„lossï¼Œç„¶åå›ä¼ \n",
    "        tr_loss = train_step(model, inputs, optimizer)\n",
    "        tr_loss.backword()\n",
    "        model.zero_grad()\n",
    "for batch in test_dataloder:\n",
    "    ***\n",
    "model.save_pretrained(OUTPUT_PATH)\n",
    "```\n",
    "\n",
    "ä½¿ç”¨äº†Trainerä¹‹åï¼Œå°±èƒ½å¤§å¤§çš„ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼š\n",
    "\n",
    "```\n",
    "# åŠ è½½æ•°æ®\n",
    "train_data, test_data = get_data()\n",
    "# è½¬æ¢æˆfeaturesï¼Œè·å¾—dataset\n",
    "train_dataset = MyDataset(train_data, args)\n",
    "test_dataset = MyDataset(test_data, args)\n",
    "# è¯»å…¥train_args\n",
    "train_args = **\n",
    "\n",
    "\n",
    "#åˆå§‹åŒ–æœ¬Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=build_compute_metrics_fn(data_args.task_name),\n",
    ")\n",
    "# è®­ç»ƒ\n",
    "if training_args.do_train:\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    # trainerä¿å­˜æ¨¡å‹ï¼Œé‡Œé¢è°ƒç”¨çš„è¿˜æ˜¯save_pretrainedçš„æ–¹æ³•\n",
    "    trainer.save_model()\n",
    "    # ä¿å­˜tokenizerè‡³åŒä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œæ–¹ä¾¿ä½¿ç”¨\n",
    "    if trainer.is_world_master():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)\n",
    "```\n",
    "\n",
    "ä»ä¸Šé¢çš„ä»£ç ï¼Œå¯ä»¥çœ‹å‡ºï¼ŒTrainerç®€åŒ–çš„è¿‡ç¨‹å°±æ˜¯è®­ç»ƒçš„è¿‡ç¨‹ã€‚å¯¹äºä¼ ç»Ÿçš„è®­ç»ƒä¸­å¸¸è§çš„è¿‡ç¨‹è¿›è¡Œäº†å°è£…ï¼Œæˆ‘ä»¬åªè¦å»åˆå§‹åŒ–Trainerè¿™ä¸ªç±»ï¼Œå°±å¾ˆæ–¹ä¾¿çš„å»è®­ç»ƒï¼Œä¿å­˜æ¨¡å‹ä½¿ç”¨æ˜¯save_modelï¼Œå†…éƒ¨è°ƒç”¨çš„è¿˜æ˜¯ä¸Šæ–‡æåˆ°çš„save_pretrainedçš„æ–¹æ³•ã€‚ä¸‹é¢çš„ç« èŠ‚ä¹Ÿä¼šç”¨åˆ°Trainerç±»è¿›è¡Œè®­ç»ƒï¼Œæ›´è¯¦ç»†çš„å¤§å®¶å¯ä»¥çœ‹Trainerçš„APIè¯´æ˜[8]åŠæºä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å››èŠ‚ GLUE/MRPCæ•°æ®é›†è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 æœ¬èŠ‚è¯´æ˜\n",
    "\n",
    "æœ¬èŠ‚è¯¾ä»¥ç®€å•çš„ä¾‹å­ï¼Œè¯´æ˜æ–‡æœ¬åˆ†ç±»çš„fine-tuneè¿‡ç¨‹ï¼Œçº¿ä¸Šéƒ¨ç½²ä»£ç ï¼Œè¯¦ç»†çš„fine-tuneè¿‡ç¨‹ã€‚å…¶ä¸­4.2èŠ‚è®­ç»ƒæ¨¡å‹ï¼Œä»¥transformersåº“é‡Œçš„è®­ç»ƒä»£ç ä¸ºä¾‹ï¼Œç®€å•çš„è·‘é€šäº†æ–‡æœ¬fine-tuneä»»åŠ¡ï¼›4.3èŠ‚çº¿ä¸Šé¢„æµ‹ä»¥4.2èŠ‚è®­ç»ƒå¥½çš„æ¨¡å‹ä¸ºä¾‹ï¼Œè¿›è¡Œçº¿ä¸Šé¢„æµ‹ï¼Œ4.4èŠ‚æ¨¡å‹è®­ç»ƒçš„è¯¦ç»†è¿‡ç¨‹æ›´åŠ è¯¦ç»†çš„è¯´æ˜äº†4.2èŠ‚çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå°†ä»£ç æ‹†è§£ï¼Œæ–¹ä¾¿å¤§å®¶å»é€‚é…æ–°çš„åˆ†ç±»ï¼Œ4.5èŠ‚transformersä½¿ç”¨æ€»ç»“ï¼Œæ€»ç»“äº†transformersåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­åŸºæœ¬ç»éªŒã€‚\n",
    "\n",
    "æœ¬èŠ‚æ¶‰åŠåˆ°çš„æ¨¡å‹æ—¶bert-base-uncasedï¼Œæ¶‰åŠåˆ°çš„æ•°æ®é›†æ˜¯glueæ•°æ®é›†ä¸‹çš„MRPCæ•°æ®é›†ã€‚\n",
    "\n",
    "glueæ•°æ®é›†ï¼Œå…±æœ‰9ä¸ªä»»åŠ¡ï¼Œå…¶ä¸­STS-Bæ˜¯ä¸€ä¸ªå›å½’ä»»åŠ¡ï¼ŒMNLIæ˜¯ä¸‰åˆ†ç±»ä»»åŠ¡ï¼Œå‰©ä½™7ç±»å‡æ˜¯äºŒåˆ†ç±»ä»»åŠ¡ã€‚æ›´è¯¦ç»†çš„glueæ•°æ®é›†çš„ä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒ[6]ã€‚ä¹ä¸ªä»»åŠ¡ä¹‹ä¸€çš„MRPC(The Microsoft Research Paraphrase Corpusï¼Œå¾®è½¯ç ”ç©¶é™¢é‡Šä¹‰è¯­æ–™åº“)ï¼Œç›¸ä¼¼æ€§å’Œé‡Šä¹‰ä»»åŠ¡ï¼Œæ˜¯ä»åœ¨çº¿æ–°é—»æºä¸­è‡ªåŠ¨æŠ½å–å¥å­å¯¹è¯­æ–™åº“ï¼Œå¹¶äººå·¥æ³¨é‡Šå¥å­å¯¹ä¸­çš„å¥å­æ˜¯å¦åœ¨è¯­ä¹‰ä¸Šç­‰æ•ˆã€‚ç±»åˆ«å¹¶ä¸å¹³è¡¡ï¼Œå…¶ä¸­68%çš„æ­£æ ·æœ¬ï¼Œæ‰€ä»¥éµå¾ªå¸¸è§„çš„åšæ³•ï¼ŒæŠ¥å‘Šå‡†ç¡®ç‡ï¼ˆaccuracyï¼‰å’ŒF1å€¼ã€‚æ ·æœ¬ä¸ªæ•°ï¼šè®­ç»ƒé›†3, 668ä¸ªï¼Œå¼€å‘é›†408ä¸ªï¼Œæµ‹è¯•é›†1, 725ä¸ªã€‚ä»»åŠ¡ï¼šæ˜¯å¦é‡Šä¹‰äºŒåˆ†ç±»ï¼Œæ˜¯é‡Šä¹‰ï¼Œä¸æ˜¯é‡Šä¹‰ä¸¤ç±»ã€‚è¯„ä»·å‡†åˆ™ï¼šå‡†ç¡®ç‡ï¼ˆaccuracyï¼‰å’ŒF1å€¼ã€‚æ ‡ç­¾ä¸º1ï¼ˆæ­£æ ·æœ¬ï¼Œäº’ä¸ºé‡Šä¹‰ï¼‰çš„æ ·ä¾‹ï¼ˆæ¯ä¸ªæ ·ä¾‹æ˜¯ä¸¤å¥è¯ï¼Œä¸­é—´ç”¨tabéš”å¼€ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬å°èŠ‚æ˜¯ä½¿ç”¨transformersåº“ç»™å®šçš„å®˜æ–¹åˆ†ç±»æ ·ä¾‹ï¼Œè¿›è¡Œæ–‡æœ¬è®­ç»ƒï¼ˆfine-tuneï¼‰çš„ä¾‹å­ï¼Œéå¸¸ç®€å•ï¼Œå°±èƒ½ç›´æ¥è·‘é€šäº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_glue.pyæ–‡ä»¶ä½ç½®ï¼Œåœ¨transformersåº“çš„ä½ç½®ï¼štransformers/examples/text-classification/run_glue.py\n",
    "RUN_GLUE_PY = 'transformers/examples/text-classification/run_glue.py'\n",
    "# ä¸‹è½½å¥½çš„é¢„è®­ç»ƒæ¨¡å‹ä½ç½®\n",
    "BERT_MODEL_NAME_OR_PATH = 'transformers_data_and_model/bert-base-uncased'\n",
    "# ä¸‹è½½å¥½çš„glueæ•°æ®é›†ä¸­çš„MRPCæ•°æ®é›†çš„ä½ç½®\n",
    "MRPC_DATA_DIR = 'transformers_data_and_model/MRPC'\n",
    "# finetuneå¥½çš„modelã€tokenizerç­‰å„ç§å‚æ•°å­˜æ”¾çš„ä½ç½®\n",
    "FINETUNED_MRPC = 'transformers_data_and_model/finetuned-mrpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "07/27/2020 08:09:54 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/27/2020 08:09:54 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "07/27/2020 08:09:54 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='transformers_data_and_model/finetuned-mrpc', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul27_08-09-54_b268e5129060', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n",
      "07/27/2020 08:09:54 - INFO - transformers.configuration_utils -   loading configuration file transformers_data_and_model/bert-base-uncased/config.json\n",
      "07/27/2020 08:09:54 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2020 08:09:54 - INFO - transformers.configuration_utils -   loading configuration file transformers_data_and_model/bert-base-uncased/config.json\n",
      "07/27/2020 08:09:54 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2020 08:09:54 - INFO - transformers.tokenization_utils_base -   Model name 'transformers_data_and_model/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'transformers_data_and_model/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/27/2020 08:09:54 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/bert-base-uncased/added_tokens.json. We won't load it.\n",
      "07/27/2020 08:09:54 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/bert-base-uncased/special_tokens_map.json. We won't load it.\n",
      "07/27/2020 08:09:54 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/bert-base-uncased/tokenizer_config.json. We won't load it.\n",
      "07/27/2020 08:09:54 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/bert-base-uncased/tokenizer.json. We won't load it.\n",
      "07/27/2020 08:09:54 - INFO - transformers.tokenization_utils_base -   loading file transformers_data_and_model/bert-base-uncased/vocab.txt\n",
      "07/27/2020 08:09:54 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 08:09:54 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 08:09:54 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 08:09:54 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 08:09:54 - INFO - transformers.modeling_utils -   loading weights file transformers_data_and_model/bert-base-uncased/pytorch_model.bin\n",
      "07/27/2020 08:09:57 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at transformers_data_and_model/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "07/27/2020 08:09:57 - WARNING - transformers.modeling_utils -   Some weights of BertForSequenceClassification were not initialized from the model checkpoint at transformers_data_and_model/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "07/27/2020 08:09:57 - INFO - filelock -   Lock 139828797735264 acquired on transformers_data_and_model/MRPC/cached_train_BertTokenizer_128_mrpc.lock\n",
      "07/27/2020 08:09:57 - INFO - transformers.data.datasets.glue -   Creating features from dataset file at transformers_data_and_model/MRPC\n",
      "07/27/2020 08:09:57 - INFO - transformers.data.processors.glue -   LOOKING AT transformers_data_and_model/MRPC/train.tsv\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   guid: train-1\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   guid: train-2\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   guid: train-3\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102, 2006, 2238, 2184, 1010, 1996, 2911, 1005, 1055, 5608, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 1010, 5378, 1996, 14792, 2005, 5096, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   guid: train-4\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 2105, 6021, 19481, 13938, 2102, 1010, 21628, 6661, 2020, 2039, 2539, 16653, 1010, 2030, 1018, 1012, 1018, 1003, 1010, 2012, 1037, 1002, 1018, 1012, 5179, 1010, 2383, 3041, 2275, 1037, 2501, 2152, 1997, 1037, 1002, 1018, 1012, 5401, 1012, 102, 21628, 6661, 5598, 2322, 16653, 1010, 2030, 1018, 1012, 1020, 1003, 1010, 2000, 2275, 1037, 2501, 5494, 2152, 2012, 1037, 1002, 1018, 1012, 5401, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   guid: train-5\n",
      "07/27/2020 08:10:00 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 1996, 4518, 3123, 1002, 1016, 1012, 2340, 1010, 2030, 2055, 2340, 3867, 1010, 2000, 2485, 5958, 2012, 1002, 2538, 1012, 4868, 2006, 1996, 2047, 2259, 4518, 3863, 1012, 102, 18720, 1004, 1041, 13058, 1012, 6661, 5598, 1002, 1015, 1012, 6191, 2030, 1022, 3867, 2000, 1002, 2538, 1012, 6021, 2006, 1996, 2047, 2259, 4518, 3863, 2006, 5958, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.datasets.glue -   Saving features into cached file transformers_data_and_model/MRPC/cached_train_BertTokenizer_128_mrpc [took 0.812 s]\n",
      "07/27/2020 08:10:01 - INFO - filelock -   Lock 139828797735264 released on transformers_data_and_model/MRPC/cached_train_BertTokenizer_128_mrpc.lock\n",
      "07/27/2020 08:10:01 - INFO - filelock -   Lock 139828723620608 acquired on transformers_data_and_model/MRPC/cached_dev_BertTokenizer_128_mrpc.lock\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.datasets.glue -   Creating features from dataset file at transformers_data_and_model/MRPC\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 2002, 2056, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2987, 1005, 1056, 4906, 1996, 2194, 1005, 1055, 2146, 1011, 2744, 3930, 5656, 1012, 102, 1000, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2515, 2025, 4906, 2256, 2146, 1011, 2744, 3930, 5656, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 20201, 22948, 2056, 10958, 19053, 4140, 6283, 1996, 8956, 6939, 1998, 2246, 2830, 2000, 2478, 2010, 2146, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 2010, 2564, 2056, 2002, 2001, 1000, 2531, 3867, 2369, 2577, 5747, 1000, 1998, 2246, 2830, 2000, 2478, 2010, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 1996, 7922, 2001, 2012, 12904, 1012, 6227, 18371, 2114, 1996, 18371, 1010, 4257, 2006, 1996, 5219, 1010, 1998, 2012, 1015, 1012, 27054, 2487, 2114, 1996, 5364, 23151, 2278, 1010, 2036, 4257, 1012, 102, 1996, 7922, 2001, 2012, 12904, 1012, 6275, 18371, 16545, 2100, 1027, 1010, 8990, 4257, 2006, 1996, 5219, 1010, 1998, 2012, 1015, 1012, 23090, 2487, 2114, 1996, 5364, 23151, 2278, 10381, 2546, 1027, 1010, 2091, 1014, 1012, 1015, 3867, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 1996, 10028, 1011, 25022, 2080, 2003, 3403, 2127, 2255, 2000, 5630, 2065, 2009, 2097, 2203, 5668, 2063, 1037, 4018, 1012, 102, 1996, 10028, 1011, 25022, 2080, 2623, 9317, 2008, 2009, 2097, 5630, 1999, 2255, 3251, 2000, 2203, 5668, 2063, 1037, 4018, 2077, 1996, 27419, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 2053, 5246, 2031, 2042, 2275, 2005, 1996, 2942, 2030, 1996, 4735, 3979, 1012, 102, 2053, 5246, 2031, 2042, 2275, 2005, 1996, 4735, 2030, 2942, 3572, 1010, 2021, 17137, 3051, 2038, 12254, 2025, 5905, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/27/2020 08:10:01 - INFO - transformers.data.datasets.glue -   Saving features into cached file transformers_data_and_model/MRPC/cached_dev_BertTokenizer_128_mrpc [took 0.109 s]\n",
      "07/27/2020 08:10:01 - INFO - filelock -   Lock 139828723620608 released on transformers_data_and_model/MRPC/cached_dev_BertTokenizer_128_mrpc.lock\n",
      "07/27/2020 08:10:05 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "07/27/2020 08:10:05 - INFO - transformers.trainer -   ***** Running training *****\n",
      "07/27/2020 08:10:05 - INFO - transformers.trainer -     Num examples = 3668\n",
      "07/27/2020 08:10:05 - INFO - transformers.trainer -     Num Epochs = 3\n",
      "07/27/2020 08:10:05 - INFO - transformers.trainer -     Instantaneous batch size per device = 32\n",
      "07/27/2020 08:10:05 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "07/27/2020 08:10:05 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "07/27/2020 08:10:05 - INFO - transformers.trainer -     Total optimization steps = 174\n",
      "07/27/2020 08:10:05 - INFO - transformers.trainer -     Starting fine-tuning.\n",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                         | 0/58 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "Iteration:   2%|â–Œ                                | 1/58 [00:04<03:55,  4.13s/it]\u001b[A\n",
      "Iteration:   3%|â–ˆâ–                               | 2/58 [00:04<02:50,  3.04s/it]\u001b[A\n",
      "Iteration:   5%|â–ˆâ–‹                               | 3/58 [00:04<02:02,  2.23s/it]\u001b[A\n",
      "Iteration:   7%|â–ˆâ–ˆâ–                              | 4/58 [00:05<01:30,  1.67s/it]\u001b[A\n",
      "Iteration:   9%|â–ˆâ–ˆâ–Š                              | 5/58 [00:05<01:07,  1.27s/it]\u001b[A\n",
      "Iteration:  10%|â–ˆâ–ˆâ–ˆâ–                             | 6/58 [00:06<00:51,  1.00it/s]\u001b[A\n",
      "Iteration:  12%|â–ˆâ–ˆâ–ˆâ–‰                             | 7/58 [00:06<00:40,  1.25it/s]\u001b[A\n",
      "Iteration:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 8/58 [00:06<00:33,  1.50it/s]\u001b[A\n",
      "Iteration:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 9/58 [00:07<00:27,  1.75it/s]\u001b[A\n",
      "Iteration:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 10/58 [00:07<00:24,  1.99it/s]\u001b[A\n",
      "Iteration:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 11/58 [00:07<00:21,  2.19it/s]\u001b[A\n",
      "Iteration:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 12/58 [00:08<00:19,  2.36it/s]\u001b[A\n",
      "Iteration:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 13/58 [00:08<00:18,  2.49it/s]\u001b[A\n",
      "Iteration:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 14/58 [00:08<00:16,  2.60it/s]\u001b[A\n",
      "Iteration:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 15/58 [00:09<00:16,  2.64it/s]\u001b[A\n",
      "Iteration:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 16/58 [00:09<00:15,  2.68it/s]\u001b[A\n",
      "Iteration:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 17/58 [00:09<00:15,  2.70it/s]\u001b[A\n",
      "Iteration:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 18/58 [00:10<00:14,  2.75it/s]\u001b[A\n",
      "Iteration:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 19/58 [00:10<00:14,  2.78it/s]\u001b[A\n",
      "Iteration:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 20/58 [00:10<00:13,  2.79it/s]\u001b[A\n",
      "Iteration:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 21/58 [00:11<00:13,  2.80it/s]\u001b[A\n",
      "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 22/58 [00:11<00:12,  2.82it/s]\u001b[A\n",
      "Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 23/58 [00:12<00:12,  2.83it/s]\u001b[A\n",
      "Iteration:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 24/58 [00:12<00:11,  2.84it/s]\u001b[A\n",
      "Iteration:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 25/58 [00:12<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 26/58 [00:13<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 27/58 [00:13<00:10,  2.86it/s]\u001b[A\n",
      "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 28/58 [00:13<00:10,  2.86it/s]\u001b[A\n",
      "Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 29/58 [00:14<00:10,  2.87it/s]\u001b[A\n",
      "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 30/58 [00:14<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 31/58 [00:14<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 32/58 [00:15<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 33/58 [00:15<00:08,  2.85it/s]\u001b[A\n",
      "Iteration:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 34/58 [00:15<00:08,  2.86it/s]\u001b[A\n",
      "Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 35/58 [00:16<00:08,  2.85it/s]\u001b[A\n",
      "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 36/58 [00:16<00:07,  2.85it/s]\u001b[A\n",
      "Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 37/58 [00:16<00:07,  2.86it/s]\u001b[A\n",
      "Iteration:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 38/58 [00:17<00:06,  2.86it/s]\u001b[A\n",
      "Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 39/58 [00:17<00:06,  2.86it/s]\u001b[A\n",
      "Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 40/58 [00:17<00:06,  2.86it/s]\u001b[A\n",
      "Iteration:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 41/58 [00:18<00:05,  2.86it/s]\u001b[A\n",
      "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 42/58 [00:18<00:05,  2.86it/s]\u001b[A\n",
      "Iteration:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 43/58 [00:18<00:05,  2.86it/s]\u001b[A\n",
      "Iteration:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 44/58 [00:19<00:04,  2.85it/s]\u001b[A\n",
      "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 45/58 [00:19<00:04,  2.85it/s]\u001b[A\n",
      "Iteration:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 46/58 [00:20<00:04,  2.86it/s]\u001b[A\n",
      "Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 47/58 [00:20<00:03,  2.86it/s]\u001b[A\n",
      "Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 48/58 [00:20<00:03,  2.85it/s]\u001b[A\n",
      "Iteration:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 49/58 [00:21<00:03,  2.85it/s]\u001b[A\n",
      "Iteration:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 50/58 [00:21<00:02,  2.85it/s]\u001b[A\n",
      "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 51/58 [00:21<00:02,  2.85it/s]\u001b[A\n",
      "Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 52/58 [00:22<00:02,  2.85it/s]\u001b[A\n",
      "Iteration:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 53/58 [00:22<00:01,  2.85it/s]\u001b[A\n",
      "Iteration:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 54/58 [00:22<00:01,  2.85it/s]\u001b[A\n",
      "Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/58 [00:23<00:01,  2.86it/s]\u001b[A\n",
      "Iteration:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/58 [00:23<00:00,  2.86it/s]\u001b[A\n",
      "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 57/58 [00:23<00:00,  2.86it/s]\u001b[A\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:24<00:00,  2.41it/s]\u001b[A\n",
      "Epoch:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 1/3 [00:24<00:48, 24.11s/it]\n",
      "Iteration:   0%|                                         | 0/58 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|â–Œ                                | 1/58 [00:00<00:19,  2.87it/s]\u001b[A\n",
      "Iteration:   3%|â–ˆâ–                               | 2/58 [00:00<00:19,  2.86it/s]\u001b[A\n",
      "Iteration:   5%|â–ˆâ–‹                               | 3/58 [00:01<00:19,  2.86it/s]\u001b[A\n",
      "Iteration:   7%|â–ˆâ–ˆâ–                              | 4/58 [00:01<00:18,  2.86it/s]\u001b[A\n",
      "Iteration:   9%|â–ˆâ–ˆâ–Š                              | 5/58 [00:01<00:18,  2.86it/s]\u001b[A\n",
      "Iteration:  10%|â–ˆâ–ˆâ–ˆâ–                             | 6/58 [00:02<00:18,  2.82it/s]\u001b[A\n",
      "Iteration:  12%|â–ˆâ–ˆâ–ˆâ–‰                             | 7/58 [00:02<00:18,  2.80it/s]\u001b[A\n",
      "Iteration:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 8/58 [00:02<00:17,  2.78it/s]\u001b[A\n",
      "Iteration:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 9/58 [00:03<00:17,  2.77it/s]\u001b[A\n",
      "Iteration:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 10/58 [00:03<00:17,  2.80it/s]\u001b[A\n",
      "Iteration:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 11/58 [00:03<00:16,  2.82it/s]\u001b[A\n",
      "Iteration:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 12/58 [00:04<00:16,  2.83it/s]\u001b[A\n",
      "Iteration:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 13/58 [00:04<00:16,  2.79it/s]\u001b[A\n",
      "Iteration:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 14/58 [00:04<00:15,  2.78it/s]\u001b[A\n",
      "Iteration:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 15/58 [00:05<00:15,  2.79it/s]\u001b[A\n",
      "Iteration:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 16/58 [00:05<00:15,  2.78it/s]\u001b[A\n",
      "Iteration:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 17/58 [00:06<00:14,  2.80it/s]\u001b[A\n",
      "Iteration:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 18/58 [00:06<00:14,  2.82it/s]\u001b[A\n",
      "Iteration:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 19/58 [00:06<00:13,  2.83it/s]\u001b[A\n",
      "Iteration:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 20/58 [00:07<00:13,  2.84it/s]\u001b[A\n",
      "Iteration:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 21/58 [00:07<00:13,  2.84it/s]\u001b[A\n",
      "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 22/58 [00:07<00:12,  2.84it/s]\u001b[A\n",
      "Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 23/58 [00:08<00:12,  2.84it/s]\u001b[A\n",
      "Iteration:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 24/58 [00:08<00:11,  2.84it/s]\u001b[A\n",
      "Iteration:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 25/58 [00:08<00:11,  2.84it/s]\u001b[A\n",
      "Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 26/58 [00:09<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 27/58 [00:09<00:10,  2.85it/s]\u001b[A\n",
      "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 28/58 [00:09<00:10,  2.85it/s]\u001b[A\n",
      "Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 29/58 [00:10<00:10,  2.85it/s]\u001b[A\n",
      "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 30/58 [00:10<00:09,  2.80it/s]\u001b[A\n",
      "Iteration:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 31/58 [00:10<00:09,  2.81it/s]\u001b[A\n",
      "Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 32/58 [00:11<00:09,  2.82it/s]\u001b[A\n",
      "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 33/58 [00:11<00:08,  2.83it/s]\u001b[A\n",
      "Iteration:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 34/58 [00:12<00:08,  2.83it/s]\u001b[A\n",
      "Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 35/58 [00:12<00:08,  2.84it/s]\u001b[A\n",
      "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 36/58 [00:12<00:07,  2.80it/s]\u001b[A\n",
      "Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 37/58 [00:13<00:07,  2.78it/s]\u001b[A\n",
      "Iteration:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 38/58 [00:13<00:07,  2.77it/s]\u001b[A\n",
      "Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 39/58 [00:13<00:06,  2.74it/s]\u001b[A\n",
      "Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 40/58 [00:14<00:06,  2.77it/s]\u001b[A\n",
      "Iteration:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 41/58 [00:14<00:06,  2.80it/s]\u001b[A\n",
      "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 42/58 [00:14<00:05,  2.81it/s]\u001b[A\n",
      "Iteration:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 43/58 [00:15<00:05,  2.83it/s]\u001b[A\n",
      "Iteration:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 44/58 [00:15<00:04,  2.83it/s]\u001b[A\n",
      "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 45/58 [00:15<00:04,  2.84it/s]\u001b[A\n",
      "Iteration:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 46/58 [00:16<00:04,  2.84it/s]\u001b[A\n",
      "Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 47/58 [00:16<00:03,  2.85it/s]\u001b[A\n",
      "Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 48/58 [00:17<00:03,  2.85it/s]\u001b[A\n",
      "Iteration:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 49/58 [00:17<00:03,  2.85it/s]\u001b[A\n",
      "Iteration:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 50/58 [00:17<00:02,  2.85it/s]\u001b[A\n",
      "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 51/58 [00:18<00:02,  2.85it/s]\u001b[A\n",
      "Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 52/58 [00:18<00:02,  2.84it/s]\u001b[A\n",
      "Iteration:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 53/58 [00:18<00:01,  2.84it/s]\u001b[A\n",
      "Iteration:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 54/58 [00:19<00:01,  2.83it/s]\u001b[A\n",
      "Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/58 [00:19<00:01,  2.83it/s]\u001b[A\n",
      "Iteration:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/58 [00:19<00:00,  2.83it/s]\u001b[A\n",
      "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 57/58 [00:20<00:00,  2.83it/s]\u001b[A\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:20<00:00,  2.84it/s]\u001b[A\n",
      "Epoch:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 2/3 [00:44<00:22, 23.00s/it]\n",
      "Iteration:   0%|                                         | 0/58 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|â–Œ                                | 1/58 [00:00<00:19,  2.86it/s]\u001b[A\n",
      "Iteration:   3%|â–ˆâ–                               | 2/58 [00:00<00:19,  2.85it/s]\u001b[A\n",
      "Iteration:   5%|â–ˆâ–‹                               | 3/58 [00:01<00:19,  2.85it/s]\u001b[A\n",
      "Iteration:   7%|â–ˆâ–ˆâ–                              | 4/58 [00:01<00:18,  2.85it/s]\u001b[A\n",
      "Iteration:   9%|â–ˆâ–ˆâ–Š                              | 5/58 [00:01<00:18,  2.84it/s]\u001b[A\n",
      "Iteration:  10%|â–ˆâ–ˆâ–ˆâ–                             | 6/58 [00:02<00:18,  2.84it/s]\u001b[A\n",
      "Iteration:  12%|â–ˆâ–ˆâ–ˆâ–‰                             | 7/58 [00:02<00:17,  2.85it/s]\u001b[A\n",
      "Iteration:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 8/58 [00:02<00:17,  2.85it/s]\u001b[A\n",
      "Iteration:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 9/58 [00:03<00:17,  2.85it/s]\u001b[A\n",
      "Iteration:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 10/58 [00:03<00:16,  2.85it/s]\u001b[A\n",
      "Iteration:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 11/58 [00:03<00:16,  2.85it/s]\u001b[A\n",
      "Iteration:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 12/58 [00:04<00:16,  2.86it/s]\u001b[A\n",
      "Iteration:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 13/58 [00:04<00:15,  2.85it/s]\u001b[A\n",
      "Iteration:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 14/58 [00:04<00:15,  2.85it/s]\u001b[A\n",
      "Iteration:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 15/58 [00:05<00:15,  2.85it/s]\u001b[A\n",
      "Iteration:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 16/58 [00:05<00:14,  2.85it/s]\u001b[A\n",
      "Iteration:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 17/58 [00:05<00:14,  2.84it/s]\u001b[A\n",
      "Iteration:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 18/58 [00:06<00:14,  2.84it/s]\u001b[A\n",
      "Iteration:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 19/58 [00:06<00:13,  2.84it/s]\u001b[A\n",
      "Iteration:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 20/58 [00:07<00:13,  2.84it/s]\u001b[A\n",
      "Iteration:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 21/58 [00:07<00:13,  2.84it/s]\u001b[A\n",
      "Iteration:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 22/58 [00:07<00:12,  2.85it/s]\u001b[A\n",
      "Iteration:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 23/58 [00:08<00:12,  2.85it/s]\u001b[A\n",
      "Iteration:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 24/58 [00:08<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 25/58 [00:08<00:11,  2.86it/s]\u001b[A\n",
      "Iteration:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 26/58 [00:09<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 27/58 [00:09<00:10,  2.85it/s]\u001b[A\n",
      "Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 28/58 [00:09<00:10,  2.84it/s]\u001b[A\n",
      "Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 29/58 [00:10<00:10,  2.84it/s]\u001b[A\n",
      "Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 30/58 [00:10<00:09,  2.84it/s]\u001b[A\n",
      "Iteration:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 31/58 [00:10<00:09,  2.85it/s]\u001b[A\n",
      "Iteration:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 32/58 [00:11<00:09,  2.85it/s]\u001b[A\n",
      "Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 33/58 [00:11<00:08,  2.84it/s]\u001b[A\n",
      "Iteration:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 34/58 [00:11<00:08,  2.84it/s]\u001b[A\n",
      "Iteration:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 35/58 [00:12<00:08,  2.85it/s]\u001b[A\n",
      "Iteration:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 36/58 [00:12<00:07,  2.84it/s]\u001b[A\n",
      "Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 37/58 [00:13<00:07,  2.84it/s]\u001b[A\n",
      "Iteration:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 38/58 [00:13<00:07,  2.84it/s]\u001b[A\n",
      "Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 39/58 [00:13<00:06,  2.84it/s]\u001b[A\n",
      "Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 40/58 [00:14<00:06,  2.84it/s]\u001b[A\n",
      "Iteration:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 41/58 [00:14<00:05,  2.83it/s]\u001b[A\n",
      "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 42/58 [00:14<00:05,  2.81it/s]\u001b[A\n",
      "Iteration:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 43/58 [00:15<00:05,  2.79it/s]\u001b[A\n",
      "Iteration:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 44/58 [00:15<00:05,  2.78it/s]\u001b[A\n",
      "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 45/58 [00:15<00:04,  2.79it/s]\u001b[A\n",
      "Iteration:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 46/58 [00:16<00:04,  2.80it/s]\u001b[A\n",
      "Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 47/58 [00:16<00:03,  2.81it/s]\u001b[A\n",
      "Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 48/58 [00:16<00:03,  2.81it/s]\u001b[A\n",
      "Iteration:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 49/58 [00:17<00:03,  2.82it/s]\u001b[A\n",
      "Iteration:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 50/58 [00:17<00:02,  2.79it/s]\u001b[A\n",
      "Iteration:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 51/58 [00:17<00:02,  2.80it/s]\u001b[A\n",
      "Iteration:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 52/58 [00:18<00:02,  2.81it/s]\u001b[A\n",
      "Iteration:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 53/58 [00:18<00:01,  2.82it/s]\u001b[A\n",
      "Iteration:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 54/58 [00:19<00:01,  2.82it/s]\u001b[A\n",
      "Iteration:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/58 [00:19<00:01,  2.82it/s]\u001b[A\n",
      "Iteration:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/58 [00:19<00:00,  2.82it/s]\u001b[A\n",
      "Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 57/58 [00:20<00:00,  2.80it/s]\u001b[A\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:20<00:00,  2.85it/s]\u001b[A\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:04<00:00, 21.62s/it]\n",
      "07/27/2020 08:11:10 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "07/27/2020 08:11:10 - INFO - transformers.trainer -   Saving model checkpoint to transformers_data_and_model/finetuned-mrpc\n",
      "07/27/2020 08:11:10 - INFO - transformers.configuration_utils -   Configuration saved in transformers_data_and_model/finetuned-mrpc/config.json\n",
      "07/27/2020 08:11:11 - INFO - transformers.modeling_utils -   Model weights saved in transformers_data_and_model/finetuned-mrpc/pytorch_model.bin\n",
      "07/27/2020 08:11:11 - INFO - __main__ -   *** Evaluate ***\n",
      "07/27/2020 08:11:11 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "07/27/2020 08:11:11 - INFO - transformers.trainer -     Num examples = 408\n",
      "07/27/2020 08:11:11 - INFO - transformers.trainer -     Batch size = 16\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:01<00:00, 15.30it/s]\n",
      "07/27/2020 08:11:13 - INFO - transformers.trainer -   {'eval_loss': 0.4694334108095903, 'eval_acc': 0.8112745098039216, 'eval_f1': 0.8718801996672212, 'eval_acc_and_f1': 0.8415773547355714, 'epoch': 3.0, 'step': 174}\n",
      "07/27/2020 08:11:13 - INFO - __main__ -   ***** Eval results mrpc *****\n",
      "07/27/2020 08:11:13 - INFO - __main__ -     eval_loss = 0.4694334108095903\n",
      "07/27/2020 08:11:13 - INFO - __main__ -     eval_acc = 0.8112745098039216\n",
      "07/27/2020 08:11:13 - INFO - __main__ -     eval_f1 = 0.8718801996672212\n",
      "07/27/2020 08:11:13 - INFO - __main__ -     eval_acc_and_f1 = 0.8415773547355714\n",
      "07/27/2020 08:11:13 - INFO - __main__ -     epoch = 3.0\n"
     ]
    }
   ],
   "source": [
    "# ç›´æ¥ä½¿ç”¨transformerså®˜æ–¹çš„ä»£ç è¿è¡Œ\n",
    "!python {RUN_GLUE_PY} \\\n",
    "  --model_name_or_path {BERT_MODEL_NAME_OR_PATH} \\\n",
    "  --task_name MRPC \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --data_dir {MRPC_DATA_DIR} \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --output_dir {FINETUNED_MRPC} \\\n",
    "  --overwrite_cache \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers_data_and_model/finetuned-mrpc\n",
      "â”œâ”€â”€ config.json\n",
      "â”œâ”€â”€ eval_results_mrpc.txt\n",
      "â”œâ”€â”€ pytorch_model.bin\n",
      "â”œâ”€â”€ special_tokens_map.json\n",
      "â”œâ”€â”€ tokenizer_config.json\n",
      "â”œâ”€â”€ training_args.bin\n",
      "â””â”€â”€ vocab.txt\n",
      "\n",
      "0 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹finetuneåçš„æ¨¡å‹ç­‰\n",
    "!tree {FINETUNED_MRPC}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ä¸ºMRPCæ•°æ®é›†éå¸¸å°ï¼Œä»»åŠ¡ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œ3ä¸ªepochï¼Œå­¦ä¹ ç‡3e-5ï¼Œå°±èƒ½è½»æ¾çš„å®Œæˆäº†è®­ç»ƒï¼Œå¯ä»¥çœ‹åˆ°åœ¨éªŒè¯é›†ä¸Šaccuracyè¾¾åˆ°äº†81.1%ï¼ŒF1å€¼è¾¾åˆ°äº†0.84ã€‚è®­ç»ƒå¥½çš„æ¨¡å‹modelå’Œtokenizeréƒ½ä¿å­˜åœ¨äº†FINETUNE_MRPCæ–‡ä»¶å¤¹ä¸‹äº†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 çº¿ä¸Šé¢„æµ‹\n",
    "\n",
    "è®­ç»ƒå¥½äº†æ¨¡å‹ï¼Œå°±å¯ä»¥çº¿ä¸Šè¿›è¡Œé¢„æµ‹äº†ï¼Œæ€ä¹ˆåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹å‘¢ï¼Ÿä¹Ÿéå¸¸ç®€å•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.4.0+cu100\n",
      "transformers: 3.0.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f'torch: {torch.__version__}')\n",
    "print(f'transformers: {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 38%\n",
      "is paraphrase: 62%\n",
      "classification result: is paraphrase\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½tokenizerå’Œmodel\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(FINETUNED_MRPC)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(FINETUNED_MRPC)\n",
    "\n",
    "# ç±»åˆ«æ ‡ç­¾\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "\n",
    "# é€šè¿‡tokenizerå°†æ–‡æœ¬è½¬æˆmodeléœ€è¦çš„æ ¼å¼ï¼Œè¿”å›ä¸ºpytorchçš„tensor\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # æ˜¯é‡Šä¹‰çš„æ ·ä¾‹\n",
    "    # è¾“å…¥æ¨¡å‹ï¼Œæ¨¡å‹è¾“å‡ºä¸ºå…ƒç»„ï¼Œä¸è¾“å…¥æ­£ç¡®æ ‡ç­¾labelså‚æ•°çš„æƒ…å†µä¸‹ï¼Œç¬¬ä¸€ä¸ªlogits\n",
    "    paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "    # logitsç»è¿‡softmaxå¾—åˆ°æœ€ç»ˆçš„æ¦‚ç‡\n",
    "    paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "# ç»“æœåº”å½“æ˜¯é‡Šä¹‰\n",
    "for i in range(len(classes)):\n",
    "     print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "print(f\"classification result: {classes[paraphrase_results.index(max(paraphrase_results))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ€»ç»“ï¼šå¯ä»¥çœ‹åˆ°ï¼Œçº¿ä¸Šç”¨äºé¢„æµ‹çš„ä»£ç ä¹Ÿéå¸¸ç®€å•ã€‚åŠ è½½tokenizerå’Œmodelï¼Œç„¶åé€šè¿‡tokenizeræ ‡å‡†åŒ–ä¸ºmodelçš„è¾“å…¥ï¼Œæ¨¡å‹è¾“å…¥è¾“å‡ºä¸ºå…ƒç»„ï¼Œå…ƒç»„çš„ç»“æœè¿›è¡Œsoftmaxï¼Œå¾—åˆ°å„ä¸ªlabelçš„æ¦‚ç‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„ç»“æœæ¦‚ç‡ã€‚å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿›è¡Œçº¿ä¸Šéƒ¨ç½²çš„ä»£ç ä¹Ÿéå¸¸ç®€å•å’Œå®¹æ˜“ã€‚\n",
    "\n",
    "ä¸Šé¢ä¸ºäº†ç®€å•èµ·è§ï¼Œæ²¡æœ‰ä½¿ç”¨GPUã€‚å¦‚æœçº¿ä¸Šéƒ¨ç½²éœ€è¦æ”¯æŒGPUï¼Œä¹Ÿå¾ˆç®€å•ï¼Œåªè¦å°†tokenizerè¾“å‡ºçš„ç»“æœä¼ å…¥GPUï¼Œmodelä¹Ÿä¼ å…¥GPUå³å¯ã€‚ä¸€èˆ¬tokenizerè¾“å‡ºç»“æœæ˜¯ä¸€ä¸ªè¯å…¸æ ¼å¼ï¼Œè¯å…¸çš„valueä¸ºtensorï¼Œå¯ä»¥è½¬å…¥GPUï¼Œè¯·çœ‹ä¸‹é¢çš„ä¾‹å­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 38%\n",
      "is paraphrase: 62%\n",
      "classification result: is paraphrase\n"
     ]
    }
   ],
   "source": [
    "# æ”¯æŒGPUåˆ™ä½¿ç”¨GPUï¼Œä¸æ”¯æŒåˆ™ä½¿ç”¨CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# åŠ è½½tokenizerå’Œmodel\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(FINETUNED_MRPC)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(FINETUNED_MRPC)\n",
    "# modelè½¬å…¥cuda\n",
    "model = model.to(device)\n",
    "\n",
    "# ç±»åˆ«æ ‡ç­¾\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "# é€šè¿‡tokenizerå°†æ–‡æœ¬è½¬æˆmodeléœ€è¦çš„æ ¼å¼ï¼Œè¿”å›ä¸ºpytorchçš„tensor\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "# paraphraseè½¬å…¥GPU\n",
    "# valueæ˜¯tensorç±»çš„ï¼Œè½¬å…¥GPUè¿›è¡ŒåŠ é€Ÿ\n",
    "# é™¤äº†modelå’Œè¾“å…¥çš„è½¬å…¥GPUï¼Œå…¶ä»–æ— åŒºåˆ«\n",
    "for k, v in paraphrase.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        paraphrase[k] = v.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # æ˜¯é‡Šä¹‰çš„æ ·ä¾‹\n",
    "    # è¾“å…¥æ¨¡å‹ï¼Œæ¨¡å‹è¾“å‡ºä¸ºå…ƒç»„ï¼Œä¸è¾“å…¥æ­£ç¡®æ ‡ç­¾labelså‚æ•°çš„æƒ…å†µä¸‹ï¼Œç¬¬ä¸€ä¸ªlogits\n",
    "    paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "    # logitsç»è¿‡softmaxå¾—åˆ°æœ€ç»ˆçš„æ¦‚ç‡\n",
    "    paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "# ç»“æœåº”å½“æ˜¯é‡Šä¹‰\n",
    "for i in range(len(classes)):\n",
    "     print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "print(f\"classification result: {classes[paraphrase_results.index(max(paraphrase_results))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨å·²ç»çœ‹åˆ°äº†çº¿ä¸Šéƒ¨ç½²çš„å…¨éƒ¨ä»£ç ï¼Œé‚£ä¹ˆè®­ç»ƒè¿‡ç¨‹4.2èŠ‚çš„è¯¦ç»†ä»£ç æ˜¯ä»€ä¹ˆæ ·çš„å‘¢ï¼Ÿä¸‹ä¸€å°èŠ‚å°±æ˜¯è¦è¯´æ˜è¿™ä¸ªé—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 æ¨¡å‹è®­ç»ƒçš„è¯¦ç»†è¿‡ç¨‹ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformersæä¾›äº†configã€tokenizerã€modelç­‰ç±»ç®€åŒ–äº†åˆ†è¯ã€æ¨¡å‹ç­‰æ­¥éª¤ï¼ŒåŒæ—¶åˆæœ‰Trainerç±»ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚é‚£ä¹ˆæ›´è¯¦ç»†çš„è®­ç»ƒè¿‡ç¨‹æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæœ¬èŠ‚ä¸»è¦çš„å†…å®¹å°±æ˜¯å®ç°å’Œè®²è§£æ¨¡å‹åˆ†ç±»çš„è¯¦ç»†è¿‡ç¨‹ã€‚\n",
    "\n",
    "ç®€å•çš„è®²ï¼Œæ–‡æœ¬æœ€å¼€å§‹éœ€è¦è½½å…¥ï¼Œå¯ä»¥é€šè¿‡å†™æ˜ä¸€ä¸ªprocessor\n",
    "\n",
    "æœ¬è¿‡ç¨‹ä¸ä»…æ˜¯ä¸ºäº†å®ç°MRPCåˆ†ç±»ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå®ƒå¯ä»¥æ˜¯æ–‡æœ¬åˆ†ç±»çš„ä¸€ä¸ªæ ‡å‡†åŒ–æµç¨‹ï¼Œä¹Ÿæ˜¯pytorchä½¿ç”¨çš„æ ‡å‡†åŒ–æµç¨‹ï¼Œå¯ä»¥æ–¹ä¾¿ä»¥åæŒ‰ç…§æ­¤æ€è·¯è¿›è¡Œæ‰©å±•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import enum\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, Optional, List, Union, NamedTuple\n",
    "\n",
    "import filelock\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥å¿—æ–‡ä»¶\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ¨¡å‹å‚æ•°ï¼ŒåŒ…å«modelã€configã€tokenizerã€cache_dirç­‰\n",
    "# æœ‰ä¸‰ç±»å‚æ•°ï¼š\n",
    "#   ä¸€ä¸ªæ˜¯æ¨¡å‹å‚æ•°ï¼Œè¿™ä¸ªå°±æ˜¯ä¸‹é¢çš„å®šä¹‰ï¼›\n",
    "#   ä¸€ä¸ªæ˜¯æ¨¡å‹è®­ç»ƒå‚æ•°ï¼Œ å¯ä»¥å‚è€ƒtransformers/src/train_args.pyæ–‡ä»¶ï¼Œä¸»è¦æ˜¯epochã€batch_sizeç­‰å¸¸è§çš„è®­ç»ƒå‚æ•°ï¼Œä¹ŸåŒ…å«deviceè¿™ç§è®¾å¤‡å‚æ•°\n",
    "#   ä¸€ä¸ªæ•°æ®å‚æ•°ï¼Œå†³å®šæ•°æ®å¤„ç†ä»»åŠ¡çš„å‚æ•°ï¼Œä½¿ç”¨ä»€ä¹ˆæ•°æ®ï¼Œæ•°æ®åç§°ï¼Œæ˜¯å¦è¦†ç›–æ•°æ®çš„cacheï¼Œæœ€é•¿é•¿åº¦ï¼Œè¿™ä¸ªé•¿åº¦æ˜¯ç”¨äºç”Ÿæˆfeaturesä½¿ç”¨çš„\n",
    "# ä¸‹é¢è¿™ä¸ªæ˜¯æ¨¡å‹å‚æ•°\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "        \n",
    "\n",
    "# æ•°æ®ï¼ˆè®­ç»ƒä½¿ç”¨çš„ï¼‰å‚æ•°\n",
    "@dataclass\n",
    "class GlueDataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "\n",
    "    Using `HfArgumentParser` we can turn this class\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: str = field(metadata={\"help\": \"The name of the task to train on: \" + \", \".join(transformers.glue_processors.keys())})\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .tsv files (or other data files) for the task.\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.task_name = self.task_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½å¥½çš„é¢„è®­ç»ƒæ¨¡å‹ä½ç½®\n",
    "BERT_MODEL_NAME_OR_PATH = '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased'\n",
    "# ä¸‹è½½å¥½çš„glueæ•°æ®é›†ä¸­çš„MRPCæ•°æ®é›†çš„ä½ç½®\n",
    "MRPC_DATA_DIR = '/dfsdata2/yucc1_data/datasets/glue_data/MRPC'\n",
    "# finetuneå¥½çš„modelã€tokenizerç­‰å„ç§å‚æ•°å­˜æ”¾çš„ä½ç½®\n",
    "FINETUNED_MRPC = '/dfsdata2/yucc1_data/output/finetuned-mrpc'\n",
    "\n",
    "input_args = ['--model_name_or_path', BERT_MODEL_NAME_OR_PATH,\n",
    "             '--task_name', 'MRPC',\n",
    "             '--do_train',\n",
    "             '--do_eval',\n",
    "             '--data_dir', MRPC_DATA_DIR,\n",
    "             '--max_seq_length', '128',\n",
    "             '--per_device_train_batch_size', '32',\n",
    "             '--learning_rate', '3e-5',\n",
    "             '--num_train_epochs', '3.0',\n",
    "             '--output_dir', FINETUNED_MRPC,\n",
    "             '--overwrite_cache',\n",
    "             '--overwrite_output_dir']\n",
    "\n",
    "# transformersé‡Œï¼Œæœ‰ä¸€ä¸ªHfArguementParserç”¨äºè§£æä¸Šé¢æ ¼å¼çš„å‚æ•°ï¼Œä¸ºæ ‡å‡†çš„pythonå‚æ•°\n",
    "parser = transformers.HfArgumentParser((ModelArguments, transformers.GlueDataTrainingArguments, transformers.TrainingArguments))\n",
    "# æŸ¥çœ‹helpï¼Ÿ\n",
    "# å°†ä¸‰ç±»å‚æ•°åˆ†åˆ«è§£æä¸ºå¯¹åº”çš„ç©ºé—´\n",
    "# æ¨¡å‹æœ¬èº«çš„å‚æ•°ï¼Œæ•°æ®çš„å‚æ•°ï¼Œè®­ç»ƒçš„å‚æ•°\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(input_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArguments(model_name_or_path='/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased', config_name=None, tokenizer_name=None, cache_dir=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlueDataTrainingArguments(task_name='mrpc', data_dir='/dfsdata2/yucc1_data/datasets/glue_data/MRPC', max_seq_length=128, overwrite_cache=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/finetuned-mrpc', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul24_16-53-35_d585a65fe6e2', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¡®ä¿output_dirå¯ä»¥ç”¨\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/24/2020 16:53:41 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/24/2020 16:53:41 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "07/24/2020 16:53:41 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/finetuned-mrpc', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul24_16-53-35_d585a65fe6e2', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n"
     ]
    }
   ],
   "source": [
    "# è®¾å®šæ—¥å¿—æ ¼å¼ï¼Œè®°å½•ä¸€äº›å…³é”®å‚æ•°ï¼Œå¹¶ä¸”å°†è®­ç»ƒå‚æ•°æ‰“å°å‡ºæ¥\n",
    "# ä¸€ä¸ªå¾ˆé‡è¦çš„æ„Ÿå—ï¼šä½¿ç”¨loggeræ‰“å°ä¸­é—´å˜é‡å¾ˆé‡è¦ï¼Œå¯¹äºtransformersåº“ï¼Œè¿˜æ˜¯æˆ‘ä»¬å­¦ä¹ ã€å·¥ä½œä¸­ï¼Œéƒ½æ˜¯å¦‚æ­¤\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "# è®¾å®šç§å­\n",
    "transformers.set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputExample \n",
    "\n",
    "=>\n",
    "\n",
    "InputFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processor:\n",
    "    \n",
    "    get_train_examples\n",
    "        list(InputExample)\n",
    "    \n",
    "    get_dev_examples\n",
    "    \n",
    "    get_labels:\n",
    "        ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_tasks_num_labels = {\n",
    "    \"cola\": 2,\n",
    "    \"mnli\": 3,\n",
    "    \"mrpc\": 2,\n",
    "    \"sst-2\": 2,\n",
    "    \"sts-b\": 1,\n",
    "    \"qqp\": 2,\n",
    "    \"qnli\": 2,\n",
    "    \"wnli\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "glue_output_modes = {\n",
    "    \"cola\": \"classification\",\n",
    "    \"mnli\": \"classification\",\n",
    "    \"mnli-mm\": \"classification\",\n",
    "    \"mrpc\": \"classification\",\n",
    "    \"sst-2\": \"classification\",\n",
    "    \"sts-b\": \"regression\",\n",
    "    \"qqp\": \"classification\",\n",
    "    \"qnli\": \"classification\",\n",
    "    \"rte\": \"classification\",\n",
    "    \"wnli\": \"classification\",\n",
    "}\n",
    "\n",
    "\n",
    "# è·å¾—æ ‡ç­¾çš„ä¸ªæ•°\n",
    "# è¾“å‡ºçš„æ¨¡å¼ï¼Œè¿™é‡Œæ˜¯classificationä¸regressionä¸¤ç§\n",
    "# å¦‚æœé€‚é…æ–°ä»»åŠ¡ï¼Œæˆ‘ä»¬è¦çš„ä¸æ˜¯å»æŒ‰ç…§è¿™ç§æ ¼å¼ï¼Œè€Œæ˜¯è¦å¾—åˆ°è¿™ä¸¤ä¸ªå‚æ•°\n",
    "try:\n",
    "    num_labels = glue_tasks_num_labels[data_args.task_name]\n",
    "    output_mode = glue_output_modes[data_args.task_name]\n",
    "except KeyError:\n",
    "    raise ValueError(\"Task not found: %s\" % (data_args.task_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/24/2020 16:53:51 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json\n",
      "07/24/2020 16:53:51 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/24/2020 16:53:51 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/config.json\n",
      "07/24/2020 16:53:51 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/24/2020 16:53:51 - INFO - transformers.tokenization_utils_base -   Model name '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/dfsdata2/yucc1_data/models/huggingface/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/24/2020 16:53:51 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/added_tokens.json. We won't load it.\n",
      "07/24/2020 16:53:51 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/special_tokens_map.json. We won't load it.\n",
      "07/24/2020 16:53:51 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer_config.json. We won't load it.\n",
      "07/24/2020 16:53:51 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/tokenizer.json. We won't load it.\n",
      "07/24/2020 16:53:51 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/vocab.txt\n",
      "07/24/2020 16:53:51 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/24/2020 16:53:51 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/24/2020 16:53:51 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/24/2020 16:53:51 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/24/2020 16:53:51 - INFO - transformers.modeling_utils -   loading weights file /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased/pytorch_model.bin\n",
      "07/24/2020 16:53:55 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "07/24/2020 16:53:55 - WARNING - transformers.modeling_utils -   Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½configã€tokenizerã€modelè¿™ä¸‰ä¸ªã€‚\n",
    "# configæ˜¯åŒ…å«å±‚æ•°ã€dropoutå‚æ•°ã€headä¸ªæ•°ã€finetuneä»»åŠ¡ç­‰æ¨¡å‹ç›¸å…³å†…å®¹çš„å‚æ•°ï¼Œè¿™ä¸ªå‚æ•°åŠ è½½ååªæ˜¯ä¸ºäº†modelä½¿ç”¨ã€‚\n",
    "# configå†…å†™å…¥æ ‡ç­¾çš„ä¸ªæ•°ï¼Œå†³å®šmodelåé¢åˆ†ç±»ä½¿ç”¨çš„å…¨è¿æ¥çš„è¾“å‡ºçš„ä¸ªæ•°\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.DataProcessoræ˜¯ä¸€ä¸ªåŸºç±»ï¼Œéœ€è¦å®ç°get_train_examples,get_dev_examples, get_test_examples, get_labelsç­‰å‡ ä¸ªå‡½æ•°ï¼Œ\n",
    "# åˆ†åˆ«ç”¨äºæä¾›çš„InputExampleçš„é›†å’Œï¼ˆlistï¼‰å’Œæ ‡ç­¾çš„é›†å’Œ\n",
    "class MrpcProcessor(transformers.DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"sentence2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = None if set_type == \"test\" else line[0]\n",
    "            examples.append(transformers.InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "    \n",
    "\n",
    "glue_processors = {\n",
    "    # ä½¿ç”¨ä¸Šé¢çš„ä»£ç \n",
    "    \"mrpc\": MrpcProcessor,\n",
    "    # ä½¿ç”¨å®˜æ–¹ä»£ç åº“ä¸­çš„ä»£ç \n",
    "    \"cola\": transformers.glue_processors['cola'],\n",
    "    \"mnli\": transformers.glue_processors['mnli'],\n",
    "    \"mnli-mm\": transformers.glue_processors['mnli-mm'],\n",
    "    \"sst-2\": transformers.glue_processors['sst-2'],\n",
    "    \"sts-b\": transformers.glue_processors['sts-b'],\n",
    "    \"qqp\": transformers.glue_processors['qqp'],\n",
    "    \"qnli\": transformers.glue_processors['qnli'],\n",
    "    \"rte\": transformers.glue_processors['rte'],\n",
    "    \"wnli\": transformers.glue_processors['wnli'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†examplesè½¬æ¢æˆfeatures\n",
    "def glue_convert_examples_to_features(\n",
    "    examples: List[transformers.InputExample],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_length: Optional[int] = None,\n",
    "    task=None,\n",
    "    label_list=None,\n",
    "    output_mode=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of ``InputFeatures``\n",
    "\n",
    "    Args:\n",
    "        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
    "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
    "        max_length: Maximum example length. Defaults to the tokenizer's max_len\n",
    "        task: GLUE task\n",
    "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
    "        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n",
    "\n",
    "    Returns:\n",
    "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
    "        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n",
    "        a list of task-specific ``InputFeatures`` which can be fed to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = tokenizer.max_len\n",
    "\n",
    "    if task is not None:\n",
    "        processor = glue_processors[task]()\n",
    "        if label_list is None:\n",
    "            label_list = processor.get_labels()\n",
    "            logger.info(\"Using label list %s for task %s\" % (label_list, task))\n",
    "        if output_mode is None:\n",
    "            output_mode = glue_output_modes[task]\n",
    "            logger.info(\"Using output mode %s for task %s\" % (output_mode, task))\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    def label_from_example(example: transformers.InputExample) -> Union[int, float, None]:\n",
    "        if example.label is None:\n",
    "            return None\n",
    "        if output_mode == \"classification\":\n",
    "            return label_map[example.label]\n",
    "        elif output_mode == \"regression\":\n",
    "            return float(example.label)\n",
    "        raise KeyError(output_mode)\n",
    "\n",
    "    labels = [label_from_example(example) for example in examples]\n",
    "\n",
    "    batch_encoding = tokenizer(\n",
    "        [(example.text_a, example.text_b) for example in examples],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "    for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "\n",
    "        feature = transformers.InputFeatures(**inputs, label=labels[i])\n",
    "        features.append(feature)\n",
    "\n",
    "    for i, example in enumerate(examples[:5]):\n",
    "        logger.info(\"*** Example ***\")\n",
    "        logger.info(\"guid: %s\" % (example.guid))\n",
    "        logger.info(\"features: %s\" % features[i])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(enum.Enum):\n",
    "    train = \"train\"\n",
    "    dev = \"dev\"\n",
    "    test = \"test\"\n",
    "    \n",
    "    \n",
    "class GlueDataset(torch.utils.data.dataset.Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    args: GlueDataTrainingArguments\n",
    "    output_mode: str\n",
    "    features: List[transformers.InputFeatures]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: transformers.GlueDataTrainingArguments,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        limit_length: Optional[int] = None,\n",
    "        mode: Union[str, Split] = Split.train,\n",
    "        cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.processor = glue_processors[args.task_name]()\n",
    "        self.output_mode = glue_output_modes[args.task_name]\n",
    "        if isinstance(mode, str):\n",
    "            try:\n",
    "                mode = Split[mode]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"mode is not a valid split name\")\n",
    "        # Load data features from cache or dataset file\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else args.data_dir,\n",
    "            \"cached_{}_{}_{}_{}\".format(\n",
    "                mode.value, tokenizer.__class__.__name__, str(args.max_seq_length), args.task_name,\n",
    "            ),\n",
    "        )\n",
    "        label_list = self.processor.get_labels()\n",
    "        if args.task_name in [\"mnli\", \"mnli-mm\"] and tokenizer.__class__ in (\n",
    "            RobertaTokenizer,\n",
    "            RobertaTokenizerFast,\n",
    "            XLMRobertaTokenizer,\n",
    "            BartTokenizer,\n",
    "            BartTokenizerFast,\n",
    "        ):\n",
    "            # HACK(label indices are swapped in RoBERTa pretrained model)\n",
    "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
    "        self.label_list = label_list\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with filelock.FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "                start = time.time()\n",
    "                self.features = torch.load(cached_features_file)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {args.data_dir}\")\n",
    "\n",
    "                if mode == Split.dev:\n",
    "                    examples = self.processor.get_dev_examples(args.data_dir)\n",
    "                elif mode == Split.test:\n",
    "                    examples = self.processor.get_test_examples(args.data_dir)\n",
    "                else:\n",
    "                    examples = self.processor.get_train_examples(args.data_dir)\n",
    "                if limit_length is not None:\n",
    "                    examples = examples[:limit_length]\n",
    "                self.features = glue_convert_examples_to_features(\n",
    "                    examples,\n",
    "                    tokenizer,\n",
    "                    max_length=args.max_seq_length,\n",
    "                    label_list=label_list,\n",
    "                    output_mode=self.output_mode,\n",
    "                )\n",
    "                start = time.time()\n",
    "                torch.save(self.features, cached_features_file)\n",
    "                # ^ This seems to take a lot of time so I want to investigate why and how we can improve.\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> transformers.InputFeatures:\n",
    "        return self.features[i]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/24/2020 16:54:29 - INFO - filelock -   Lock 139787600255688 acquired on /dfsdata2/yucc1_data/datasets/glue_data/MRPC/cached_train_BertTokenizer_128_mrpc.lock\n",
      "07/24/2020 16:54:29 - INFO - __main__ -   Creating features from dataset file at /dfsdata2/yucc1_data/datasets/glue_data/MRPC\n",
      "07/24/2020 16:54:29 - INFO - __main__ -   LOOKING AT /dfsdata2/yucc1_data/datasets/glue_data/MRPC/train.tsv\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   *** Example ***\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   guid: train-1\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   *** Example ***\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   guid: train-2\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   *** Example ***\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   guid: train-3\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102, 2006, 2238, 2184, 1010, 1996, 2911, 1005, 1055, 5608, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 1010, 5378, 1996, 14792, 2005, 5096, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   *** Example ***\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   guid: train-4\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 2105, 6021, 19481, 13938, 2102, 1010, 21628, 6661, 2020, 2039, 2539, 16653, 1010, 2030, 1018, 1012, 1018, 1003, 1010, 2012, 1037, 1002, 1018, 1012, 5179, 1010, 2383, 3041, 2275, 1037, 2501, 2152, 1997, 1037, 1002, 1018, 1012, 5401, 1012, 102, 21628, 6661, 5598, 2322, 16653, 1010, 2030, 1018, 1012, 1020, 1003, 1010, 2000, 2275, 1037, 2501, 5494, 2152, 2012, 1037, 1002, 1018, 1012, 5401, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   *** Example ***\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   guid: train-5\n",
      "07/24/2020 16:54:32 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 1996, 4518, 3123, 1002, 1016, 1012, 2340, 1010, 2030, 2055, 2340, 3867, 1010, 2000, 2485, 5958, 2012, 1002, 2538, 1012, 4868, 2006, 1996, 2047, 2259, 4518, 3863, 1012, 102, 18720, 1004, 1041, 13058, 1012, 6661, 5598, 1002, 1015, 1012, 6191, 2030, 1022, 3867, 2000, 1002, 2538, 1012, 6021, 2006, 1996, 2047, 2259, 4518, 3863, 2006, 5958, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   Saving features into cached file /dfsdata2/yucc1_data/datasets/glue_data/MRPC/cached_train_BertTokenizer_128_mrpc [took 0.820 s]\n",
      "07/24/2020 16:54:33 - INFO - filelock -   Lock 139787600255688 released on /dfsdata2/yucc1_data/datasets/glue_data/MRPC/cached_train_BertTokenizer_128_mrpc.lock\n",
      "07/24/2020 16:54:33 - INFO - filelock -   Lock 139786857983784 acquired on /dfsdata2/yucc1_data/datasets/glue_data/MRPC/cached_dev_BertTokenizer_128_mrpc.lock\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   Creating features from dataset file at /dfsdata2/yucc1_data/datasets/glue_data/MRPC\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   *** Example ***\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   guid: dev-1\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 2002, 2056, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2987, 1005, 1056, 4906, 1996, 2194, 1005, 1055, 2146, 1011, 2744, 3930, 5656, 1012, 102, 1000, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2515, 2025, 4906, 2256, 2146, 1011, 2744, 3930, 5656, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   *** Example ***\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   guid: dev-2\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 20201, 22948, 2056, 10958, 19053, 4140, 6283, 1996, 8956, 6939, 1998, 2246, 2830, 2000, 2478, 2010, 2146, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 2010, 2564, 2056, 2002, 2001, 1000, 2531, 3867, 2369, 2577, 5747, 1000, 1998, 2246, 2830, 2000, 2478, 2010, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   *** Example ***\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   guid: dev-3\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 1996, 7922, 2001, 2012, 12904, 1012, 6227, 18371, 2114, 1996, 18371, 1010, 4257, 2006, 1996, 5219, 1010, 1998, 2012, 1015, 1012, 27054, 2487, 2114, 1996, 5364, 23151, 2278, 1010, 2036, 4257, 1012, 102, 1996, 7922, 2001, 2012, 12904, 1012, 6275, 18371, 16545, 2100, 1027, 1010, 8990, 4257, 2006, 1996, 5219, 1010, 1998, 2012, 1015, 1012, 23090, 2487, 2114, 1996, 5364, 23151, 2278, 10381, 2546, 1027, 1010, 2091, 1014, 1012, 1015, 3867, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   *** Example ***\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   guid: dev-4\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 1996, 10028, 1011, 25022, 2080, 2003, 3403, 2127, 2255, 2000, 5630, 2065, 2009, 2097, 2203, 5668, 2063, 1037, 4018, 1012, 102, 1996, 10028, 1011, 25022, 2080, 2623, 9317, 2008, 2009, 2097, 5630, 1999, 2255, 3251, 2000, 2203, 5668, 2063, 1037, 4018, 2077, 1996, 27419, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   *** Example ***\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   guid: dev-5\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 2053, 5246, 2031, 2042, 2275, 2005, 1996, 2942, 2030, 1996, 4735, 3979, 1012, 102, 2053, 5246, 2031, 2042, 2275, 2005, 1996, 4735, 2030, 2942, 3572, 1010, 2021, 17137, 3051, 2038, 12254, 2025, 5905, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/24/2020 16:54:33 - INFO - __main__ -   Saving features into cached file /dfsdata2/yucc1_data/datasets/glue_data/MRPC/cached_dev_BertTokenizer_128_mrpc [took 0.102 s]\n",
      "07/24/2020 16:54:33 - INFO - filelock -   Lock 139786857983784 released on /dfsdata2/yucc1_data/datasets/glue_data/MRPC/cached_dev_BertTokenizer_128_mrpc.lock\n"
     ]
    }
   ],
   "source": [
    "# è·å¾—dataset\n",
    "train_dataset = (\n",
    "    GlueDataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir) if training_args.do_train else None\n",
    ")\n",
    "eval_dataset = (\n",
    "    GlueDataset(data_args, tokenizer=tokenizer, mode=\"dev\", cache_dir=model_args.cache_dir)\n",
    "    if training_args.do_eval\n",
    "    else None\n",
    ")\n",
    "test_dataset = (\n",
    "    GlueDataset(data_args, tokenizer=tokenizer, mode=\"test\", cache_dir=model_args.cache_dir)\n",
    "    if training_args.do_predict\n",
    "    else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalPrediction(NamedTuple):\n",
    "    \"\"\"\n",
    "    Evaluation output (always contains labels), to be used to compute metrics.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (:obj:`np.ndarray`): Predictions of the model.\n",
    "        label_ids (:obj:`np.ndarray`): Targets to be matched.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions: np.ndarray\n",
    "    label_ids: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"acc_and_f1\": (acc + f1) / 2,\n",
    "    }\n",
    "\n",
    "def pearson_and_spearman(preds, labels):\n",
    "    pearson_corr = pearsonr(preds, labels)[0]\n",
    "    spearman_corr = spearmanr(preds, labels)[0]\n",
    "    return {\n",
    "        \"pearson\": pearson_corr,\n",
    "        \"spearmanr\": spearman_corr,\n",
    "        \"corr\": (pearson_corr + spearman_corr) / 2,\n",
    "    }\n",
    "\n",
    "def glue_compute_metrics(task_name, preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    if task_name == \"cola\":\n",
    "        return {\"mcc\": matthews_corrcoef(labels, preds)}\n",
    "    elif task_name == \"sst-2\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    elif task_name == \"mrpc\":\n",
    "        return acc_and_f1(preds, labels)\n",
    "    elif task_name == \"sts-b\":\n",
    "        return pearson_and_spearman(preds, labels)\n",
    "    elif task_name == \"qqp\":\n",
    "        return acc_and_f1(preds, labels)\n",
    "    elif task_name == \"mnli\":\n",
    "        return {\"mnli/acc\": simple_accuracy(preds, labels)}\n",
    "    elif task_name == \"mnli-mm\":\n",
    "        return {\"mnli-mm/acc\": simple_accuracy(preds, labels)}\n",
    "    elif task_name == \"qnli\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    elif task_name == \"rte\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    elif task_name == \"wnli\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    elif task_name == \"hans\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    else:\n",
    "        raise KeyError(task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾—åˆ°è®¡ç®—å‡½æ•°\n",
    "def build_compute_metrics_fn(task_name: str) -> Callable[[EvalPrediction], Dict]:\n",
    "    def compute_metrics_fn(p: EvalPrediction):\n",
    "        if output_mode == \"classification\":\n",
    "            preds = np.argmax(p.predictions, axis=1)\n",
    "        elif output_mode == \"regression\":\n",
    "            preds = np.squeeze(p.predictions)\n",
    "        return glue_compute_metrics(task_name, preds, p.label_ids)\n",
    "\n",
    "    return compute_metrics_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/24/2020 16:55:06 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"
     ]
    }
   ],
   "source": [
    "#åˆå§‹åŒ–æœ¬Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=build_compute_metrics_fn(data_args.task_name),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/24/2020 16:55:06 - INFO - transformers.trainer -   ***** Running training *****\n",
      "07/24/2020 16:55:06 - INFO - transformers.trainer -     Num examples = 3668\n",
      "07/24/2020 16:55:06 - INFO - transformers.trainer -     Num Epochs = 3\n",
      "07/24/2020 16:55:06 - INFO - transformers.trainer -     Instantaneous batch size per device = 32\n",
      "07/24/2020 16:55:06 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "07/24/2020 16:55:06 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "07/24/2020 16:55:06 - INFO - transformers.trainer -     Total optimization steps = 174\n",
      "07/24/2020 16:55:06 - INFO - transformers.trainer -     Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87af3bffbc1b467d98fb2788705160e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f46a716c8214573b26f1e9a3e39100f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=58.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05587fa8638497db9d3bee45f9c170e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=58.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6220b7d5372849918d0d4eec909ba010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=58.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/24/2020 16:56:12 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/24/2020 16:56:12 - INFO - transformers.trainer -   Saving model checkpoint to /dfsdata2/yucc1_data/output/finetuned-mrpc\n",
      "07/24/2020 16:56:13 - INFO - transformers.configuration_utils -   Configuration saved in /dfsdata2/yucc1_data/output/finetuned-mrpc/config.json\n",
      "07/24/2020 16:56:13 - INFO - transformers.modeling_utils -   Model weights saved in /dfsdata2/yucc1_data/output/finetuned-mrpc/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒ\n",
    "if training_args.do_train:\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    trainer.save_model()\n",
    "    # For convenience, we also re-save the tokenizer to the same directory,\n",
    "    # so that you can share your model easily on huggingface.co/models =)\n",
    "    if trainer.is_world_master():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/24/2020 16:56:14 - INFO - __main__ -   *** Evaluate ***\n",
      "07/24/2020 16:56:14 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "07/24/2020 16:56:14 - INFO - transformers.trainer -     Num examples = 408\n",
      "07/24/2020 16:56:14 - INFO - transformers.trainer -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a98c16b3faa47749d51866a348ca472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=26.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/24/2020 16:56:15 - INFO - transformers.trainer -   {'eval_loss': 0.4694334108095903, 'eval_acc': 0.8112745098039216, 'eval_f1': 0.8718801996672212, 'eval_acc_and_f1': 0.8415773547355714, 'epoch': 3.0, 'step': 174}\n",
      "07/24/2020 16:56:15 - INFO - __main__ -   ***** Eval results mrpc *****\n",
      "07/24/2020 16:56:15 - INFO - __main__ -     eval_loss = 0.4694334108095903\n",
      "07/24/2020 16:56:15 - INFO - __main__ -     eval_acc = 0.8112745098039216\n",
      "07/24/2020 16:56:15 - INFO - __main__ -     eval_f1 = 0.8718801996672212\n",
      "07/24/2020 16:56:15 - INFO - __main__ -     eval_acc_and_f1 = 0.8415773547355714\n",
      "07/24/2020 16:56:15 - INFO - __main__ -     epoch = 3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°ç»“æœ\n",
    "eval_results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_datasets = [eval_dataset]\n",
    "    if data_args.task_name == \"mnli\":\n",
    "        mnli_mm_data_args = dataclasses.replace(data_args, task_name=\"mnli-mm\")\n",
    "        eval_datasets.append(\n",
    "            GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, mode=\"dev\", cache_dir=model_args.cache_dir)\n",
    "        )\n",
    "\n",
    "    for eval_dataset in eval_datasets:\n",
    "        trainer.compute_metrics = build_compute_metrics_fn(eval_dataset.args.task_name)\n",
    "        eval_result = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "        output_eval_file = os.path.join(\n",
    "            training_args.output_dir, f\"eval_results_{eval_dataset.args.task_name}.txt\"\n",
    "        )\n",
    "        if trainer.is_world_master():\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                logger.info(\"***** Eval results {} *****\".format(eval_dataset.args.task_name))\n",
    "                for key, value in eval_result.items():\n",
    "                    logger.info(\"  %s = %s\", key, value)\n",
    "                    writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        eval_results.update(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„æµ‹\n",
    "if training_args.do_predict:\n",
    "    logging.info(\"*** Test ***\")\n",
    "    test_datasets = [test_dataset]\n",
    "    if data_args.task_name == \"mnli\":\n",
    "        mnli_mm_data_args = dataclasses.replace(data_args, task_name=\"mnli-mm\")\n",
    "        test_datasets.append(\n",
    "            GlueDataset(mnli_mm_data_args, tokenizer=tokenizer, mode=\"test\", cache_dir=model_args.cache_dir)\n",
    "        )\n",
    "\n",
    "    for test_dataset in test_datasets:\n",
    "        predictions = trainer.predict(test_dataset=test_dataset).predictions\n",
    "        if output_mode == \"classification\":\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "        output_test_file = os.path.join(\n",
    "            training_args.output_dir, f\"test_results_{test_dataset.args.task_name}.txt\"\n",
    "        )\n",
    "        if trainer.is_world_master():\n",
    "            with open(output_test_file, \"w\") as writer:\n",
    "                logger.info(\"***** Test results {} *****\".format(test_dataset.args.task_name))\n",
    "                writer.write(\"index\\tprediction\\n\")\n",
    "                for index, item in enumerate(predictions):\n",
    "                    if output_mode == \"regression\":\n",
    "                        writer.write(\"%d\\t%3.3f\\n\" % (index, item))\n",
    "                    else:\n",
    "                        item = test_dataset.get_labels()[item]\n",
    "                        writer.write(\"%d\\t%s\\n\" % (index, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4694334108095903, 'eval_acc': 0.8112745098039216, 'eval_f1': 0.8718801996672212, 'eval_acc_and_f1': 0.8415773547355714, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 transformersä½¿ç”¨æ€»ç»“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŸºäºtransformersæ¡†æ¶ï¼Œå†™ä»£ç ä¼šéå¸¸ç®€å•ï¼Œæ€»ä½“æ­¥éª¤ä¹Ÿéå¸¸å°‘ã€‚\n",
    "\n",
    "1. å‚æ•°çš„ä¼ å…¥ï¼Œå‚æ•°åŒ…å«ä¸‰ç§ç±»å‹çš„å‚æ•°ï¼šæ¨¡å‹çš„å‚æ•°ï¼Œæ¨¡å‹è®­ç»ƒçš„å‚æ•°ï¼Œæ•°æ®çš„å‚æ•°ã€‚\n",
    "2. æ•°æ®çš„å¤„ç†ï¼Œä»åŸå§‹æ•°æ®åˆ°æ¨¡å‹å¯æ¥å—çš„æ•°æ®ï¼Œæœ€ç»ˆæ˜¯å°†æ•°æ®åˆ†æˆå¯ä»¥è¿­ä»£çš„ï¼Œå®šé•¿çš„ã€æ ‡å‡†çš„batchï¼Œä¾›æ¨¡å‹ä½¿ç”¨ã€‚è¿™é‡Œé¢é€šå¸¸æ ‡å‡†åŒ–ä¸ºå‡ ä¸ªæ­¥éª¤ï¼Œè¿™ä¹Ÿæ˜¯pytorchä»£ç ç¼–å†™çš„æµç¨‹ï¼š\n",
    "\n",
    "ç¬¬ä¸€æ­¥ï¼Œè®¾ç«‹processorï¼Œå®ŒæˆåŠ è½½æ•°æ®é›†ä¸ºexamplesçš„é›†å’Œï¼ˆlistï¼‰ï¼Œæ¯ä¸ªexampleå¯ä»¥ä¸ºä¸€ä¸ªInputExampleå¯¹è±¡ã€‚è¿™é‡Œé€šå¸¸è¿˜æ˜¯æ–‡æœ¬ï¼Œåªæ˜¯text_a, text_bï¼Œlabelå˜ä¸ºInputExampleçš„å±æ€§ã€‚\n",
    "\n",
    "ç¬¬äºŒæ­¥ï¼Œè½¬æ¢ä¸ºtorch.utils.data.Datasetï¼Œpytorchçš„Datasetæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œæˆ‘ä»¬éœ€è¦é‡å†™__len__,\\_\\_getitem__è¿™ä¸¤ä¸ªæ–¹æ³•ã€‚æœ€ç»ˆçš„ç›®æ ‡æ˜¯å°†æ–‡æœ¬å˜æˆä¸€ä¸ªä¸ªæ¨¡å‹å¯ä»¥ç”¨çš„ä¸€æ¡æ¡çš„æ•°æ®ã€‚åœ¨è¿™é‡Œå°†æ¯ä¸ªInputExampleè½¬æ¢æˆfeatureï¼Œè€ŒInputExampleè½¬æ¢æˆfeatureçš„æ—¶å€™ï¼Œå°±éœ€è¦tokenizerå‘æŒ¥ä½œç”¨äº†ï¼Œè¿™é‡Œå¯ä»¥è¿›è¡Œpaddingå’Œtruncationã€‚\n",
    "\n",
    "ç¬¬ä¸‰æ­¥ï¼Œåœ¨transformersé‡Œï¼Œå¯ä»¥å°†ä¸Šä¸€æ­¥å¾—åˆ°çš„datasetä¼ å…¥Trainerè¿›è¡Œä½¿ç”¨ï¼ŒTrainerä¼šè‡ªåŠ¨å¤„ç†æ­¤æ­¥éª¤ï¼Œä¸å¿…è¿‡åˆ†æ“å¿ƒé‡Œé¢çš„äº‹æƒ…ã€‚å¯¹äºé€šå¸¸çš„pytorchæ¥è¯´ï¼Œæ­¤æ­¥éª¤æ˜¯å°†datasetè½¬æ¢ä¸ºdataloaderï¼Œè¦å†³å®šbatch_sizeï¼Œæ˜¯å¦shuffleï¼ŒæŠ½æ ·æ–¹æ³•ï¼Œcollate_fnèšåˆæ–¹å¼ã€‚å¯¹äºtransformersé‡Œï¼Œcollate_fnå¯ä»¥ä½¿ç”¨é»˜è®¤æˆ–è€…è‡ªå·±å†™çš„ä¼ å…¥ï¼Œå¯ä»¥å°†batch_sizeé€šè¿‡1çš„å‚æ•°ä¼ å…¥ã€‚\n",
    "\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n",
    "\n",
    "3. å®šä¹‰metricsç”¨æ¥è¯„ä¼°é¢„æµ‹çš„æ•ˆæœã€‚å¦‚æœæœ‰éœ€è¦ï¼Œä¹Ÿéœ€è¦å†™collate_fnèšåˆå‡½æ•°ã€‚\n",
    "4. å¯¹äºå·²ç»å¤„ç†å¥½çš„æ•°æ®train_datasetã€dev_datasetã€test_datasetï¼Œè¿åŒmodelã€tokenizerã€metricã€argsä¸€åŒé€å…¥trainerï¼Œè¿›è¡Œè®­ç»ƒï¼Œä¿å­˜ã€‚è¿™é‡Œä½¿ç”¨çš„æ˜¯save_modelï¼Œå…¶å®å°±æ˜¯è°ƒç”¨çš„æ ‡å‡†çš„save_pretrainedçš„æ¥å£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äº”èŠ‚ GPT2è®­ç»ƒä½¿ç”¨ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 ä»å¤´é¢„è®­ç»ƒæ¨¡å‹ä¸æ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "ä»¥GPT2ä¸ºä¾‹ï¼Œç¤ºèŒƒå¦‚ä½•ä½¿ç”¨ä»å¤´è¿›è¡Œé¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŠå¦‚ä½•ä½¿ç”¨GPT2è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼ŒåŒæ—¶æ›´åŠ è¯¦ç»†çš„é¢„è®­ç»ƒä»‹ç»ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ç›´æ¥ä»å¤´è¿›è¡Œé¢„è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_LANGUAGE_MODEL_PY = ('/dfsdata2/yucc1_data/projects/transformers_study/'\n",
    "    'transformers/examples/language-modeling/run_language_modeling.py')\n",
    "# config, tokenizer, model\n",
    "CONFIG_NAME = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "TOKENIZER_NAME = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "# ä»å¤´è¿›è¡Œé¢„è®­ç»ƒï¼Œä¸éœ€è¦æŒ‡å®šæ­¤å‚æ•°ï¼›ç»§ç»­finetuneï¼Œéœ€è¦æŒ‡å®šåŸå§‹çš„gpt2æ¨¡å‹æ‰€åœ¨ä½ç½®\n",
    "GPT2_MODEL_NAME_OR_PATH = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "# ç”Ÿæˆçš„æ¨¡å‹\n",
    "GPT2_OUTPUT_DIR = '/dfsdata2/yucc1_data/output/gpt2-train-new-model'\n",
    "# TRAIN_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.train.raw'\n",
    "TRAIN_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.test.raw'\n",
    "EVAL_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.test.raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹æ˜¯é‡å¤´é¢„è®­ç»ƒçš„ä»£ç ï¼Œå¦‚æœåªæ˜¯finetuneï¼ŒåŠ å…¥ä¸€é¢ä¸€è¡Œä»£ç å³å¯ï¼š\n",
    "\n",
    "--model_name_or_path={GPT2_MODEL_NAME_OR_PATH} \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/24/2020 17:34:26 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/24/2020 17:34:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "07/24/2020 17:34:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/gpt2-train-new-model', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul24_17-34-26_d585a65fe6e2', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n",
      "07/24/2020 17:34:26 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/models/huggingface/gpt2/config.json\n",
      "07/24/2020 17:34:26 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/24/2020 17:34:26 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/models/huggingface/gpt2/config.json\n",
      "07/24/2020 17:34:26 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   Model name '/dfsdata2/yucc1_data/models/huggingface/gpt2' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/dfsdata2/yucc1_data/models/huggingface/gpt2' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/added_tokens.json. We won't load it.\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/special_tokens_map.json. We won't load it.\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/tokenizer_config.json. We won't load it.\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/tokenizer.json. We won't load it.\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/models/huggingface/gpt2/vocab.json\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/models/huggingface/gpt2/merges.txt\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/24/2020 17:34:26 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/24/2020 17:34:27 - INFO - __main__ -   Training new model from scratch\n",
      "/dfsdata2/yucc1_data/projects/transformers_study/transformers/src/transformers/modeling_auto.py:716: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "07/24/2020 17:34:30 - INFO - filelock -   Lock 139999647184824 acquired on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw.lock\n",
      "07/24/2020 17:34:30 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at /dfsdata2/yucc1_data/datasets/wikitext-2-raw\n",
      "07/24/2020 17:34:33 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw [took 0.052 s]\n",
      "07/24/2020 17:34:33 - INFO - filelock -   Lock 139999647184824 released on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw.lock\n",
      "07/24/2020 17:34:33 - INFO - filelock -   Lock 139998622998712 acquired on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw.lock\n",
      "07/24/2020 17:34:33 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at /dfsdata2/yucc1_data/datasets/wikitext-2-raw\n",
      "07/24/2020 17:34:34 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw [took 0.021 s]\n",
      "07/24/2020 17:34:34 - INFO - filelock -   Lock 139998622998712 released on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw.lock\n",
      "07/24/2020 17:34:38 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "07/24/2020 17:34:38 - INFO - transformers.trainer -   ***** Running training *****\n",
      "07/24/2020 17:34:38 - INFO - transformers.trainer -     Num examples = 561\n",
      "07/24/2020 17:34:38 - INFO - transformers.trainer -     Num Epochs = 2\n",
      "07/24/2020 17:34:38 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "07/24/2020 17:34:38 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "07/24/2020 17:34:38 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "07/24/2020 17:34:38 - INFO - transformers.trainer -     Total optimization steps = 72\n",
      "Epoch:   0%|                                              | 0/2 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                         | 0/36 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "Iteration:   3%|â–‰                                | 1/36 [00:04<02:39,  4.55s/it]\u001b[A\n",
      "Iteration:   6%|â–ˆâ–Š                               | 2/36 [00:05<01:54,  3.38s/it]\u001b[A\n",
      "Iteration:   8%|â–ˆâ–ˆâ–Š                              | 3/36 [00:05<01:24,  2.56s/it]\u001b[A\n",
      "Iteration:  11%|â–ˆâ–ˆâ–ˆâ–‹                             | 4/36 [00:06<01:03,  1.98s/it]\u001b[A\n",
      "Iteration:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 5/36 [00:07<00:49,  1.58s/it]\u001b[A\n",
      "Iteration:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 6/36 [00:07<00:38,  1.30s/it]\u001b[A\n",
      "Iteration:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 7/36 [00:08<00:31,  1.10s/it]\u001b[A\n",
      "Iteration:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 8/36 [00:09<00:27,  1.03it/s]\u001b[A\n",
      "Iteration:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 9/36 [00:09<00:23,  1.15it/s]\u001b[A\n",
      "Iteration:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 10/36 [00:10<00:20,  1.24it/s]\u001b[A\n",
      "Iteration:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 11/36 [00:10<00:18,  1.32it/s]\u001b[A\n",
      "Iteration:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 12/36 [00:11<00:17,  1.39it/s]\u001b[A\n",
      "Iteration:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 13/36 [00:12<00:16,  1.43it/s]\u001b[A\n",
      "Iteration:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 14/36 [00:12<00:14,  1.47it/s]\u001b[A\n",
      "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 15/36 [00:13<00:14,  1.50it/s]\u001b[A\n",
      "Iteration:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 16/36 [00:14<00:13,  1.52it/s]\u001b[A\n",
      "Iteration:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 17/36 [00:14<00:12,  1.53it/s]\u001b[A\n",
      "Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 18/36 [00:15<00:11,  1.53it/s]\u001b[A\n",
      "Iteration:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 19/36 [00:16<00:11,  1.54it/s]\u001b[A\n",
      "Iteration:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 20/36 [00:16<00:10,  1.54it/s]\u001b[A\n",
      "Iteration:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 21/36 [00:17<00:09,  1.55it/s]\u001b[A\n",
      "Iteration:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 22/36 [00:18<00:09,  1.55it/s]\u001b[A\n",
      "Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 23/36 [00:18<00:08,  1.55it/s]\u001b[A\n",
      "Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 24/36 [00:19<00:07,  1.55it/s]\u001b[A\n",
      "Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 25/36 [00:20<00:07,  1.55it/s]\u001b[A\n",
      "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 26/36 [00:20<00:06,  1.55it/s]\u001b[A\n",
      "Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 27/36 [00:21<00:05,  1.55it/s]\u001b[A\n",
      "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 28/36 [00:21<00:05,  1.55it/s]\u001b[A\n",
      "Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 29/36 [00:22<00:04,  1.55it/s]\u001b[A\n",
      "Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/36 [00:23<00:03,  1.55it/s]\u001b[A\n",
      "Iteration:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 31/36 [00:23<00:03,  1.56it/s]\u001b[A\n",
      "Iteration:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/36 [00:24<00:02,  1.56it/s]\u001b[A\n",
      "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 33/36 [00:25<00:01,  1.56it/s]\u001b[A\n",
      "Iteration:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 34/36 [00:25<00:01,  1.56it/s]\u001b[A\n",
      "Iteration:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 35/36 [00:26<00:00,  1.56it/s]\u001b[A\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:26<00:00,  1.36it/s]\u001b[A\n",
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 1/2 [00:26<00:26, 26.56s/it]\n",
      "Iteration:   0%|                                         | 0/36 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   3%|â–‰                                | 1/36 [00:00<00:22,  1.56it/s]\u001b[A\n",
      "Iteration:   6%|â–ˆâ–Š                               | 2/36 [00:01<00:21,  1.56it/s]\u001b[A\n",
      "Iteration:   8%|â–ˆâ–ˆâ–Š                              | 3/36 [00:01<00:21,  1.56it/s]\u001b[A\n",
      "Iteration:  11%|â–ˆâ–ˆâ–ˆâ–‹                             | 4/36 [00:02<00:20,  1.56it/s]\u001b[A\n",
      "Iteration:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 5/36 [00:03<00:19,  1.55it/s]\u001b[A\n",
      "Iteration:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 6/36 [00:03<00:19,  1.55it/s]\u001b[A\n",
      "Iteration:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 7/36 [00:04<00:18,  1.55it/s]\u001b[A\n",
      "Iteration:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 8/36 [00:05<00:18,  1.55it/s]\u001b[A\n",
      "Iteration:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 9/36 [00:05<00:17,  1.55it/s]\u001b[A\n",
      "Iteration:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 10/36 [00:06<00:16,  1.55it/s]\u001b[A\n",
      "Iteration:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 11/36 [00:07<00:16,  1.55it/s]\u001b[A\n",
      "Iteration:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 12/36 [00:07<00:15,  1.55it/s]\u001b[A\n",
      "Iteration:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 13/36 [00:08<00:14,  1.55it/s]\u001b[A\n",
      "Iteration:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 14/36 [00:09<00:14,  1.55it/s]\u001b[A\n",
      "Iteration:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 15/36 [00:09<00:13,  1.55it/s]\u001b[A\n",
      "Iteration:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 16/36 [00:10<00:12,  1.55it/s]\u001b[A\n",
      "Iteration:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 17/36 [00:10<00:12,  1.54it/s]\u001b[A\n",
      "Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 18/36 [00:11<00:11,  1.55it/s]\u001b[A\n",
      "Iteration:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 19/36 [00:12<00:10,  1.55it/s]\u001b[A\n",
      "Iteration:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 20/36 [00:12<00:10,  1.54it/s]\u001b[A\n",
      "Iteration:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 21/36 [00:13<00:09,  1.55it/s]\u001b[A\n",
      "Iteration:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 22/36 [00:14<00:09,  1.54it/s]\u001b[A\n",
      "Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 23/36 [00:14<00:08,  1.54it/s]\u001b[A\n",
      "Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 24/36 [00:15<00:07,  1.54it/s]\u001b[A\n",
      "Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 25/36 [00:16<00:07,  1.55it/s]\u001b[A\n",
      "Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 26/36 [00:16<00:06,  1.54it/s]\u001b[A\n",
      "Iteration:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 27/36 [00:17<00:05,  1.55it/s]\u001b[A\n",
      "Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 28/36 [00:18<00:05,  1.55it/s]\u001b[A\n",
      "Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 29/36 [00:18<00:04,  1.54it/s]\u001b[A\n",
      "Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/36 [00:19<00:03,  1.54it/s]\u001b[A\n",
      "Iteration:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 31/36 [00:20<00:03,  1.54it/s]\u001b[A\n",
      "Iteration:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/36 [00:20<00:02,  1.54it/s]\u001b[A\n",
      "Iteration:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 33/36 [00:21<00:01,  1.55it/s]\u001b[A\n",
      "Iteration:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 34/36 [00:21<00:01,  1.55it/s]\u001b[A\n",
      "Iteration:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 35/36 [00:22<00:00,  1.55it/s]\u001b[A\n",
      "Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:22<00:00,  1.58it/s]\u001b[A\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:49<00:00, 24.64s/it]\n",
      "07/24/2020 17:35:28 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "07/24/2020 17:35:28 - INFO - transformers.trainer -   Saving model checkpoint to /dfsdata2/yucc1_data/output/gpt2-train-new-model\n",
      "07/24/2020 17:35:28 - INFO - transformers.configuration_utils -   Configuration saved in /dfsdata2/yucc1_data/output/gpt2-train-new-model/config.json\n",
      "07/24/2020 17:35:29 - INFO - transformers.modeling_utils -   Model weights saved in /dfsdata2/yucc1_data/output/gpt2-train-new-model/pytorch_model.bin\n",
      "07/24/2020 17:35:29 - INFO - __main__ -   *** Evaluate ***\n",
      "07/24/2020 17:35:29 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "07/24/2020 17:35:29 - INFO - transformers.trainer -     Num examples = 561\n",
      "07/24/2020 17:35:29 - INFO - transformers.trainer -     Batch size = 16\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:10<00:00,  3.36it/s]\n",
      "07/24/2020 17:35:40 - INFO - transformers.trainer -   {'eval_loss': 8.00778447257148, 'epoch': 2.0, 'step': 72}\n",
      "07/24/2020 17:35:40 - INFO - __main__ -   ***** Eval results *****\n",
      "07/24/2020 17:35:40 - INFO - __main__ -     perplexity = 3004.2537276158396\n"
     ]
    }
   ],
   "source": [
    "!python {RUN_LANGUAGE_MODEL_PY} \\\n",
    "--output_dir={GPT2_OUTPUT_DIR} \\\n",
    "--model_type=gpt2 \\\n",
    "--config_name={CONFIG_NAME} \\\n",
    "--tokenizer_name={TOKENIZER_NAME} \\\n",
    "--do_train \\\n",
    "--train_data_file={TRAIN_DATA_FILE} \\\n",
    "--do_eval \\\n",
    "--eval_data_file={EVAL_DATA_FILE} \\\n",
    "--block_size=510 \\\n",
    "--save_steps=5000 \\\n",
    "--num_train_epochs=2.0 \\\n",
    "--overwrite_cache \\\n",
    "--overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config, tokenizer, model\n",
    "CONFIG_NAME = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "TOKENIZER_NAME = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "# ä»å¤´è¿›è¡Œé¢„è®­ç»ƒï¼Œä¸éœ€è¦æŒ‡å®šæ­¤å‚æ•°ï¼›ç»§ç»­finetuneï¼Œéœ€è¦æŒ‡å®šåŸå§‹çš„gpt2æ¨¡å‹æ‰€åœ¨ä½ç½®\n",
    "GPT2_MODEL_NAME_OR_PATH = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "# ç”Ÿæˆçš„æ¨¡å‹\n",
    "GPT2_OUTPUT_DIR = '/dfsdata2/yucc1_data/output/gpt2-train-new-model'\n",
    "# TRAIN_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.train.raw'\n",
    "TRAIN_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.test.raw'\n",
    "EVAL_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.test.raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dfsdata2/yucc1_data/output/gpt2-train-new-model\n",
      "â”œâ”€â”€ config.json\n",
      "â”œâ”€â”€ eval_results_lm.txt\n",
      "â”œâ”€â”€ merges.txt\n",
      "â”œâ”€â”€ pytorch_model.bin\n",
      "â”œâ”€â”€ special_tokens_map.json\n",
      "â”œâ”€â”€ tokenizer_config.json\n",
      "â”œâ”€â”€ training_args.bin\n",
      "â””â”€â”€ vocab.json\n",
      "\n",
      "0 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree {GPT2_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨åˆšåˆšé¢„è®­ç»ƒçš„æ¨¡å‹\n",
    "# æœ¬æ¨¡å‹ä¸ä¸‹é¢çš„æ¨¡å‹åŠ è½½ï¼ŒäºŒé€‰ä¸€å³å¯\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(GPT2_OUTPUT_DIR)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(GPT2_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨GPT2å®˜æ–¹çš„é¢„è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(GPT2_MODEL_NAME_OR_PATH)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(GPT2_MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### æ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "æ–‡æœ¬è§£ç çš„ç­–ç•¥æœ‰ä¸¤ç±»[7]ï¼š\n",
    "\n",
    "- Argmax Decoding: ä¸»è¦åŒ…æ‹¬beam search, class-factored softmaxç­‰\n",
    "- Stochastic Decoding: ä¸»è¦åŒ…æ‹¬temperature sampling, top-k samplingç­‰\n",
    "\n",
    "åœ¨å¤§å¤šæ•°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå¤§å®¶éƒ½ç›´æ¥é‡‡ç”¨Argmax Decodingï¼Œæœ€å¸¸è§çš„å°±æ˜¯beam searchã€‚ä½†å¦‚æœæˆ‘ä»¬çš„vocabulary sizeè¾ƒå¤§ï¼Œè¾¾åˆ°äº†50kç”šè‡³150kï¼Œåœ¨softmaxå±‚çš„è¿ç®—é‡å°±ä¼šå˜å¾—éå¸¸å¤§ï¼Œå› ä¸ºè¦ç»è¿‡softmaxã€‚æœ‰ä¸¤ç§æ•ˆç‡æ›´é«˜çš„æ–¹æ³•ï¼ŒClass-factored Softmaxå’ŒPointer-generator Networkã€‚\n",
    "\n",
    "å®é™…ä¸ŠArgmax Decodingå¸¸å¸¸ä¼šå¯¼è‡´æ¨¡å‹ç”Ÿæˆé‡å¤çš„å¥å­ï¼Œå¦‚\"I don't know. I don't know. I don't know....\"ã€‚ä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆå°±æ˜¯åœ¨decodingè¿‡ç¨‹ä¸­å¼•å…¥randomnessï¼Œï¼Œä½†æ˜¯The Curious Case of Neural Text Degenerationè¿™ç¯‡è®ºæ–‡æŒ‡å‡ºï¼Œsampling from full vocabulary distributionç”Ÿæˆçš„å¥å­ä¼šéå¸¸çš„æ‚ä¹±æ— ç« ï¼Œå› ä¸ºå½“vocabulary sizeéå¸¸å¤§æ—¶ï¼Œæ¯ä¸ªè¯çš„probabilityéƒ½ä¼šå˜å¾—å¾ˆå°ï¼Œè¿™æ—¶æ¨¡å‹ä¼šæœ‰éå¸¸é«˜çš„å¯èƒ½æ€§sampleåˆ°ä¸€ä¸ªtail distributionä¸­çš„è¯ï¼Œä¸€æ—¦sampleåˆ°äº†tail distributionä¸­ä¸€ä¸ªå’Œå‰æ–‡éå¸¸ä¸ç›¸å…³çš„è¯ï¼Œå¾ˆæœ‰å¯èƒ½æ¥ä¸‹æ¥çš„è¯éƒ½å—å…¶å½±å“ï¼Œä½¿å¾—å¥å­è„±ç¦»åŸæœ¬çš„æ„æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦sampling from truncated vocabulary distributionï¼Œæ¯”è¾ƒå¸¸è§çš„ç®—æ³•ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç§ï¼š(1) Temperature Samplingï¼Œ(2) Top-k Samplingï¼Œ(3) Top-p Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decode 0: Today the weather is really nice and I am planning on Â going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going\n",
      "decode 1: Today the weather is really nice and I am planning on Â going to go out on a limb and go out on a limb and go out on a limb and go out on a limb and go out on a limb and go out on a limb and go out on a limb and go out on a limb\n",
      "decode 2: Today the weather is really nice and I am planning on Â going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb.\n",
      "\n",
      "decode 3: Today the weather is really nice and I am planning on Â going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb andÂ \n",
      "decode 4: Today the weather is really nice and I am planning on Â going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and going out on a limb and coming\n"
     ]
    }
   ],
   "source": [
    "# è§£ç ç¤ºä¾‹ä¸€ï¼š\n",
    "# åˆå§‹æ–‡æœ¬\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "# è½¬æ¢æˆtensor\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# ä½¿ç”¨beam searchç”Ÿæˆ\n",
    "beam_outputs = model.generate(input_ids=inputs,\n",
    "           max_length=50+inputs.shape[-1],\n",
    "           min_length=2+inputs.shape[-1],\n",
    "           num_beams=10,\n",
    "           num_return_sequences=5,)\n",
    "for i in range(5):\n",
    "    output_ids = beam_outputs[i].tolist()\n",
    "    text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(f'decode {i}: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on Â going to do a run in April and May this year Â to play theÂ Tibetan Cup games in Nairobi or just to play with me as a spectator for a year. I will post a few pictures shortly!\n",
      "I will be going over a few of my favourite items that were last year, but there will be many more! Â \n",
      "These items are good stuff so i won't do them up here just to show you! Â \n",
      "There are two main things I would like to see in your purchases during this tour, and you will find them on my shop, right here. Â For example, let's say you were to visit Nairobi in April, I was going to visit Nairobi and if you visited in May I would give you a discount on Nairobi tickets from the best book sellers here as well as a free lunch and drinks from Nairobi, which is great! Â Now, you will also want to see my new 3 year anniversary event to watch the cricket team play in Nairobi during the World Cup 2015 in South Africa!\n",
      "Here at Nairobi you will get everything that you need\n"
     ]
    }
   ],
   "source": [
    "# è§£ç ç¤ºä¾‹äºŒï¼š\n",
    "# åˆå§‹æ–‡æœ¬\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "# è½¬æ¢æˆtensor\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# ç»™å®štensorï¼Œç”Ÿæˆæ–°çš„tensor\n",
    "# æ­¤å¤„éœ€è¦æ³¨æ„çš„æ˜¯max_lengthåŒ…å«ç€å‰é¢çš„promptçš„é•¿åº¦ï¼Œä¹Ÿå°±æ˜¯å‰é¢çš„é•¿åº¦éå¸¸é•¿ï¼Œè¶…è¿‡250ï¼Œå°±æ— æ³•ç”Ÿæˆäº†\n",
    "# top_pæ˜¯æ¦‚ç‡ä»å¤§åˆ°å°æ’åˆ—ï¼Œæœ€å°çš„ä¸ªæ•°è¾¾åˆ°0.95ï¼Œåé¢çš„ç æ‰ä¸ä½¿ç”¨ï¼›top_kæ˜¯é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„60ä¸ªï¼Œåé¢çš„ç æ‰ä¸ä½¿ç”¨ï¼›\n",
    "# ç»¼åˆèµ·æ¥å°±æ˜¯æ¦‚ç‡ç›¸åŠ ä¸è¶…è¿‡0.95ä¸”æœ€å¤š60ä¸ªçš„å•è¯å»ç”Ÿæˆã€‚\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = tokenizer.decode(outputs[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 é¢„è®­ç»ƒçš„è¯¦ç»†è¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "import filelock\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–æ—¥å¿—\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG_CLASSES = list(transformers.MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒä¸Šé¢çš„è®­ç»ƒä¸€æ ·ï¼Œè¿™é‡Œæ˜¯æ¨¡å‹å‚æ•°ï¼Œä¸‹é¢æ˜¯æ•°æ®å‚æ•°\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    train_data_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
    "    )\n",
    "    eval_data_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    line_by_line: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
    "    )\n",
    "\n",
    "    mlm: bool = field(\n",
    "        default=False, metadata={\"help\": \"Train with masked-language modeling loss instead of language modeling.\"}\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
    "    )\n",
    "    plm_probability: float = field(\n",
    "        default=1 / 6,\n",
    "        metadata={\n",
    "            \"help\": \"Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling.\"\n",
    "        },\n",
    "    )\n",
    "    max_span_length: int = field(\n",
    "        default=5, metadata={\"help\": \"Maximum length of a span of masked tokens for permutation language modeling.\"}\n",
    "    )\n",
    "\n",
    "    block_size: int = field(\n",
    "        default=-1,\n",
    "        metadata={\n",
    "            \"help\": \"Optional input sequence length after tokenization.\"\n",
    "            \"The training dataset will be truncated in block of this size for training.\"\n",
    "            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_LANGUAGE_MODEL_PY = ('/dfsdata2/yucc1_data/projects/transformers_study/'\n",
    "    'transformers/examples/language-modeling/run_language_modeling.py')\n",
    "# config, tokenizer, model\n",
    "CONFIG_NAME = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "TOKENIZER_NAME = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "# ä»å¤´è¿›è¡Œé¢„è®­ç»ƒï¼Œä¸éœ€è¦æŒ‡å®šæ­¤å‚æ•°ï¼›ç»§ç»­finetuneï¼Œéœ€è¦æŒ‡å®šåŸå§‹çš„gpt2æ¨¡å‹æ‰€åœ¨ä½ç½®\n",
    "GPT2_MODEL_NAME_OR_PATH = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "# ç”Ÿæˆçš„æ¨¡å‹\n",
    "GPT2_OUTPUT_DIR = '/dfsdata2/yucc1_data/output/gpt2-train-new-model'\n",
    "# TRAIN_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.train.raw'\n",
    "TRAIN_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.test.raw'\n",
    "EVAL_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.test.raw'\n",
    "\n",
    "input_args = [\n",
    "    '--output_dir', GPT2_OUTPUT_DIR,\n",
    "    '--model_type', 'gpt2',\n",
    "    '--config_name', CONFIG_NAME,\n",
    "    '--tokenizer_name', TOKENIZER_NAME,\n",
    "    '--do_train',\n",
    "    '--train_data_file', TRAIN_DATA_FILE,\n",
    "    '--do_eval',\n",
    "    '--eval_data_file', EVAL_DATA_FILE,\n",
    "    '--block_size', '510',\n",
    "    '--save_steps', '5000',\n",
    "    '--num_train_epochs', '2.0',\n",
    "    '--overwrite_cache',\n",
    "    '--overwrite_output_dir',\n",
    "]\n",
    "\n",
    "parser = transformers.HfArgumentParser((ModelArguments, DataTrainingArguments, transformers.TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(input_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¡®ä¿è¾“å…¥å‚æ•°æ­£ç¡®\n",
    "# do_evalä¸eval_data_fileä¸¤ä¸ªå‚æ•°ç»Ÿä¸€\n",
    "# è¾“å‡ºæ–‡ä»¶å¤¹åˆç†\n",
    "if data_args.eval_data_file is None and training_args.do_eval:\n",
    "    raise ValueError(\n",
    "        \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "        \"or remove the --do_eval argument.\"\n",
    "    )\n",
    "\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/26/2020 09:34:46 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/26/2020 09:34:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "07/26/2020 09:34:46 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/gpt2-train-new-model', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul26_09-34-44_b2275efd265b', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, dataloader_drop_last=False)\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®æ—¥å¿—æ ¼å¼ï¼Œå¹¶è®°å½•æœ¬æ¬¡è®­ç»ƒçš„é‡è¦å‚æ•°\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "# è®¾ç½®seed\n",
    "transformers.set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/26/2020 09:34:48 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/models/huggingface/gpt2/config.json\n",
      "07/26/2020 09:34:48 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/26/2020 09:34:48 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/models/huggingface/gpt2/config.json\n",
      "07/26/2020 09:34:48 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   Model name '/dfsdata2/yucc1_data/models/huggingface/gpt2' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/dfsdata2/yucc1_data/models/huggingface/gpt2' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/added_tokens.json. We won't load it.\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/special_tokens_map.json. We won't load it.\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/tokenizer_config.json. We won't load it.\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/tokenizer.json. We won't load it.\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/models/huggingface/gpt2/vocab.json\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/models/huggingface/gpt2/merges.txt\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/26/2020 09:34:48 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/26/2020 09:34:48 - INFO - __main__ -   Training new model from scratch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŠ è½½config\n",
    "if model_args.config_name:\n",
    "    config = transformers.AutoConfig.from_pretrained(model_args.config_name, cache_dir=model_args.cache_dir)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = transformers.AutoConfig.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
    "else:\n",
    "    config = CONFIG_MAPPING[model_args.model_type]()\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "# åŠ è½½tokenizer\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
    "        \"and load it from here, using --tokenizer_name\"\n",
    "    )\n",
    "\n",
    "if model_args.model_name_or_path:\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = transformers.AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# å¦‚æœtokenizerä¸modelçš„embeddingä¸ªæ•°ä¸åŒï¼Œè®¾ç½®ä¸ºç›¸åŒ\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¡éªŒå‚æ•°ï¼Œå¹¶ä¸”è®¾ç½®block_size\n",
    "if config.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"] and not data_args.mlm:\n",
    "    raise ValueError(\n",
    "        \"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the\"\n",
    "        \"--mlm flag (masked language modeling).\"\n",
    "    )\n",
    "\n",
    "if data_args.block_size <= 0:\n",
    "    data_args.block_size = tokenizer.max_len\n",
    "    # Our input block size will be the max possible for the model\n",
    "else:\n",
    "    data_args.block_size = min(data_args.block_size, tokenizer.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.dataset.Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, tokenizer: transformers.PreTrainedTokenizer, file_path: str, block_size: int, overwrite_cache=False,\n",
    "    ):\n",
    "        assert os.path.isfile(file_path)\n",
    "\n",
    "        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, \"cached_lm_{}_{}_{}\".format(tokenizer.__class__.__name__, str(block_size), filename,),\n",
    "        )\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with filelock.FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "\n",
    "                self.examples = []\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "\n",
    "                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
    "                    self.examples.append(\n",
    "                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])\n",
    "                    )\n",
    "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "                # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "                # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
    "\n",
    "\n",
    "class LineByLineTextDataset(torch.utils.data.dataset.Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, file_path: str, block_size: int):\n",
    "        assert os.path.isfile(file_path)\n",
    "        # Here, we do not cache the features, operating under the assumption\n",
    "        # that we will soon use fast multithreaded tokenizers from the\n",
    "        # `tokenizers` repo everywhere =)\n",
    "        logger.info(\"Creating features from dataset file at %s\", file_path)\n",
    "\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n",
    "        self.examples = batch_encoding[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForLanguageModeling:\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling.\n",
    "    - collates batches of tensors, honoring their tokenizer's pad_token\n",
    "    - preprocesses batches for masked language modeling\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "\n",
    "    def __call__(self, examples: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self._tensorize_batch(examples)\n",
    "        if self.mlm:\n",
    "            inputs, labels = self.mask_tokens(batch)\n",
    "            return {\"input_ids\": inputs, \"labels\": labels}\n",
    "        else:\n",
    "            labels = batch.clone().detach()\n",
    "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            return {\"input_ids\": batch, \"labels\": labels}\n",
    "\n",
    "    def _tensorize_batch(self, examples: List[torch.Tensor]) -> torch.Tensor:\n",
    "        length_of_first = examples[0].size(0)\n",
    "        are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "        if are_tensors_same_length:\n",
    "            return torch.stack(examples, dim=0)\n",
    "        else:\n",
    "            if self.tokenizer._pad_token is None:\n",
    "                raise ValueError(\n",
    "                    \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "                    f\" ({self.tokenizer.__class__.__name__}) does not have one.\"\n",
    "                )\n",
    "            return pad_sequence(examples, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "\n",
    "    def mask_tokens(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å¾—dataset\n",
    "def get_dataset(args: DataTrainingArguments, tokenizer: transformers.PreTrainedTokenizer, evaluate=False):\n",
    "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
    "    if args.line_by_line:\n",
    "        return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)\n",
    "    else:\n",
    "        return TextDataset(\n",
    "            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size, overwrite_cache=args.overwrite_cache\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/26/2020 09:35:01 - INFO - filelock -   Lock 139977698655704 acquired on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw.lock\n",
      "07/26/2020 09:35:01 - INFO - __main__ -   Creating features from dataset file at /dfsdata2/yucc1_data/datasets/wikitext-2-raw\n",
      "07/26/2020 09:35:03 - INFO - __main__ -   Saving features into cached file /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw [took 0.036 s]\n",
      "07/26/2020 09:35:03 - INFO - filelock -   Lock 139977698655704 released on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw.lock\n",
      "07/26/2020 09:35:03 - INFO - filelock -   Lock 139976122421544 acquired on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw.lock\n",
      "07/26/2020 09:35:03 - INFO - __main__ -   Creating features from dataset file at /dfsdata2/yucc1_data/datasets/wikitext-2-raw\n",
      "07/26/2020 09:35:06 - INFO - __main__ -   Saving features into cached file /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw [took 1.364 s]\n",
      "07/26/2020 09:35:06 - INFO - filelock -   Lock 139976122421544 released on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw.lock\n"
     ]
    }
   ],
   "source": [
    "# è·å–dataset\n",
    "train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
    "eval_dataset = get_dataset(data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None\n",
    "if config.model_type == \"xlnet\":\n",
    "    data_collator = transformers.DataCollatorForPermutationLanguageModeling(\n",
    "        tokenizer=tokenizer, plm_probability=data_args.plm_probability, max_span_length=data_args.max_span_length,\n",
    "    )\n",
    "else:\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([510])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/21/2020 13:44:23 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/21/2020 13:44:37 - INFO - transformers.trainer -   ***** Running training *****\n",
      "07/21/2020 13:44:37 - INFO - transformers.trainer -     Num examples = 561\n",
      "07/21/2020 13:44:37 - INFO - transformers.trainer -     Num Epochs = 2\n",
      "07/21/2020 13:44:37 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "07/21/2020 13:44:37 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "07/21/2020 13:44:37 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "07/21/2020 13:44:37 - INFO - transformers.trainer -     Total optimization steps = 72\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836362b4f72d42b19ff122286f3389c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567f95cc004548e198ed436bd090d3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=36.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1260c04896f4b65ab01769fec68e449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=36.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/21/2020 13:45:26 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "07/21/2020 13:45:26 - INFO - transformers.trainer -   Saving model checkpoint to /dfsdata2/yucc1_data/output/gpt2-train-new-model\n",
      "07/21/2020 13:45:26 - INFO - transformers.configuration_utils -   Configuration saved in /dfsdata2/yucc1_data/output/gpt2-train-new-model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/21/2020 13:45:27 - INFO - transformers.modeling_utils -   Model weights saved in /dfsdata2/yucc1_data/output/gpt2-train-new-model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒ\n",
    "if training_args.do_train:\n",
    "    model_path = (\n",
    "        model_args.model_name_or_path\n",
    "        if model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path)\n",
    "        else None\n",
    "    )\n",
    "    trainer.train(model_path=model_path)\n",
    "    trainer.save_model()\n",
    "    # For convenience, we also re-save the tokenizer to the same directory,\n",
    "    # so that you can share your model easily on huggingface.co/models =)\n",
    "    if trainer.is_world_master():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/21/2020 13:45:27 - INFO - __main__ -   *** Evaluate ***\n",
      "07/21/2020 13:45:27 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "07/21/2020 13:45:27 - INFO - transformers.trainer -     Num examples = 561\n",
      "07/21/2020 13:45:27 - INFO - transformers.trainer -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcaef743a9e43e699e05a0715178cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=36.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/21/2020 13:45:38 - INFO - transformers.trainer -   {'eval_loss': 8.00778447257148, 'epoch': 2.0, 'step': 72}\n",
      "07/21/2020 13:45:38 - INFO - __main__ -   ***** Eval results *****\n",
      "07/21/2020 13:45:38 - INFO - __main__ -     perplexity = 3004.2537276158396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°\n",
    "results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    eval_output = trainer.evaluate()\n",
    "\n",
    "    perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
    "    if trainer.is_world_master():\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    results.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexity': 3004.2537276158396}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç­”ç–‘ï¼š\n",
    "Traineråˆ°åº•å¹²ä»€ä¹ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å…­èŠ‚ æ€»ç»“ã€æ€è€ƒä¸å±•æœ›\n",
    "\n",
    "1. å¼€æºä»£ç åŠå·¥å…·æŠ¥çš„ä¹¦å†™æ–¹å¼ã€‚setup.pyç”¨äºpipåŒ…å†™ä½œï¼›testsæ˜¯æµ‹è¯•æ ·ä¾‹ï¼›åŒåçš„æ–‡ä»¶å¤¹ä¸‹æ˜¯ä»£ç ã€‚\n",
    "2. æ‰€æœ‰è¾“å‡ºæ—¥å¿—éå¸¸é‡è¦ä¸”æœ‰ç”¨ã€‚\n",
    "3. typing, dataclasses, classmethodç­‰æ–¹æ³•çš„ä½¿ç”¨ã€‚\n",
    "4. å¼€æºåº“çš„å­¦ä¹ ã€‚ç¬¬ä¸€æ­¥ï¼Œçœ‹æ–‡æ¡£ï¼Œè·‘å®˜æ–¹examplesï¼šç¬¬äºŒæ­¥ï¼Œä¿®æ”¹ä»£ç ï¼Œä»¿å†™è‡ªå·±çš„ä»£ç ï¼›ç¬¬ä¸‰æ­¥ï¼Œé‡å¤å‰ä¸¤æ­¥ï¼›ç¬¬å››æ­¥ï¼Œçœ‹æºç ï¼Œä¿®æ”¹æºç ã€‚\n",
    "5. é€šè¿‡å­¦ä¹ æºç ï¼Œå­¦åˆ°äº†å¾ˆå¤šï¼Œå¸Œæœ›åç»­å¯¹æºç æ›´åŠ æ¸…æ¥šï¼Œä½¿ç”¨æ›´åŠ ç†Ÿç»ƒï¼›å¸Œæœ›èƒ½åœ¨å·¥ä½œã€å­¦ä¹ ã€ç§‘ç ”ä¸Šå¯¹è‡ªå·±ã€å¯¹å¤§å®¶æ›´æœ‰å¸®åŠ©ã€‚ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸ƒèŠ‚ ç›¸å…³ç½‘å€\n",
    "1. githubä»£ç ï¼šhttps://github.com/huggingface/transformers\n",
    "2. docè¯´æ˜ï¼šhttps://huggingface.co/transformers/index.html\n",
    "3. æ¨¡å‹ä¸‹è½½åœ°å€ï¼šhttps://huggingface.co/models\n",
    "4. åˆ©ç”¨transformerså¼€å‘çš„ä¸­æ–‡chitchatç”Ÿæˆï¼šhttps://github.com/yangjianxin1/GPT2-chitchat\n",
    "5. DialoGPTï¼šhttps://github.com/microsoft/DialoGPT\n",
    "6. glueæ•°æ®é›†ä»‹ç»ï¼šhttps://zhuanlan.zhihu.com/p/135283598\n",
    "7. æ–‡æœ¬è§£ç ç­–ç•¥ï¼šhttps://zhuanlan.zhihu.com/p/68383015\n",
    "8. Trainerè¯´æ˜ï¼šhttps://huggingface.co/transformers/main_classes/trainer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
