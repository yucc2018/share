{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>ä¸€è¯¾Transformers</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€èŠ‚ ä»‹ç»ä¸å‡†å¤‡å·¥ä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 transformersç®€ä»‹åŠæœ¬æ•™ç¨‹ç›®çš„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éšç€NLPé¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹çš„ç››è¡Œï¼Œä»BERTã€GPTåˆ°T5ã€ELECTRAã€Longformerã€MobileBERTç­‰è¶Šæ¥è¶Šå¤šçš„æ¨¡å‹æ¶Œç°äº†å‡ºæ¥ã€‚æ¯ä¸ªæ¨¡å‹çš„ä½œè€…å¯èƒ½ç”¨tfï¼Œä¹Ÿå¯èƒ½æ˜¯pytorchï¼Œè€Œä¸”å¾ˆå¯èƒ½ä¸åŒçš„ç¯å¢ƒç‰ˆæœ¬ï¼Œè¿™å¯¹äºå­¦æœ¯ç•Œã€å·¥ä¸šç•Œçš„å­¦ä¹ ã€å¤ç°ã€ä½¿ç”¨éƒ½å¸¦æ¥äº†ä¸€å®šå›°éš¾ã€‚å¹¸å¥½ï¼Œhuggingfaceå…¬å¸ä¸‹çš„transformersåº“å¸®æˆ‘ä»¬è§£å†³äº†è¿™ä¸ªéš¾é¢˜ã€‚\n",
    "\n",
    "huggingfaceæ˜¯ä¸€å®¶å…¬å¸ï¼Œtransformers[1][2][3]æ˜¯å…¶å…¬å¸å¼€å‘çš„å¼€æºåº“ï¼Œå·²æœ‰30k+ä¸ªstarã€‚è¯¥åº“è¡¨ç°ä¸ºï¼šç®€å•æ˜“ç”¨ï¼›åŒæ—¶æ”¯æŒtf2å’Œpytorchï¼›æ”¯æŒå¾ˆå¤šé¢„è®­ç»ƒæ¨¡å‹å¦‚BERT, GPT, ALBERT, T5, DialoGPT, ELECTRAç­‰ï¼Œè€Œä¸”éšæ—¶ç»´æŠ¤ï¼›æä¾›ç»Ÿä¸€çš„ã€æ ‡å‡†çš„Configã€Modelã€Tokenizerã€Traineræ¥å£ï¼ŒåŒæ—¶æä¾›æ ‡å‡†åŒ–çš„æ¨¡å‹æ–¹å¼ï¼Œæ–¹ä¾¿å¤ç°ã€æ‹“å±•å’Œå®éªŒã€‚\n",
    "\n",
    "ç›®å‰ä¸€äº›å¼€æºåº“ä½¿ç”¨huggingface/transformersä¸ºåŸºç¡€è¿›è¡Œå¼€å‘ï¼Œæä¾›ä¸€äº›åˆ†ç±»ã€ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚åŸºäºtransformersåº“å¼€å‘çš„ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆ[4]ï¼›åŸºäºtransformersåº“è¿›è¡Œçš„ç§‘ç ”ï¼Œå¦‚DialoGPT[5]ã€‚\n",
    "\n",
    "ä¸¤ç§æ–¹å¼å»ä½¿ç”¨ï¼Œç¬¬ä¸€ç§æ˜¯pipelineæ–¹å¼ï¼Œé«˜åº¦é›†æˆï¼Œç›´æ¥ä½¿ç”¨ï¼Œå¯ä»¥ç”¨äºæƒ…æ„Ÿåˆ†ç±»ã€NERæ ‡æ³¨ç­‰ï¼›ç¬¬äºŒç§æ˜¯æä¾›æ ‡æ³¨çš„æ¨¡å‹ï¼Œå»è®­ç»ƒï¼Œæ›´ç¬¦åˆæ ‡å‡†çš„ä½¿ç”¨ï¼Œæ›´èƒ½ä½¿ç”¨æˆ‘ä»¬æ—¥å¸¸å­¦ä¹ ã€å·¥ä½œä¸­çš„ä»»åŠ¡ã€‚\n",
    "\n",
    "æœ¬ç¯‡è®²è§£ç¬¬äºŒç§ä½¿ç”¨æ–¹å¼ï¼Œä»¥pytorchç‰ˆçš„æ¨¡å‹ä½¿ç”¨ä¸ºä¾‹ï¼Œå¸Œæœ›èƒ½é€šè¿‡ä¸€èŠ‚è¯¾çš„æ—¶é—´å¸®åŠ©å¤§å®¶å…¥é—¨transformersåº“çš„ä½¿ç”¨ã€‚\n",
    "\n",
    "æ³¨ï¼šæœ¬ä»£ç å‚è€ƒå’Œä½¿ç”¨äº†å¤§é‡çš„å®˜æ–¹æ ·ä¾‹ï¼Œè¯¥æ–‡çš„æ‰€æœ‰ä»£ç å·²ç»é€šè¿‡æµ‹è¯•ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æœ¬æ–‡ç»“æ„\n",
    "\n",
    "#### ç¬¬ä¸€èŠ‚ ä»‹ç»ä¸å‡†å¤‡å·¥ä½œ\n",
    "\n",
    "æœ¬èŠ‚å¯¹huggingface/transformersæ˜¯ä»€ä¹ˆè¿›è¡Œäº†è¯´æ˜ï¼Œè¯´æ˜æœ¬æ–‡çš„ç›®çš„ï¼Œç« èŠ‚ç»“æ„ï¼Œå‡†å¤‡å·¥ä½œç­‰å†…å®¹ã€‚\n",
    "\n",
    "#### ç¬¬äºŒèŠ‚ å¿«é€Ÿå…¥é—¨\n",
    "\n",
    "ä¸€ä¸ªå¿«é€Ÿå…¥é—¨çš„ä¾‹å­ï¼Œtokenizerã€modelåŠ è½½ä¸ä¿å­˜çš„æ¦‚å¿µï¼Œè®²è§£å¦‚ä½•åˆ©ç”¨tokenizerå°†æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹çš„è¾“å…¥ï¼Œé€šè¿‡modelå¾—åˆ°logitsä¸lossç­‰ç»“æœã€‚é€šè¿‡è¿™ä¹ˆç®€å•åœ°å‡ æ­¥ï¼Œè¿™æ˜¯æ ‡å‡†çš„pytorchä¸€ä¸ªbatchçš„æŸå¤±è®¡ç®—è¿‡ç¨‹ã€‚åŸºæœ¬ä¸Šå®ç°äº†ä¸€ä¸ªå®Œæ•´çš„å‘¨æœŸè¿­ä»£ã€‚\n",
    "\n",
    "#### ç¬¬ä¸‰èŠ‚ æ¦‚å¿µä¸è¯´æ˜\n",
    "\n",
    "ä»‹ç»transformersçš„æœåŠ¡äººç¾¤ã€ç›®æ ‡ã€Configurationã€Tokenizerã€Modelã€AutoModelsã€Trainerè¿™å‡ ä¸ªç±»çš„è®¾è®¡ç›®çš„ï¼Œä»é«˜å±‚æ¬¡ä¸Šç†è§£è¯¥åº“çš„è®¾è®¡æ€è·¯ã€‚\n",
    "\n",
    "#### ç¬¬å››èŠ‚ GLUE/MRPCæ•°æ®é›†è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„ç¤ºä¾‹\n",
    "\n",
    "åŸºäºä¸Šé¢çš„çŸ¥è¯†ï¼Œè¿›è¡Œå®Œæ•´çš„å®æˆ˜ä»£ç æ¼”ç¤ºã€‚è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ–‡æœ¬åˆ†ç±»fine-tuneä¸çº¿ä¸Šéƒ¨ç½²çš„ä»£ç ï¼Œå…±åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š\n",
    "\n",
    "ç¬¬ä¸€éƒ¨åˆ†ï¼Œè®²è§£å¦‚ä½•åŠ è½½æ•°æ®ã€æ¨¡å‹ã€tokenizerç­‰ï¼Œå»åˆå§‹åŒ–Trainerï¼Œå¹¶è®­ç»ƒã€‚\n",
    "\n",
    "ç¬¬äºŒéƒ¨åˆ†ï¼ŒåŠ è½½ç¬¬ä¸€éƒ¨åˆ†å¾—åˆ°çš„æ¨¡å‹ç»“æœï¼Œè¿›è¡Œé¢„æµ‹ç»“æœã€‚\n",
    "\n",
    "#### ç¬¬äº”èŠ‚ GPT2è®­ç»ƒä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "ç±»ä¼¼äºç¬¬å››èŠ‚ã€‚è®²è§£ä»å¤´é¢„è®­ç»ƒGPT2çš„è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”ä½¿ç”¨beam searchã€top kã€top pè§£ç è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚\n",
    "\n",
    "#### ç¬¬å…­èŠ‚ æ€»ç»“ã€æ€è€ƒä¸å±•æœ›\n",
    "\n",
    "è¿›è¡Œå…¨æ–‡æ€»ç»“ã€‚\n",
    "\n",
    "#### ç¬¬ä¸ƒèŠ‚ ç›¸å…³ç½‘å€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 å‡†å¤‡å·¥ä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†\n",
    "\n",
    "ä¸‹é¢çš„è®²è§£ä½¿ç”¨åˆ°bert-base-uncasedã€gpt2ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼›åŒæ—¶ä¼šä½¿ç”¨åˆ°glueä¸‹çš„MRPCæ•°æ®é›†ã€wikitext-2-rawæ•°æ®é›†ã€‚å¯ä»¥ç›´æ¥é€šè¿‡ä¸‹é¢çš„ç™¾åº¦äº‘é“¾æ¥å’Œå¯†ç å»ä¸‹è½½ã€‚å¯ä»¥å°†ä¸‹è½½å¥½çš„æ•°æ®æ”¾åœ¨ä¸æœ¬notebookåŒçº§ç›®å½•ä¸‹ï¼Œæ–¹ä¾¿ä½¿ç”¨ã€‚\n",
    "\n",
    "é“¾æ¥ï¼š\n",
    "\n",
    "å¯†ç ï¼š\n",
    "\n",
    "#### 1.3.2 å®‰è£…ç›¸åº”çš„ç¯å¢ƒ\n",
    "\n",
    "æœ¬æ•™ç¨‹ä»¥pytorchä¸ºåŸºç¡€ï¼Œéœ€è¦å®‰è£…pytorchï¼Œå»ºè®®pytorch>=1.4.0ã€‚\n",
    "\n",
    "transformerså‡çº§è‡³æœ€æ–°ç‰ˆæœ¬ï¼Œå‡çº§æ–¹å¼ï¼š\n",
    "\n",
    "```\n",
    "pip install --upgrade transformers\n",
    "```\n",
    "\n",
    "å¯èƒ½ä¼šéœ€è¦å…¶ä»–ç¯å¢ƒï¼Œå¦‚pandasã€xlrdã€sklearnç­‰ï¼Œæç¤ºç¼ºå°‘ä»€ä¹ˆåŒ…ï¼Œç›´æ¥å®‰è£…å³å¯ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äºŒèŠ‚ å¿«é€Ÿå…¥é—¨\n",
    "\n",
    "é€šè¿‡ç®€å•çš„ä¾‹å­ï¼Œæ„Ÿå—ä»æ–‡æœ¬åˆ°æ¨¡å‹è¾“å‡ºçš„å¿«æ·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.4.0+cu100\n",
      "transformers: 3.0.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f'torch: {torch.__version__}')\n",
    "print(f'transformers: {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰€æœ‰å¤§å†™çš„å†…å®¹ï¼Œéœ€è¦æ”¹ä¸ºè‡ªå·±çš„å®é™…è·¯å¾„\n",
    "BERT_MODEL_NAME_OR_PATH = 'transformers_data_and_model/bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers_data_and_model/bert-base-uncased\n",
      "â”œâ”€â”€ config.json\n",
      "â”œâ”€â”€ modelcard.json\n",
      "â”œâ”€â”€ pytorch_model.bin\n",
      "â””â”€â”€ vocab.txt\n",
      "\n",
      "0 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "# å±•ç¤ºæ–‡ä»¶å¤¹ä¸­çš„å†…å®¹\n",
    "!tree {BERT_MODEL_NAME_OR_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at transformers_data_and_model/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at transformers_data_and_model/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–modelå’Œtokenizer\n",
    "#ã€€æ‰€æœ‰modelå’Œtokenizerçš„åˆå§‹åŒ–éƒ½ä½¿ç”¨from_pretrainedçš„æ–¹æ³•ï¼Œä¿å­˜éƒ½ä½¿ç”¨save_pretrainedçš„æ–¹æ³•\n",
    "# å¯¹from_pretrained:ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æ–‡ä»¶å¤¹çš„è·¯å¾„/æ–‡ä»¶çš„è·¯å¾„/æ¨¡å‹çš„short nameç­‰å‡ ç§æ–¹æ³•ï¼Œè¿™é‡Œæ¨èä½¿ç”¨æ–‡ä»¶å¤¹çš„æ–¹å¼\n",
    "# modelåˆå§‹åŒ–é»˜è®¤æ˜¯evalæ¨¡å¼ï¼Œè¿™é‡ŒåŠ è½½çš„æ˜¯BERTçš„tokenizerå’Œåˆ†ç±»æ¨¡å‹model\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME_OR_PATH)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BERT_MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# tokenizerçš„ä½œç”¨ï¼šå¯¹äºç»™å®šçš„æ–‡æœ¬ï¼Œç»è¿‡tokenizerå¤„ç†æˆmodelå¯ä»¥æ¥å—çš„æ ¼å¼\n",
    "# tokenizeræœ€é‡è¦çš„ç”¨æ³•æ˜¯__call__ï¼Œè¿™ä¸ªæ–¹æ³•å¯ä»¥å°†æ–‡æœ¬è¾“å‡ºä¸ºæ¨¡å‹çš„è¦çš„æ ¼å¼\n",
    "# tokenizerè¿˜æœ‰å…¶ä»–æ–¹æ³•ï¼Œencode/decodeï¼Œé¡¾åæ€ä¹‰ï¼Œå°±æ˜¯å°†æ–‡æœ¬è½¬æ¢æˆinput_idsåŠå°†input_idsè½¬æ¢ä¸ºæ–‡æœ¬\n",
    "# encode/decodeä¸__call__å…¶å®æ— æœ¬è´¨åŒºåˆ«ï¼Œåªæ˜¯__call__ä¸ºäº†æä¾›ç»Ÿä¸€çš„å¤„ç†æ¥å£\n",
    "inputs = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
    "# input_idsæ˜¯æ–‡æœ¬æ¯ä¸ªè¯çš„indexï¼›token_type_idæ˜¯è¡¨ç¤ºæ–‡æœ¬æ˜¯ç¬¬ä¸€å¥/ç¬¬äºŒå¥ï¼› attention_maskæ˜¯å¤„ç†maskç”¨çš„\n",
    "# è¿™é‡Œå¤„ç†çš„æ˜¯ä¸€æ¡æ ·æœ¬ä¸”åªæœ‰ä¸€å¥è¯çš„ä¾‹å­ï¼Œå¦‚æœæ˜¯å¤šæ¡å•å¥æ ·æœ¬ï¼Œè¾“å…¥ä¸ºä¸€ä¸ªæ–‡æœ¬listå³å¯ã€‚\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 102, 2204, 2851, 102], 'token_type_ids': [0, 0, 0, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# ä¸€æ¡æ ·æœ¬ä¸”æœ‰ä¸¤å¥è¯çš„ä¾‹å­ï¼Œåˆ†åˆ«ä½œä¸ºç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå‚æ•°è¾“å…¥\n",
    "# å¦‚æœæ˜¯å¤šä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯ä¸¤å¥è¯ï¼Œåˆ™ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ç¬¬ä¸€å¥è¯çš„æ–‡æœ¬listï¼Œç¬¬äºŒä¸ªå‚æ•°ä¸ºç¬¬äºŒå¥è¯çš„æ–‡æœ¬list\n",
    "inputs2 = tokenizer('hello', 'good morning')\n",
    "print(inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# æ›´å¤šæ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦çš„modelè¾“å…¥æ˜¯æˆbatchæ ¼å¼çš„\n",
    "# ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯æ–‡æœ¬listï¼Œpaddingè®¾ç½®ä¸ºTrueï¼Œtruncationè®¾ç½®ä¸ºTrueå¯ä»¥è¿›è¡Œpaddingå’Œtruncation\n",
    "# return_tensorså†™æ˜äº†è¿”å›çš„æ ¼å¼ï¼Œæ˜¯ä¸€ä¸ªpytorchçš„tensor\n",
    "pt_batch = tokenizer(\n",
    "     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "     padding=True,\n",
    "     truncation=True,\n",
    "     return_tensors=\"pt\"\n",
    ")\n",
    "for key, value in pt_batch.items():\n",
    "     print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:tensor([[0.2274, 0.1681],\n",
      "        [0.1150, 0.2867]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "loss: 0.7529614567756653\n",
      "logits: tensor([[0.2274, 0.1681],\n",
      "        [0.1150, 0.2867]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# å¯¹äºtokenizerå¤„ç†åçš„æ–‡æœ¬ï¼Œç›®æ ‡æ˜¯é€å…¥modelï¼Œç”¨äºåˆ†ç±»ã€é¢„æµ‹ç­‰ä»»åŠ¡\n",
    "# ä¸Šé¢çš„pt_batchå°±æ˜¯ä¸€ä¸ªbatchï¼Œå¢åŠ **ç›´æ¥è¾“å…¥æ¨¡å‹\n",
    "# æ ¹æ®transformersåº“çš„è§„åˆ™ï¼Œ æ‰€æœ‰modelçš„è¾“å‡ºéƒ½æ˜¯å…ƒç»„\n",
    "# å¦‚æœåªæœ‰æ¯ä¸ªbatchçš„è¾“å…¥ï¼Œå…ƒç»„è¾“å‡ºçš„ç¬¬ä¸€ä¸ªæ˜¯logits\n",
    "# å¦‚æœåŒæ—¶ä¼ å…¥äº†labelsçš„å‚æ•°ï¼Œåˆ™å…ƒç»„è¾“å‡ºçš„ç¬¬ä¸€ä¸ªæ˜¯lossï¼Œç¬¬äºŒä¸ªæ˜¯logits\n",
    "# ä¸€èˆ¬å›å½’ä»»åŠ¡lossç”¨çš„Mean-Square lossï¼Œåˆ†ç±»ä»»åŠ¡åˆ™æ˜¯Cross-Entroy\n",
    "pt_outputs = model(**pt_batch)\n",
    "print(f'logits:{pt_outputs[0]}\\n')\n",
    "# ä¼ å…¥labelså‚æ•°çš„æƒ…å†µ\n",
    "pt_outputs = model(**pt_batch, labels=torch.LongTensor([1, 0]))\n",
    "print(f'loss: {pt_outputs[0]}\\nlogits: {pt_outputs[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢å°±æ˜¯pytorchç‰ˆtransformersçš„åŸºæœ¬è¾“å…¥é€»è¾‘ï¼š\n",
    "\n",
    "transformersä¸­çš„æ‰€æœ‰modeléƒ½æ˜¯pytorchçš„æ ‡å‡†æ¨¡å‹ç±»torch.nn.Moduleã€‚æ–‡æœ¬å¯ä»¥é€šè¿‡tokenizerè°ƒç”¨è½¬æ¢ä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œæ¨¡å‹è¾“å…¥è¿™äº›ä¿¡æ¯ï¼Œå¾—åˆ°logitsï¼Œè®¡ç®—lossï¼Œè¿›è¡Œè¯¯å·®å›ä¼ backwardï¼Œè¿›è¡Œè¿­ä»£ï¼Œå°±å®Œæˆäº†è®­ç»ƒ/fine-tuneã€‚æ¨¡å‹è¾“å…¥çš„æ—¶å€™ï¼Œå¦‚æœä¼ å…¥labelsçš„å‚æ•°ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å¾—åˆ°ç›¸åº”çš„lossï¼Œä¸€æ ·backwardï¼Œå¤šæ¬¡è¿­ä»£ï¼Œå®Œæˆè®­ç»ƒã€‚\n",
    "\n",
    "é™¤äº†æ ‡å‡†çš„pytorchæ–¹å¼ï¼Œtransformersè¿˜å°è£…äº†Trainerç±»æ¥å¸®åŠ©æˆ‘ä»¬ç®€åŒ–pytorchä»£ç ï¼Œåé¢å†è®²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæˆè®­ç»ƒ/å¾®è°ƒåï¼Œå¯ä»¥å°†tokenizerå’Œmodelä¿å­˜è‡³ç›¸åŒçš„æ–‡ä»¶å¤¹ã€‚\n",
    "# åœ¨transformersæ¡†æ¶é‡Œï¼Œä¸€ä¸ªå¾ˆå¥½çš„ä¹ æƒ¯ï¼Œå°†modelã€tokenizerå‚æ•°ã€è®­ç»ƒå‚æ•°ç­‰æ‰€æœ‰å­˜æ”¾åœ¨åŒä¸€æ–‡ä»¶å¤¹ã€‚\n",
    "# è¿™é‡Œçš„model/tokenizer/configçš„åˆå§‹åŒ–ä½¿ç”¨from_pretrainedï¼Œä¿å­˜ä½¿ç”¨save_pretrained\n",
    "# save_pretrainedä¼ å…¥å…·ä½“çš„æ–‡ä»¶å¤¹åå³å¯\n",
    "SAVE_DIRECTORY = 'transformers_data_and_model/bert_save_example'\n",
    "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
    "model.save_pretrained(SAVE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers_data_and_model/bert_save_example\n",
      "â”œâ”€â”€ config.json\n",
      "â”œâ”€â”€ pytorch_model.bin\n",
      "â”œâ”€â”€ special_tokens_map.json\n",
      "â”œâ”€â”€ tokenizer_config.json\n",
      "â””â”€â”€ vocab.txt\n",
      "\n",
      "0 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜çš„ç»“æœå±•ç¤º\n",
    "!tree {SAVE_DIRECTORY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ€»ç»“ï¼š\n",
    "\n",
    "1. åœ¨transformersæ¡†æ¶é‡Œï¼Œæä¾›äº†model/tokenizer/configé€šç”¨åŒ–çš„åŠ è½½å’Œä¿å­˜ï¼Œä¹Ÿå°±æ˜¯from_pretraiend/save_pretrainedï¼›\n",
    "2. tokenizerçš„ä½œç”¨åœ¨äºï¼Œé€šè¿‡\\_\\_call__å°†æ–‡æœ¬è¿›è¡Œè½¬æ¢æˆæ¨¡å‹æ¥å—çš„æ ¼å¼ï¼Œmodelçš„è¾“å‡ºéƒ½æ˜¯å…ƒç»„ï¼Œä¾æ®è¿™äº›å…ƒç»„çš„å†…å®¹è¿›è¡Œè®¡ç®—lossï¼›\n",
    "3. tokenizeråŒ…è£…äº†Byte-Pair Encodingã€WordPieceã€SentencePieceç­‰ä¸åŒçš„æ–¹å¼ï¼›\n",
    "4. modelæ˜¯åŒ…è£…äº†BERTã€GPT2ã€ALBERTç­‰ä¸åŒçš„æ¨¡å‹ï¼Œå¹¶ä¸”æä¾›æ ‡å‡†åŒ–çš„ç±»ã€‚ç­‰ä¸‹ç»†è¯´ã€‚\n",
    "5. å¯¹äºæˆ‘ä»¬æ¥è®²ï¼Œå»å¤ç°ã€å®éªŒã€ç ”ç©¶æ›´å®¹æ˜“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸‰èŠ‚ æ¦‚å¿µä¸è¯´æ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬èŠ‚è®²è§£transformersæ¶‰åŠçš„æœåŠ¡ç¾¤ä½“ã€ç›®æ ‡ã€ä¸»è¦æ¦‚å¿µã€AutoModelsã€Trainerç±»ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 transformersçš„æœåŠ¡äººç¾¤ï¼š\n",
    "- å¯»æ‰¾ç”¨äºä½¿ç”¨ã€å­¦ä¹ ã€æ‰©å±•å¤§å‹transformersæ¨¡å‹çš„NLPç ”ç©¶è€…å’Œæ•™è‚²è€…\n",
    "- å¸Œæœ›å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–åœ¨ç”Ÿäº§ç”Ÿæä¾›æœåŠ¡çš„å®è·µè€…\n",
    "- åªæƒ³ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å°†å…¶ç”¨äºè§£å†³NLPä»»åŠ¡çš„å·¥ç¨‹å¸ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 transformersçš„ä¸¤ä¸ªå¼ºç›®æ ‡ï¼š\n",
    "- å°½å¯èƒ½çš„ç®€å•ï¼Œå¿«é€Ÿçš„ä½¿ç”¨\n",
    "- ä¸ºæœ€æ–°æ¨¡å‹æä¾›ä¸åŸå§‹æ¨¡å‹å°½å¯èƒ½æ¥è¿‘çš„æ€§èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 transformersçš„å°ç›®æ ‡:\n",
    "- å°½å¯èƒ½çš„å†…éƒ¨æ¥å£ä¸€è‡´\n",
    "- çº³å…¥ä¸»è§‚é€‰æ‹©çš„æœ‰å‰é€”çš„å·¥å…·ï¼Œä»¥å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒ/ç ”ç©¶\n",
    "- åœ¨PyTorchå’ŒTensorFlow 2.0ä¹‹é—´è½»æ¾åˆ‡æ¢ï¼Œä»è€Œå…è®¸ä½¿ç”¨ä¸€ç§æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œè€Œä½¿ç”¨å¦ä¸€ç§æ¡†æ¶è¿›è¡Œæ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 ä¸»è¦æ¦‚å¿µï¼š\n",
    "\n",
    "ä¸»è¦çš„ç±»æœ‰Modelã€Configurationã€Tokenizerè¿™ä¸‰ä¸ªç±»ï¼Œä¸‹é¢åˆ†åˆ«ä»‹ç»ã€‚\n",
    "\n",
    "Modelç±»ï¼Œæ¯”å¦‚BertModelï¼Œå‡ä»pytorch models(torch.nn.Module)æˆ–è€…keras models(tf.keras.Model)ç»§æ‰¿è€Œæ¥ï¼Œç”¨äºå¤„ç†é¢„è®­ç»ƒæƒé‡ã€‚\n",
    "\n",
    "Configurationç±»ï¼Œæ¯”å¦‚BertConfigï¼Œé‡Œé¢ä¿å­˜ç€å»ºç«‹æ¨¡å‹æ‰€éœ€è¦çš„æ‰€æœ‰å‚æ•°ã€‚å¹¶ä¸æ˜¯æ€»æ˜¯æˆ‘ä»¬æ‰‹åŠ¨å»åˆå§‹åŒ–è¿™ä¸ªç±»ï¼Œå°¤å…¶æ˜¯å½“ä½ ä½¿ç”¨æ²¡æœ‰åšä»»ä½•æ›´æ”¹çš„é¢„è®­ç»ƒæ—¶ï¼Œmodelä¼šè‡ªåŠ¨å¤„ç†å¥½è¿™ä¸ªç±»ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœè‡ªå·±é‡æ–°é¢„è®­ç»ƒçš„æ¨¡å‹ä¸”æ¶æ„ä¸ä¸€è‡´æ—¶ï¼Œæ˜¯è®¸æˆ‘ä»¬æˆ‘ä»¬å»åˆå§‹åŒ–è¿™ä¸ªç±»çš„ã€‚\n",
    "\n",
    "Tokenizerç±»ï¼Œæ¯”å¦‚BERTTokenizerï¼Œä¸ºæ¯ä¸ªæ¨¡å‹ä¿å­˜è¯å…¸ï¼Œå¹¶å°†æ–‡æœ¬è¿›è¡Œç¼–ç /è§£ç æˆæ¨¡å‹éœ€è¦çš„æ ¼å¼â€”â€”tokenåµŒå…¥çš„ç´¢å¼•ã€‚\n",
    "\n",
    "ä¸Šé¢çš„ç±»ï¼Œéƒ½æœ‰ä¸‹é¢ä¸¤ä¸ªæ–¹æ³•å»å®ä¾‹åŒ–ç±»å’Œä¿å­˜è‡³æœ¬åœ°ï¼š\n",
    "\n",
    "`from_pretrained()` å…è®¸æˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨short_nameï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æœ¬åœ°çš„æ¨¡å‹ï¼Œä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°model_name_or_pathä¼ å…¥ï¼Œå¯ä»¥æ˜¯æ–‡ä»¶å¤¹ã€æ–‡ä»¶ã€short_nameç­‰ã€‚å…¶ä¸­æ–‡ä»¶å¤¹çš„è¯ï¼Œä¼šé»˜è®¤å¯»æ‰¾æ–‡ä»¶å¤¹ä¸­çš„pytorh_model.binã€‚\n",
    "\n",
    "`save_pretrained()` å…è®¸æˆ‘ä»¬å°†æ¨¡å‹ä¿å­˜è‡³æœ¬åœ°ï¼Œä¿å­˜çš„å‚æ•°æ˜¯å¯ä»¥æ˜¯æ–‡ä»¶å¤¹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 AutoModels\n",
    "\n",
    "ä»¥BERTä¸ºä¾‹ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªConfigï¼ˆBertConfigï¼‰ï¼›æœ‰1-2ä¸ªtokenizerï¼Œåˆ†åˆ«æ˜¯åŸºäºrustçš„å¿«é€Ÿtokenizerï¼ˆBertTokenizerFastï¼‰ï¼Œä¸€ä¸ªæ˜¯åŸºäºpythonåŸç‰ˆçš„tokenizerï¼ˆBertTokenizerï¼‰ï¼Œéƒ¨åˆ†æ²¡æœ‰æä¾›rustçš„å¿«é€Ÿtokenizerï¼›æœ‰å¤šä¸ªçš†æœ‰ä¸åŒheadçš„Modelï¼Œæ¯”å¦‚æœ€åŸå§‹çš„æ¨¡å‹ï¼Œä¸å«headçš„BertModelã€é¢„è®­ç»ƒMLMå’ŒNSPçš„BertForPreTrainingã€MLM headçš„BertForMaskedLMï¼ŒNSPçš„BertForNextSentencePredictionï¼Œç”¨äºå¥å­åˆ†ç±»çš„BertForSequenceClassificationï¼Œç”¨äºå¤šé€‰çš„BertForMultipleChoiceï¼Œç”¨å•è¯åˆ†ç±»çš„BertForTokenClassificationï¼Œç”¨äºé—®ç­”çš„BertForQuestionAnsweringã€‚\n",
    "\n",
    "ä¸åŒçš„æ¨¡å‹ï¼Œä¼šç¨æœ‰ä¸åŒã€‚ä½†æ˜¯configç±»éƒ½ç»§æ‰¿è‡ªPretrainedConfigï¼›tokenizeréƒ½ç»§æ‰¿è‡ªPreTrainedTokenizeræˆ–PreTrainedTokenizerFastï¼›modeléƒ½ç»§æ‰¿è‡ªPreTrainedModelã€‚\n",
    "\n",
    "ä¸ºäº†ä½¿ç”¨æ–¹ä¾¿ï¼ŒAutoConfigã€AutoTokenizerã€AutoModelã€AutoModelForPreTrainingã€AutoModelWithLMHeadã€AutoModelForSequenceClassificationã€AutoModelForQuestionAnsweringã€AutoModelForTokenClassificationç­‰å¯ä»¥ç”¨äºè‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Trainerç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainerç±»æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„æ ‡å‡†è®­ç»ƒçš„APIï¼Œç›®å‰æ”¯æŒè¯­è¨€æ¨¡å‹ã€æ–‡æœ¬åˆ†ç±»ã€å•è¯åˆ†ç±»ï¼ˆNERï¼‰ç­‰ä»»åŠ¡ã€‚å¯¹äºå‰é¢çš„configã€tokenizerã€modelï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºï¼Œå¸®åŠ©æˆ‘ä»¬ç®€åŒ–çš„æ˜¯å†™æ¨¡å‹çš„è¿™ä¸€æ­¥ï¼Œæ­£å¸¸ç”Ÿæˆdatasetã€dataloaderï¼Œç„¶åå†æ¯ä¸ªepochã€batchè¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°æœ€ç»ˆçš„ç»“æœã€‚æ­£å¸¸å†™çš„è¯ï¼ŒTrainerç±»å¯ä»¥ä¸ç”¨ï¼ŒTrainerå…¶å®æ˜¯ç®€åŒ–çš„æ˜¯æˆ‘ä»¬è®­ç»ƒçš„è¿™ä¸€æ­¥ã€‚\n",
    "\n",
    "å¯¹äºé€šå¸¸çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå†™æ³•å¤§è‡´æ˜¯è¿™æ ·çš„ï¼ˆä¸‹é¢æ˜¯ä¼ªä»£ç ï¼‰:\n",
    "\n",
    "```\n",
    "# åŠ è½½æ•°æ®\n",
    "train_data, test_data = get_data()\n",
    "# è½¬æ¢æˆfeaturesï¼Œè·å¾—dataset\n",
    "train_dataset = MyDataset(train_data, args)\n",
    "test_dataset = MyDataset(test_data, args)\n",
    "\n",
    "# è½¬æ¢æˆdataloaderï¼Œç”¨äºç”Ÿæˆbatch\n",
    "train_sampler, test_sampler = \n",
    "train_dataloader = Dataloader(train_dataset, sampler=train_sampler, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = Dataloader(train_dataset, sampler=test_sampler, batch_size=batch_size, collate_fn=collate_fn)\n",
    "# åˆå§‹åŒ–tensorboard\n",
    "tb_writer = SummaryWriter(log_dir=None)\n",
    "# åŠ è½½optimizer\n",
    "optimizer = \n",
    "mode.to(GPU)\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # åˆ‡æ¢ä¸ºtrain\n",
    "        model.train()\n",
    "        # ä¼ å…¥ï¼§ï¼°ï¼µ\n",
    "        batch.to(GPU)\n",
    "        # è®¡ç®—æ¯ä¸€æ­¥çš„lossï¼Œç„¶åå›ä¼ \n",
    "        tr_loss = train_step(model, inputs, optimizer)\n",
    "        tr_loss.backword()\n",
    "        model.zero_grad()\n",
    "for batch in test_dataloder:\n",
    "    ***\n",
    "model.save_pretrained(OUTPUT_PATH)\n",
    "```\n",
    "\n",
    "ä½¿ç”¨äº†Trainerä¹‹åï¼Œå°±èƒ½å¤§å¤§çš„ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼š\n",
    "\n",
    "```\n",
    "# åŠ è½½æ•°æ®\n",
    "train_data, test_data = get_data()\n",
    "# è½¬æ¢æˆfeaturesï¼Œè·å¾—dataset\n",
    "train_dataset = MyDataset(train_data, args)\n",
    "test_dataset = MyDataset(test_data, args)\n",
    "# è¯»å…¥train_args\n",
    "train_args = **\n",
    "\n",
    "\n",
    "#åˆå§‹åŒ–æœ¬Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=build_compute_metrics_fn(data_args.task_name),\n",
    ")\n",
    "# è®­ç»ƒ\n",
    "if training_args.do_train:\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    # trainerä¿å­˜æ¨¡å‹ï¼Œé‡Œé¢è°ƒç”¨çš„è¿˜æ˜¯save_pretrainedçš„æ–¹æ³•\n",
    "    trainer.save_model()\n",
    "    # ä¿å­˜tokenizerè‡³åŒä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œæ–¹ä¾¿ä½¿ç”¨\n",
    "    if trainer.is_world_master():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)\n",
    "```\n",
    "\n",
    "ä»ä¸Šé¢çš„ä»£ç ï¼Œå¯ä»¥çœ‹å‡ºï¼ŒTrainerç®€åŒ–çš„è¿‡ç¨‹å°±æ˜¯è®­ç»ƒçš„è¿‡ç¨‹ã€‚å¯¹äºä¼ ç»Ÿçš„è®­ç»ƒä¸­å¸¸è§çš„è¿‡ç¨‹è¿›è¡Œäº†å°è£…ï¼Œæˆ‘ä»¬åªè¦å»åˆå§‹åŒ–Trainerè¿™ä¸ªç±»ï¼Œå°±å¾ˆæ–¹ä¾¿çš„å»è®­ç»ƒï¼Œä¿å­˜æ¨¡å‹ä½¿ç”¨æ˜¯save_modelï¼Œå†…éƒ¨è°ƒç”¨çš„è¿˜æ˜¯ä¸Šæ–‡æåˆ°çš„save_pretrainedçš„æ–¹æ³•ã€‚ä¸‹é¢çš„ç« èŠ‚ä¹Ÿä¼šç”¨åˆ°Trainerç±»è¿›è¡Œè®­ç»ƒï¼Œæ›´è¯¦ç»†çš„å¤§å®¶å¯ä»¥çœ‹Trainerçš„APIè¯´æ˜[8]åŠæºä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å››èŠ‚ GLUE/MRPCæ•°æ®é›†è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 æœ¬èŠ‚è¯´æ˜\n",
    "\n",
    "æœ¬èŠ‚è¯¾ä»¥ç®€å•çš„ä¾‹å­ï¼Œè¯´æ˜æ–‡æœ¬åˆ†ç±»çš„fine-tuneè¯¦ç»†è¿‡ç¨‹ï¼Œçº¿ä¸Šéƒ¨ç½²ä»£ç ã€‚\n",
    "\n",
    "4.2èŠ‚è¯¦ç»†å®ç°äº†æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è¯¦ç»†ä»£ç ï¼Œ4.3èŠ‚å®ç°äº†åŸºäº4.2èŠ‚è®­ç»ƒå¥½çš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹çš„çº¿ä¸Šé¢„æµ‹ä»£ç ï¼Œ4.4èŠ‚æ€»ç»“äº†æ–‡æœ¬åˆ†ç±»çš„åŸºæœ¬ç»éªŒå’Œtransformersä½¿ç”¨çš„åŸºæœ¬ç»éªŒã€‚\n",
    "\n",
    "æœ¬èŠ‚æ¶‰åŠåˆ°çš„æ¨¡å‹æ—¶bert-base-uncasedï¼Œæ¶‰åŠåˆ°çš„æ•°æ®é›†æ˜¯glueæ•°æ®é›†ä¸‹çš„MRPCæ•°æ®é›†ã€‚\n",
    "\n",
    "glueæ•°æ®é›†ï¼Œå…±æœ‰9ä¸ªä»»åŠ¡ï¼Œå…¶ä¸­STS-Bæ˜¯ä¸€ä¸ªå›å½’ä»»åŠ¡ï¼ŒMNLIæ˜¯ä¸‰åˆ†ç±»ä»»åŠ¡ï¼Œå‰©ä½™7ç±»å‡æ˜¯äºŒåˆ†ç±»ä»»åŠ¡ã€‚æ›´è¯¦ç»†çš„glueæ•°æ®é›†çš„ä¿¡æ¯ï¼Œå¯ä»¥å‚è€ƒ[6]ã€‚ä¹ä¸ªä»»åŠ¡ä¹‹ä¸€çš„MRPC(The Microsoft Research Paraphrase Corpusï¼Œå¾®è½¯ç ”ç©¶é™¢é‡Šä¹‰è¯­æ–™åº“)ï¼Œç›¸ä¼¼æ€§å’Œé‡Šä¹‰ä»»åŠ¡ï¼Œæ˜¯ä»åœ¨çº¿æ–°é—»æºä¸­è‡ªåŠ¨æŠ½å–å¥å­å¯¹è¯­æ–™åº“ï¼Œå¹¶äººå·¥æ³¨é‡Šå¥å­å¯¹ä¸­çš„å¥å­æ˜¯å¦åœ¨è¯­ä¹‰ä¸Šç­‰æ•ˆã€‚ç±»åˆ«å¹¶ä¸å¹³è¡¡ï¼Œå…¶ä¸­68%çš„æ­£æ ·æœ¬ï¼Œæ‰€ä»¥éµå¾ªå¸¸è§„çš„åšæ³•ï¼ŒæŠ¥å‘Šå‡†ç¡®ç‡ï¼ˆaccuracyï¼‰å’ŒF1å€¼ã€‚æ ·æœ¬ä¸ªæ•°ï¼šè®­ç»ƒé›†3, 668ä¸ªï¼Œå¼€å‘é›†408ä¸ªï¼Œæµ‹è¯•é›†1, 725ä¸ªã€‚ä»»åŠ¡ï¼šæ˜¯å¦é‡Šä¹‰äºŒåˆ†ç±»ï¼Œæ˜¯é‡Šä¹‰ï¼Œä¸æ˜¯é‡Šä¹‰ä¸¤ç±»ã€‚è¯„ä»·å‡†åˆ™ï¼šå‡†ç¡®ç‡ï¼ˆaccuracyï¼‰å’ŒF1å€¼ã€‚æ ‡ç­¾ä¸º1ï¼ˆæ­£æ ·æœ¬ï¼Œäº’ä¸ºé‡Šä¹‰ï¼‰çš„æ ·ä¾‹ï¼ˆæ¯ä¸ªæ ·ä¾‹æ˜¯ä¸¤å¥è¯ï¼Œä¸­é—´ç”¨tabéš”å¼€ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 æ¨¡å‹è®­ç»ƒçš„è¯¦ç»†è¿‡ç¨‹ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformersæä¾›äº†configã€tokenizerã€modelç­‰ç±»ç®€åŒ–äº†åˆ†è¯ã€æ¨¡å‹ç­‰æ­¥éª¤ï¼ŒåŒæ—¶åˆæœ‰Trainerç±»ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚é‚£ä¹ˆæ›´è¯¦ç»†çš„è®­ç»ƒè¿‡ç¨‹æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæœ¬èŠ‚ä¸»è¦çš„å†…å®¹å°±æ˜¯å®ç°å’Œè®²è§£æ¨¡å‹åˆ†ç±»çš„è¯¦ç»†è¿‡ç¨‹ã€‚\n",
    "\n",
    "ç®€å•çš„è®²ï¼Œä¸»è¦åˆ†ä¸ºå‡ ä¸ªæ­¥éª¤ï¼ŒåŠ è½½å‚æ•°ï¼Œæ–‡æœ¬å¤„ç†æˆDatasetã€‚å†™collate_fnï¼Œç”¨äºå¤„ç†paddingã€‚åŠ è½½configã€tokenizerã€modelç­‰ã€‚å†™metricsï¼Œç”¨äºè¯„ä¼°æ•ˆæœã€‚å°†è¿™äº›å‚æ•°éƒ½é€å…¥Trainerå»åˆå§‹åŒ–æ­¤ç±»ï¼Œç„¶åè°ƒç”¨Trainerçš„trainæ–¹æ³•å»è®­ç»ƒã€‚\n",
    "\n",
    "é¦–å…ˆè¦åŠ è½½å‚æ•°ï¼Œè¿™äº›å‚æ•°åˆ†ä¸ºä¸‰ç±»ï¼Œä¸€ç±»æ˜¯modelç›¸å…³å‚æ•°ï¼Œç”¨äºè®°å½•æ¨¡å‹çš„ä½ç½®ç­‰ä¿¡æ¯ï¼Œç”¨äºåˆå§‹åŒ–æ¨¡å‹ï¼›ä¸€ç±»æ˜¯æ•°æ®å‚æ•°ï¼Œæ•°æ®çš„ä½ç½®ï¼Œä»»åŠ¡åç§°ç­‰ç”¨äºæä¾›æ¨¡å‹è¾“å…¥å‰çš„å‚æ•°ï¼›ä¸€ç±»æ˜¯è®­ç»ƒå‚æ•°ï¼Œè¿™äº›æ˜¯æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°ï¼Œæ¯”å¦‚learning_rateã€epochsç­‰ã€‚\n",
    "\n",
    "æ–‡æœ¬æœ€å¼€å§‹éœ€è¦è½½å…¥ï¼Œå¯ä»¥é€šè¿‡å†™æ˜ä¸€ä¸ªProcessorç±»ï¼Œè¿™ä¸ªç±»ç”¨äºæä¾›å‡ ä¸ªæ–¹æ³•ï¼šè·å¾—è®­ç»ƒæ ·æœ¬ã€è·å¾—å¼€å‘æ ·æœ¬ã€è·å¾—æ ‡ç­¾listç­‰ã€‚è¿™é‡Œçš„è·å¾—çš„æ ·æœ¬æ˜¯ä¸€ä¸ªlistï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªexampleï¼Œexampleé‡ŒåŒ…å«æ–‡æœ¬å’Œå¯¹åº”çš„æ ‡ç­¾ï¼ˆæœ‰çš„ä¸å«ï¼Œæ¯”å¦‚æµ‹è¯•é›†ï¼‰ã€‚è¿™ä¸ªprocessoræä¾›çš„æ–¹æ³•ä¸»è¦æ˜¯ä¸ºäº†Datasetç±»ä½¿ç”¨ï¼ŒDatasetå®ç°å•ä¸ªè¾“å…¥çš„æ ·æœ¬ã€‚\n",
    "\n",
    "collate_fnæ˜¯Dataloaderç±»çš„è¾“å…¥ï¼Œå¯¹äºå¤„ç†å¥½çš„æ’å®šé•¿åº¦çš„featureï¼Œå¯ä»¥ä¸è¾“å…¥collate_fnï¼Œä½¿ç”¨é»˜è®¤çš„collate_fnï¼Œå¯¹äºé•¿åº¦ä¸å®šæ¶‰åŠåˆ°paddingçš„æ–‡æœ¬ï¼Œéœ€è¦è‡ªå·±å†™æ­¤å‚æ•°ã€‚\n",
    "\n",
    "configã€tokenizerã€modelçš„åŠ è½½æˆ‘ä»¬å·²ç»åŸºæœ¬ç†Ÿæ‚‰ã€‚\n",
    "\n",
    "metricsçš„å†™æ³•å¯ä»¥å‚è€ƒä¸‹é¢çš„æ–¹æ³•ã€‚\n",
    "\n",
    "ç„¶åå°†è¿™äº›å‚æ•°é€å…¥Trainerï¼Œå°±å¯ä»¥è®­ç»ƒå’Œè¯„ä¼°äº†ã€‚\n",
    "\n",
    "æœ¬è¿‡ç¨‹ä¸ä»…æ˜¯ä¸ºäº†å®ç°MRPCåˆ†ç±»ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå®ƒå¯ä»¥æ˜¯æ–‡æœ¬åˆ†ç±»çš„ä¸€ä¸ªæ ‡å‡†åŒ–æµç¨‹ï¼Œä¹Ÿæ˜¯pytorchä½¿ç”¨çš„æ ‡å‡†åŒ–æµç¨‹ï¼Œå¯ä»¥æ–¹ä¾¿ä»¥åæŒ‰ç…§æ­¤æ€è·¯è¿›è¡Œæ‰©å±•ã€‚ä»¥ä¸Šçš„è¯´æ³•ç›¸å¯¹æ¯”è¾ƒæŠ½è±¡ï¼Œä½†æ˜¯å®é™…æ“ä½œçš„è¿‡ç¨‹ä¸­ï¼Œè¿˜æ˜¯éå¸¸æ˜äº†çš„ï¼Œæ‰€ä»¥è¯·çœ‹ä¸‹é¢çš„ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import enum\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, Optional, List, Union, NamedTuple\n",
    "\n",
    "import filelock\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½å¥½çš„é¢„è®­ç»ƒæ¨¡å‹ä½ç½®\n",
    "BERT_MODEL_NAME_OR_PATH = 'transformers_data_and_model/bert-base-uncased'\n",
    "# ä¸‹è½½å¥½çš„glueæ•°æ®é›†ä¸­çš„MRPCæ•°æ®é›†çš„ä½ç½®\n",
    "MRPC_DATA_DIR = 'transformers_data_and_model/MRPC'\n",
    "# finetuneå¥½çš„modelã€tokenizerç­‰å„ç§å‚æ•°å­˜æ”¾çš„ä½ç½®\n",
    "FINETUNED_MRPC = 'transformers_data_and_model/finetuned-mrpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥å¿—æ–‡ä»¶\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ¨¡å‹å‚æ•°ï¼ŒåŒ…å«modelã€configã€tokenizerã€cache_dirç­‰\n",
    "# æœ‰ä¸‰ç±»å‚æ•°ï¼š\n",
    "#   ä¸€ä¸ªæ˜¯æ¨¡å‹å‚æ•°ï¼Œè¿™ä¸ªå°±æ˜¯ä¸‹é¢çš„å®šä¹‰ï¼›\n",
    "#   ä¸€ä¸ªæ˜¯æ¨¡å‹è®­ç»ƒå‚æ•°ï¼Œ å¯ä»¥å‚è€ƒtransformers/src/train_args.pyæ–‡ä»¶ï¼Œä¸»è¦æ˜¯epochã€batch_sizeç­‰å¸¸è§çš„è®­ç»ƒå‚æ•°ï¼Œä¹ŸåŒ…å«deviceè¿™ç§è®¾å¤‡å‚æ•°\n",
    "#   ä¸€ä¸ªæ•°æ®å‚æ•°ï¼Œå†³å®šæ•°æ®å¤„ç†ä»»åŠ¡çš„å‚æ•°ï¼Œä½¿ç”¨ä»€ä¹ˆæ•°æ®ï¼Œæ•°æ®åç§°ï¼Œæ˜¯å¦è¦†ç›–æ•°æ®çš„cacheï¼Œæœ€é•¿é•¿åº¦ï¼Œè¿™ä¸ªé•¿åº¦æ˜¯ç”¨äºç”Ÿæˆfeaturesä½¿ç”¨çš„\n",
    "# ä¸‹é¢è¿™ä¸ªæ˜¯æ¨¡å‹å‚æ•°\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "        \n",
    "\n",
    "# æ•°æ®ï¼ˆè®­ç»ƒä½¿ç”¨çš„ï¼‰å‚æ•°\n",
    "# è¿™äº›å‚æ•°ç”¨äºé€å…¥Datasetã€processorç­‰ï¼Œç”¨äºæ–¹ä¾¿åŠ è½½æ•°æ®ï¼Œå®Œæˆä»å¼€å§‹åˆ°æ¨¡å‹è¾“å…¥å‰çš„å‚æ•°\n",
    "@dataclass\n",
    "class GlueDataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "\n",
    "    Using `HfArgumentParser` we can turn this class\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: str = field(metadata={\"help\": \"The name of the task to train on: MRPC\"})\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .tsv files (or other data files) for the task.\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.task_name = self.task_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_args = ['--model_name_or_path', BERT_MODEL_NAME_OR_PATH,\n",
    "             '--task_name', 'MRPC',\n",
    "             '--do_train',\n",
    "             '--do_eval',\n",
    "             '--data_dir', MRPC_DATA_DIR,\n",
    "             '--max_seq_length', '128',\n",
    "             '--per_device_train_batch_size', '32',\n",
    "             '--learning_rate', '3e-5',\n",
    "             '--num_train_epochs', '3.0',\n",
    "             '--output_dir', FINETUNED_MRPC,\n",
    "             '--overwrite_cache',\n",
    "             '--overwrite_output_dir']\n",
    "\n",
    "# ä»¥åæ–°çš„å®éªŒï¼Œéœ€è¦å®ç°ä¸Šé¢çš„æ¨¡å‹å‚æ•°å’Œæ•°æ®å‚æ•°ï¼ŒåŸºæœ¬ä¸Šç…§ä¸Šé¢æ ¼å¼å»å†™ï¼Œä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•è½¬æ¢æˆå‚æ•°ç©ºé—´å³å¯\n",
    "# transformersé‡Œï¼Œæœ‰ä¸€ä¸ªHfArguementParserç”¨äºè§£æä¸Šé¢æ ¼å¼çš„å‚æ•°ï¼Œä¸ºæ ‡å‡†çš„pythonå‚æ•°\n",
    "parser = transformers.HfArgumentParser((ModelArguments, transformers.GlueDataTrainingArguments, transformers.TrainingArguments))\n",
    "# å°†ä¸‰ç±»å‚æ•°åˆ†åˆ«è§£æä¸ºå¯¹åº”çš„ç©ºé—´\n",
    "# æ¨¡å‹æœ¬èº«çš„å‚æ•°ï¼Œæ•°æ®çš„å‚æ•°ï¼Œè®­ç»ƒçš„å‚æ•°\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(input_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(model_name_or_path='transformers_data_and_model/bert-base-uncased', config_name=None, tokenizer_name=None, cache_dir=None)\n",
      "\n",
      "GlueDataTrainingArguments(task_name='mrpc', data_dir='transformers_data_and_model/MRPC', max_seq_length=128, overwrite_cache=True)\n",
      "\n",
      "TrainingArguments(output_dir='transformers_data_and_model/finetuned-mrpc', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul27_14-48-16_d1f9a32623b6', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n"
     ]
    }
   ],
   "source": [
    "# å±•ç¤ºä¸€ä¸‹æ‰€æœ‰çš„å‚æ•°\n",
    "print(f'{model_args}\\n\\n{data_args}\\n\\n{training_args}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¡®ä¿output_dirå¯ä»¥ç”¨\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:48:20 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/27/2020 14:48:20 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "07/27/2020 14:48:20 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='transformers_data_and_model/finetuned-mrpc', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul27_14-48-16_d1f9a32623b6', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n"
     ]
    }
   ],
   "source": [
    "# è®¾å®šæ—¥å¿—æ ¼å¼ï¼Œè®°å½•ä¸€äº›å…³é”®å‚æ•°ï¼Œå¹¶ä¸”å°†è®­ç»ƒå‚æ•°æ‰“å°å‡ºæ¥\n",
    "# ä¸€ä¸ªå¾ˆé‡è¦çš„æ„Ÿå—ï¼šä½¿ç”¨loggeræ‰“å°ä¸­é—´å˜é‡å¾ˆé‡è¦ï¼Œå¯¹äºtransformersåº“ï¼Œè¿˜æ˜¯æˆ‘ä»¬å­¦ä¹ ã€å·¥ä½œä¸­ï¼Œéƒ½æ˜¯å¦‚æ­¤\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "# è®¾å®šç§å­\n",
    "transformers.set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å¾—æ ‡ç­¾çš„ä¸ªæ•°\n",
    "# è¾“å‡ºçš„æ¨¡å¼ï¼Œè¿™é‡Œæ˜¯classificationä¸regressionä¸¤ç§\n",
    "# å¦‚æœé€‚é…æ–°ä»»åŠ¡ï¼Œæˆ‘ä»¬è¦çš„ä¸æ˜¯å»æŒ‰ç…§è¿™ç§æ ¼å¼ï¼Œè€Œæ˜¯è¦å¾—åˆ°è¿™ä¸¤ä¸ªå‚æ•°\n",
    "num_labels = 2\n",
    "output_mode = 'classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:48:25 - INFO - transformers.configuration_utils -   loading configuration file transformers_data_and_model/bert-base-uncased/config.json\n",
      "07/27/2020 14:48:25 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2020 14:48:25 - INFO - transformers.configuration_utils -   loading configuration file transformers_data_and_model/bert-base-uncased/config.json\n",
      "07/27/2020 14:48:25 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2020 14:48:25 - INFO - transformers.tokenization_utils_base -   Model name 'transformers_data_and_model/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'transformers_data_and_model/bert-base-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/27/2020 14:48:25 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/bert-base-uncased/added_tokens.json. We won't load it.\n",
      "07/27/2020 14:48:25 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/bert-base-uncased/special_tokens_map.json. We won't load it.\n",
      "07/27/2020 14:48:25 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/bert-base-uncased/tokenizer_config.json. We won't load it.\n",
      "07/27/2020 14:48:25 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/bert-base-uncased/tokenizer.json. We won't load it.\n",
      "07/27/2020 14:48:25 - INFO - transformers.tokenization_utils_base -   loading file transformers_data_and_model/bert-base-uncased/vocab.txt\n",
      "07/27/2020 14:48:25 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 14:48:25 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 14:48:25 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 14:48:25 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 14:48:25 - INFO - transformers.modeling_utils -   loading weights file transformers_data_and_model/bert-base-uncased/pytorch_model.bin\n",
      "07/27/2020 14:48:28 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at transformers_data_and_model/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "07/27/2020 14:48:28 - WARNING - transformers.modeling_utils -   Some weights of BertForSequenceClassification were not initialized from the model checkpoint at transformers_data_and_model/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½configã€tokenizerã€modelè¿™ä¸‰ä¸ªã€‚\n",
    "# configæ˜¯åŒ…å«å±‚æ•°ã€dropoutå‚æ•°ã€headä¸ªæ•°ã€finetuneä»»åŠ¡ç­‰æ¨¡å‹ç›¸å…³å†…å®¹çš„å‚æ•°ï¼Œè¿™ä¸ªå‚æ•°åŠ è½½ååªæ˜¯ä¸ºäº†modelä½¿ç”¨ã€‚\n",
    "# configå†…å†™å…¥æ ‡ç­¾çš„ä¸ªæ•°num_labelsï¼Œå†³å®šmodelåé¢åˆ†ç±»ä½¿ç”¨çš„å…¨è¿æ¥çš„è¾“å‡ºçš„ä¸ªæ•°\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=data_args.task_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.DataProcessoræ˜¯ä¸€ä¸ªåŸºç±»ï¼Œéœ€è¦å®ç°get_train_examples,get_dev_examples, get_test_examples, get_labelsç­‰å‡ ä¸ªå‡½æ•°ï¼Œ\n",
    "# åˆ†åˆ«ç”¨äºæä¾›çš„InputExampleçš„é›†å’Œï¼ˆlistï¼‰å’Œæ ‡ç­¾çš„é›†å’Œ\n",
    "class MrpcProcessor(transformers.DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"sentence2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = None if set_type == \"test\" else line[0]\n",
    "            examples.append(transformers.InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†examplesè½¬æ¢æˆfeatures\n",
    "def glue_convert_examples_to_features(\n",
    "    examples: List[transformers.InputExample],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_length: Optional[int] = None,\n",
    "    task=None,\n",
    "    label_list=None,\n",
    "    output_mode=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of ``InputFeatures``\n",
    "\n",
    "    Args:\n",
    "        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
    "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
    "        max_length: Maximum example length. Defaults to the tokenizer's max_len\n",
    "        task: GLUE task\n",
    "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
    "        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n",
    "\n",
    "    Returns:\n",
    "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
    "        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n",
    "        a list of task-specific ``InputFeatures`` which can be fed to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = tokenizer.max_len\n",
    "\n",
    "    if task is not None:\n",
    "        processor = MrpcProcessor()\n",
    "        if label_list is None:\n",
    "            label_list = processor.get_labels()\n",
    "            logger.info(\"Using label list %s for task %s\" % (label_list, task))\n",
    "        if output_mode is None:\n",
    "            output_mode = glue_output_modes[task]\n",
    "            logger.info(\"Using output mode %s for task %s\" % (output_mode, task))\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    def label_from_example(example: transformers.InputExample) -> Union[int, float, None]:\n",
    "        if example.label is None:\n",
    "            return None\n",
    "        if output_mode == \"classification\":\n",
    "            return label_map[example.label]\n",
    "        elif output_mode == \"regression\":\n",
    "            return float(example.label)\n",
    "        raise KeyError(output_mode)\n",
    "\n",
    "    labels = [label_from_example(example) for example in examples]\n",
    "\n",
    "    batch_encoding = tokenizer(\n",
    "        [(example.text_a, example.text_b) for example in examples],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    features = []\n",
    "    for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "\n",
    "        feature = transformers.InputFeatures(**inputs, label=labels[i])\n",
    "        features.append(feature)\n",
    "\n",
    "    for i, example in enumerate(examples[:5]):\n",
    "        logger.info(\"*** Example ***\")\n",
    "        logger.info(\"guid: %s\" % (example.guid))\n",
    "        logger.info(\"features: %s\" % features[i])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(enum.Enum):\n",
    "    train = \"train\"\n",
    "    dev = \"dev\"\n",
    "    test = \"test\"\n",
    "    \n",
    "    \n",
    "class GlueDataset(torch.utils.data.dataset.Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    args: GlueDataTrainingArguments\n",
    "    output_mode: str\n",
    "    features: List[transformers.InputFeatures]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: transformers.GlueDataTrainingArguments,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        limit_length: Optional[int] = None,\n",
    "        mode: Union[str, Split] = Split.train,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        output_mode = \"classification\",\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.processor = MrpcProcessor()\n",
    "        self.output_mode = output_mode\n",
    "        if isinstance(mode, str):\n",
    "            try:\n",
    "                mode = Split[mode]\n",
    "            except KeyError:\n",
    "                raise KeyError(\"mode is not a valid split name\")\n",
    "        # Load data features from cache or dataset file\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else args.data_dir,\n",
    "            \"cached_{}_{}_{}_{}\".format(\n",
    "                mode.value, tokenizer.__class__.__name__, str(args.max_seq_length), args.task_name,\n",
    "            ),\n",
    "        )\n",
    "        label_list = self.processor.get_labels()\n",
    "        self.label_list = label_list\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with filelock.FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "                start = time.time()\n",
    "                self.features = torch.load(cached_features_file)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {args.data_dir}\")\n",
    "\n",
    "                if mode == Split.dev:\n",
    "                    examples = self.processor.get_dev_examples(args.data_dir)\n",
    "                elif mode == Split.test:\n",
    "                    examples = self.processor.get_test_examples(args.data_dir)\n",
    "                else:\n",
    "                    examples = self.processor.get_train_examples(args.data_dir)\n",
    "                if limit_length is not None:\n",
    "                    examples = examples[:limit_length]\n",
    "                self.features = glue_convert_examples_to_features(\n",
    "                    examples,\n",
    "                    tokenizer,\n",
    "                    max_length=args.max_seq_length,\n",
    "                    label_list=label_list,\n",
    "                    output_mode=self.output_mode,\n",
    "                )\n",
    "                start = time.time()\n",
    "                torch.save(self.features, cached_features_file)\n",
    "                # ^ This seems to take a lot of time so I want to investigate why and how we can improve.\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> transformers.InputFeatures:\n",
    "        return self.features[i]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:48:50 - INFO - filelock -   Lock 140217473189984 acquired on transformers_data_and_model/MRPC/cached_train_BertTokenizer_128_mrpc.lock\n",
      "07/27/2020 14:48:50 - INFO - __main__ -   Creating features from dataset file at transformers_data_and_model/MRPC\n",
      "07/27/2020 14:48:50 - INFO - __main__ -   LOOKING AT transformers_data_and_model/MRPC/train.tsv\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   *** Example ***\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   guid: train-1\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   *** Example ***\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   guid: train-2\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   *** Example ***\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   guid: train-3\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102, 2006, 2238, 2184, 1010, 1996, 2911, 1005, 1055, 5608, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 1010, 5378, 1996, 14792, 2005, 5096, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   *** Example ***\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   guid: train-4\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 2105, 6021, 19481, 13938, 2102, 1010, 21628, 6661, 2020, 2039, 2539, 16653, 1010, 2030, 1018, 1012, 1018, 1003, 1010, 2012, 1037, 1002, 1018, 1012, 5179, 1010, 2383, 3041, 2275, 1037, 2501, 2152, 1997, 1037, 1002, 1018, 1012, 5401, 1012, 102, 21628, 6661, 5598, 2322, 16653, 1010, 2030, 1018, 1012, 1020, 1003, 1010, 2000, 2275, 1037, 2501, 5494, 2152, 2012, 1037, 1002, 1018, 1012, 5401, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   *** Example ***\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   guid: train-5\n",
      "07/27/2020 14:48:53 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 1996, 4518, 3123, 1002, 1016, 1012, 2340, 1010, 2030, 2055, 2340, 3867, 1010, 2000, 2485, 5958, 2012, 1002, 2538, 1012, 4868, 2006, 1996, 2047, 2259, 4518, 3863, 1012, 102, 18720, 1004, 1041, 13058, 1012, 6661, 5598, 1002, 1015, 1012, 6191, 2030, 1022, 3867, 2000, 1002, 2538, 1012, 6021, 2006, 1996, 2047, 2259, 4518, 3863, 2006, 5958, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   Saving features into cached file transformers_data_and_model/MRPC/cached_train_BertTokenizer_128_mrpc [took 0.764 s]\n",
      "07/27/2020 14:48:54 - INFO - filelock -   Lock 140217473189984 released on transformers_data_and_model/MRPC/cached_train_BertTokenizer_128_mrpc.lock\n",
      "07/27/2020 14:48:54 - INFO - filelock -   Lock 140217465209800 acquired on transformers_data_and_model/MRPC/cached_dev_BertTokenizer_128_mrpc.lock\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   Creating features from dataset file at transformers_data_and_model/MRPC\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   *** Example ***\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   guid: dev-1\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 2002, 2056, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2987, 1005, 1056, 4906, 1996, 2194, 1005, 1055, 2146, 1011, 2744, 3930, 5656, 1012, 102, 1000, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2515, 2025, 4906, 2256, 2146, 1011, 2744, 3930, 5656, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   *** Example ***\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   guid: dev-2\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 20201, 22948, 2056, 10958, 19053, 4140, 6283, 1996, 8956, 6939, 1998, 2246, 2830, 2000, 2478, 2010, 2146, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 2010, 2564, 2056, 2002, 2001, 1000, 2531, 3867, 2369, 2577, 5747, 1000, 1998, 2246, 2830, 2000, 2478, 2010, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   *** Example ***\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   guid: dev-3\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 1996, 7922, 2001, 2012, 12904, 1012, 6227, 18371, 2114, 1996, 18371, 1010, 4257, 2006, 1996, 5219, 1010, 1998, 2012, 1015, 1012, 27054, 2487, 2114, 1996, 5364, 23151, 2278, 1010, 2036, 4257, 1012, 102, 1996, 7922, 2001, 2012, 12904, 1012, 6275, 18371, 16545, 2100, 1027, 1010, 8990, 4257, 2006, 1996, 5219, 1010, 1998, 2012, 1015, 1012, 23090, 2487, 2114, 1996, 5364, 23151, 2278, 10381, 2546, 1027, 1010, 2091, 1014, 1012, 1015, 3867, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   *** Example ***\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   guid: dev-4\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 1996, 10028, 1011, 25022, 2080, 2003, 3403, 2127, 2255, 2000, 5630, 2065, 2009, 2097, 2203, 5668, 2063, 1037, 4018, 1012, 102, 1996, 10028, 1011, 25022, 2080, 2623, 9317, 2008, 2009, 2097, 5630, 1999, 2255, 3251, 2000, 2203, 5668, 2063, 1037, 4018, 2077, 1996, 27419, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   *** Example ***\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   guid: dev-5\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   features: InputFeatures(input_ids=[101, 2053, 5246, 2031, 2042, 2275, 2005, 1996, 2942, 2030, 1996, 4735, 3979, 1012, 102, 2053, 5246, 2031, 2042, 2275, 2005, 1996, 4735, 2030, 2942, 3572, 1010, 2021, 17137, 3051, 2038, 12254, 2025, 5905, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "07/27/2020 14:48:54 - INFO - __main__ -   Saving features into cached file transformers_data_and_model/MRPC/cached_dev_BertTokenizer_128_mrpc [took 0.095 s]\n",
      "07/27/2020 14:48:54 - INFO - filelock -   Lock 140217465209800 released on transformers_data_and_model/MRPC/cached_dev_BertTokenizer_128_mrpc.lock\n"
     ]
    }
   ],
   "source": [
    "# è·å¾—dataset\n",
    "train_dataset, eval_dataset, test_dataset = None, None, None\n",
    "if training_args.do_train:\n",
    "    train_dataset = GlueDataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir, output_mode=output_mode)\n",
    "if training_args.do_eval:\n",
    "    eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"dev\", cache_dir=model_args.cache_dir)\n",
    "if training_args.do_predict:\n",
    "    test_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"test\", cache_dir=model_args.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾—åˆ°è®¡ç®—ç»“æœçš„å‡½æ•°\n",
    "# è¿™é‡Œè®¡ç®—çš„æ˜¯accå’Œf1\n",
    "# EvalPredictionæ˜¯é¢„æµ‹çš„ç»“æœæ ¼å¼ï¼Œpredictionsæ˜¯é¢„æµ‹çš„ï¼Œlabels_idsæ˜¯æ­£ç¡®çš„\n",
    "class EvalPrediction(NamedTuple):\n",
    "    \"\"\"\n",
    "    Evaluation output (always contains labels), to be used to compute metrics.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (:obj:`np.ndarray`): Predictions of the model.\n",
    "        label_ids (:obj:`np.ndarray`): Targets to be matched.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions: np.ndarray\n",
    "    label_ids: np.ndarray\n",
    "        \n",
    "# å¾—åˆ°è®¡ç®—å‡½æ•°\n",
    "def compute_metrics_fn(p: EvalPrediction):\n",
    "    if output_mode == \"classification\":\n",
    "        # é¢„æµ‹çš„ç»“æœ\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "    # æ­£ç¡®çš„ç»“æœ\n",
    "    labels = p.label_ids\n",
    "    \n",
    "    # accå’Œf1\n",
    "    acc = (preds == labels).mean()\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\"acc\": acc, \"f1\": f1, \"acc_and_f1\": (acc + f1) / 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:49:02 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"
     ]
    }
   ],
   "source": [
    "#åˆå§‹åŒ–æœ¬Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:49:03 - INFO - transformers.trainer -   ***** Running training *****\n",
      "07/27/2020 14:49:03 - INFO - transformers.trainer -     Num examples = 3668\n",
      "07/27/2020 14:49:03 - INFO - transformers.trainer -     Num Epochs = 3\n",
      "07/27/2020 14:49:03 - INFO - transformers.trainer -     Instantaneous batch size per device = 32\n",
      "07/27/2020 14:49:03 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "07/27/2020 14:49:03 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "07/27/2020 14:49:03 - INFO - transformers.trainer -     Total optimization steps = 174\n",
      "07/27/2020 14:49:03 - INFO - transformers.trainer -     Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96aa92dde624a1a81f3ccf34ea6ca5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3981165aac44d5a3feafa8cd61669a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=58.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e143d08da9143bfae734aaecd59c42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=58.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea531d3b8204e51a0b1f308ce77abad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=58.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:50:08 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "07/27/2020 14:50:08 - INFO - transformers.trainer -   Saving model checkpoint to transformers_data_and_model/finetuned-mrpc\n",
      "07/27/2020 14:50:08 - INFO - transformers.configuration_utils -   Configuration saved in transformers_data_and_model/finetuned-mrpc/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:50:08 - INFO - transformers.modeling_utils -   Model weights saved in transformers_data_and_model/finetuned-mrpc/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒ\n",
    "if training_args.do_train:\n",
    "    # æ­¤æ—¶ä¼ å…¥model_pathæ˜¯ä¸ºäº†åŠ è½½optimiztorï¼Œè¿›è¡Œç»§ç»­è®­ç»ƒï¼Œå¯¹äºé€šå¸¸çš„fine-tuneæ¥è¯´ï¼Œmodel_pathå¯ä»¥ä¸ä¼ å…¥\n",
    "    # å¯¹äºåˆå§‹åŒ–åçš„Trainerï¼Œè°ƒç”¨trainæ–¹æ³•å°±å¯ä»¥è®­ç»ƒäº†ï¼Œç®€åŒ–äº†è®­ç»ƒçš„è¿‡ç¨‹\n",
    "    trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "    )\n",
    "    # Trainerä¿å­˜æ¨¡å‹è°ƒç”¨æ­¤æ–¹æ³•\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨èµ·è§ï¼Œå°†tokenizerçš„æ¨¡å‹å‚æ•°ä¹Ÿå­˜å…¥modelåŒç›®å½•\n",
    "    if trainer.is_world_master():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:50:08 - INFO - __main__ -   *** Evaluate ***\n",
      "07/27/2020 14:50:08 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "07/27/2020 14:50:08 - INFO - transformers.trainer -     Num examples = 408\n",
      "07/27/2020 14:50:08 - INFO - transformers.trainer -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdea439e7a54e69a895f35c5b920e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=26.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:50:10 - INFO - transformers.trainer -   {'eval_loss': 0.4694334108095903, 'eval_acc': 0.8112745098039216, 'eval_f1': 0.8718801996672212, 'eval_acc_and_f1': 0.8415773547355714, 'epoch': 3.0, 'step': 174}\n",
      "07/27/2020 14:50:10 - INFO - __main__ -   ***** Eval results mrpc *****\n",
      "07/27/2020 14:50:10 - INFO - __main__ -     eval_loss = 0.4694334108095903\n",
      "07/27/2020 14:50:10 - INFO - __main__ -     eval_acc = 0.8112745098039216\n",
      "07/27/2020 14:50:10 - INFO - __main__ -     eval_f1 = 0.8718801996672212\n",
      "07/27/2020 14:50:10 - INFO - __main__ -     eval_acc_and_f1 = 0.8415773547355714\n",
      "07/27/2020 14:50:10 - INFO - __main__ -     epoch = 3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°ç»“æœ\n",
    "eval_results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    # ä¼ å…¥metricsï¼Œå¯¹äºå‰é¢åˆå§‹åŒ–å·²ç»ä¼ å…¥çš„ï¼Œæ­¤æ—¶å¦‚æœæ²¡å˜åŒ–ï¼Œå¯ä»¥çœç•¥æ­¤æ­¥éª¤\n",
    "    trainer.compute_metrics = compute_metrics_fn\n",
    "    # ä¼ å…¥è¯„ä¼°çš„dataset\n",
    "    eval_result = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "    output_eval_file = os.path.join(\n",
    "        training_args.output_dir, f\"eval_results_{eval_dataset.args.task_name}.txt\"\n",
    "    )\n",
    "    if trainer.is_world_master():\n",
    "        # å†™å…¥æœ¬åœ°æ–‡ä»¶\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results {} *****\".format(eval_dataset.args.task_name))\n",
    "            for key, value in eval_result.items():\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        eval_results.update(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•é›†çš„è¯„ä¼°\n",
    "if training_args.do_predict:\n",
    "    logging.info(\"*** Test ***\")\n",
    "\n",
    "    predictions = trainer.predict(test_dataset=test_dataset).predictions\n",
    "    if output_mode == \"classification\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    output_test_file = os.path.join(\n",
    "        training_args.output_dir, f\"test_results_{test_dataset.args.task_name}.txt\"\n",
    "    )\n",
    "    if trainer.is_world_master():\n",
    "        with open(output_test_file, \"w\") as writer:\n",
    "            logger.info(\"***** Test results {} *****\".format(test_dataset.args.task_name))\n",
    "            writer.write(\"index\\tprediction\\n\")\n",
    "            for index, item in enumerate(predictions):\n",
    "                item = test_dataset.get_labels()[item]\n",
    "                writer.write(\"%d\\t%s\\n\" % (index, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4694334108095903, 'eval_acc': 0.8112745098039216, 'eval_f1': 0.8718801996672212, 'eval_acc_and_f1': 0.8415773547355714, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FINETUNED_MRPC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8aff68b1d1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFINETUNED_MRPC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'FINETUNED_MRPC' is not defined"
     ]
    }
   ],
   "source": [
    "FINETUNED_MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers_data_and_model/finetuned-mrpc\n",
      "â”œâ”€â”€ config.json\n",
      "â”œâ”€â”€ eval_results_mrpc.txt\n",
      "â”œâ”€â”€ pytorch_model.bin\n",
      "â”œâ”€â”€ special_tokens_map.json\n",
      "â”œâ”€â”€ tokenizer_config.json\n",
      "â”œâ”€â”€ training_args.bin\n",
      "â””â”€â”€ vocab.txt\n",
      "\n",
      "0 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶\n",
    "!tree {FINETUNED_MRPC}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ä¸ºMRPCæ•°æ®é›†éå¸¸å°ï¼Œä»»åŠ¡ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œ3ä¸ªepochï¼Œå­¦ä¹ ç‡3e-5ï¼Œå°±èƒ½è½»æ¾çš„å®Œæˆäº†è®­ç»ƒï¼Œå¯ä»¥çœ‹åˆ°åœ¨éªŒè¯é›†ä¸Šaccuracyè¾¾åˆ°äº†81.1%ï¼ŒF1å€¼è¾¾åˆ°äº†0.87ã€‚è®­ç»ƒå¥½çš„æ¨¡å‹modelå’Œtokenizeréƒ½ä¿å­˜åœ¨äº†FINETUNE_MRPCæ–‡ä»¶å¤¹ä¸‹äº†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 çº¿ä¸Šé¢„æµ‹\n",
    "\n",
    "è®­ç»ƒå¥½äº†æ¨¡å‹ï¼Œå°±å¯ä»¥çº¿ä¸Šè¿›è¡Œé¢„æµ‹äº†ï¼Œæ€ä¹ˆåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹å‘¢ï¼Ÿä¹Ÿéå¸¸ç®€å•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.4.0+cu100\n",
      "transformers: 3.0.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f'torch: {torch.__version__}')\n",
    "print(f'transformers: {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:54:12 - INFO - transformers.configuration_utils -   loading configuration file transformers_data_and_model/finetuned-mrpc/config.json\n",
      "07/27/2020 14:54:12 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2020 14:54:12 - INFO - transformers.tokenization_utils_base -   Model name 'transformers_data_and_model/finetuned-mrpc' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'transformers_data_and_model/finetuned-mrpc' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/27/2020 14:54:12 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/finetuned-mrpc/added_tokens.json. We won't load it.\n",
      "07/27/2020 14:54:12 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/finetuned-mrpc/tokenizer.json. We won't load it.\n",
      "07/27/2020 14:54:12 - INFO - transformers.tokenization_utils_base -   loading file transformers_data_and_model/finetuned-mrpc/vocab.txt\n",
      "07/27/2020 14:54:12 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 14:54:12 - INFO - transformers.tokenization_utils_base -   loading file transformers_data_and_model/finetuned-mrpc/special_tokens_map.json\n",
      "07/27/2020 14:54:12 - INFO - transformers.tokenization_utils_base -   loading file transformers_data_and_model/finetuned-mrpc/tokenizer_config.json\n",
      "07/27/2020 14:54:12 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 14:54:12 - INFO - transformers.configuration_utils -   loading configuration file transformers_data_and_model/finetuned-mrpc/config.json\n",
      "07/27/2020 14:54:12 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2020 14:54:12 - INFO - transformers.modeling_utils -   loading weights file transformers_data_and_model/finetuned-mrpc/pytorch_model.bin\n",
      "07/27/2020 14:54:15 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "07/27/2020 14:54:15 - INFO - transformers.modeling_utils -   All the weights of BertForSequenceClassification were initialized from the model checkpoint at transformers_data_and_model/finetuned-mrpc.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 38%\n",
      "is paraphrase: 62%\n",
      "classification result: is paraphrase\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½tokenizerå’Œmodel\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(FINETUNED_MRPC)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(FINETUNED_MRPC)\n",
    "\n",
    "# ç±»åˆ«æ ‡ç­¾\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# é€šè¿‡tokenizerå°†æ–‡æœ¬è½¬æˆmodeléœ€è¦çš„æ ¼å¼ï¼Œè¿”å›ä¸ºpytorchçš„tensor\n",
    "paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # æ˜¯é‡Šä¹‰çš„æ ·ä¾‹\n",
    "    # è¾“å…¥æ¨¡å‹ï¼Œæ¨¡å‹è¾“å‡ºä¸ºå…ƒç»„ï¼Œä¸è¾“å…¥æ­£ç¡®æ ‡ç­¾labelså‚æ•°çš„æƒ…å†µä¸‹ï¼Œç¬¬ä¸€ä¸ªlogits\n",
    "    paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "    # logitsç»è¿‡softmaxå¾—åˆ°æœ€ç»ˆçš„æ¦‚ç‡\n",
    "    paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "# ç»“æœåº”å½“æ˜¯é‡Šä¹‰\n",
    "for i in range(len(classes)):\n",
    "     print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "print(f\"classification result: {classes[paraphrase_results.index(max(paraphrase_results))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ€»ç»“ï¼šå¯ä»¥çœ‹åˆ°ï¼Œçº¿ä¸Šç”¨äºé¢„æµ‹çš„ä»£ç ä¹Ÿéå¸¸ç®€å•ã€‚åŠ è½½tokenizerå’Œmodelï¼Œç„¶åé€šè¿‡tokenizeræ ‡å‡†åŒ–ä¸ºmodelçš„è¾“å…¥ï¼Œæ¨¡å‹è¾“å…¥è¾“å‡ºä¸ºå…ƒç»„ï¼Œå…ƒç»„çš„ç»“æœè¿›è¡Œsoftmaxï¼Œå¾—åˆ°å„ä¸ªlabelçš„æ¦‚ç‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„ç»“æœæ¦‚ç‡ã€‚å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿›è¡Œçº¿ä¸Šéƒ¨ç½²çš„ä»£ç ä¹Ÿéå¸¸ç®€å•å’Œå®¹æ˜“ã€‚\n",
    "\n",
    "ä¸Šé¢ä¸ºäº†ç®€å•èµ·è§ï¼Œæ²¡æœ‰ä½¿ç”¨GPUã€‚å¦‚æœçº¿ä¸Šéƒ¨ç½²éœ€è¦æ”¯æŒGPUï¼Œä¹Ÿå¾ˆç®€å•ï¼Œåªè¦å°†tokenizerè¾“å‡ºçš„ç»“æœä¼ å…¥GPUï¼Œmodelä¹Ÿä¼ å…¥GPUå³å¯ã€‚ä¸€èˆ¬tokenizerè¾“å‡ºç»“æœæ˜¯ä¸€ä¸ªè¯å…¸æ ¼å¼ï¼Œè¯å…¸çš„valueä¸ºtensorï¼Œå¯ä»¥è½¬å…¥GPUï¼Œè¯·çœ‹ä¸‹é¢çš„ä¾‹å­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 14:53:36 - INFO - transformers.configuration_utils -   loading configuration file transformers_data_and_model/finetuned-mrpc/config.json\n",
      "07/27/2020 14:53:36 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2020 14:53:36 - INFO - transformers.tokenization_utils_base -   Model name 'transformers_data_and_model/finetuned-mrpc' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'transformers_data_and_model/finetuned-mrpc' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/27/2020 14:53:36 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/finetuned-mrpc/added_tokens.json. We won't load it.\n",
      "07/27/2020 14:53:36 - INFO - transformers.tokenization_utils_base -   Didn't find file transformers_data_and_model/finetuned-mrpc/tokenizer.json. We won't load it.\n",
      "07/27/2020 14:53:36 - INFO - transformers.tokenization_utils_base -   loading file transformers_data_and_model/finetuned-mrpc/vocab.txt\n",
      "07/27/2020 14:53:36 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 14:53:36 - INFO - transformers.tokenization_utils_base -   loading file transformers_data_and_model/finetuned-mrpc/special_tokens_map.json\n",
      "07/27/2020 14:53:36 - INFO - transformers.tokenization_utils_base -   loading file transformers_data_and_model/finetuned-mrpc/tokenizer_config.json\n",
      "07/27/2020 14:53:36 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 14:53:36 - INFO - transformers.configuration_utils -   loading configuration file transformers_data_and_model/finetuned-mrpc/config.json\n",
      "07/27/2020 14:53:36 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2020 14:53:36 - INFO - transformers.modeling_utils -   loading weights file transformers_data_and_model/finetuned-mrpc/pytorch_model.bin\n",
      "07/27/2020 14:53:39 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "07/27/2020 14:53:39 - INFO - transformers.modeling_utils -   All the weights of BertForSequenceClassification were initialized from the model checkpoint at transformers_data_and_model/finetuned-mrpc.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 38%\n",
      "is paraphrase: 62%\n",
      "classification result: is paraphrase\n"
     ]
    }
   ],
   "source": [
    "# æ”¯æŒGPUåˆ™ä½¿ç”¨GPUï¼Œä¸æ”¯æŒåˆ™ä½¿ç”¨CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# åŠ è½½tokenizerå’Œmodel\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(FINETUNED_MRPC)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(FINETUNED_MRPC)\n",
    "# modelè½¬å…¥cuda\n",
    "model = model.to(device)\n",
    "\n",
    "# ç±»åˆ«æ ‡ç­¾\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "# é€šè¿‡tokenizerå°†æ–‡æœ¬è½¬æˆmodeléœ€è¦çš„æ ¼å¼ï¼Œè¿”å›ä¸ºpytorchçš„tensor\n",
    "paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "# paraphraseè½¬å…¥GPU\n",
    "# valueæ˜¯tensorç±»çš„ï¼Œè½¬å…¥GPUè¿›è¡ŒåŠ é€Ÿ\n",
    "# é™¤äº†modelå’Œè¾“å…¥çš„è½¬å…¥GPUï¼Œå…¶ä»–æ— åŒºåˆ«\n",
    "for k, v in paraphrase.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        paraphrase[k] = v.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # æ˜¯é‡Šä¹‰çš„æ ·ä¾‹\n",
    "    # è¾“å…¥æ¨¡å‹ï¼Œæ¨¡å‹è¾“å‡ºä¸ºå…ƒç»„ï¼Œä¸è¾“å…¥æ­£ç¡®æ ‡ç­¾labelså‚æ•°çš„æƒ…å†µä¸‹ï¼Œç¬¬ä¸€ä¸ªlogits\n",
    "    paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "    # logitsç»è¿‡softmaxå¾—åˆ°æœ€ç»ˆçš„æ¦‚ç‡\n",
    "    paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "# ç»“æœåº”å½“æ˜¯é‡Šä¹‰\n",
    "for i in range(len(classes)):\n",
    "     print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "print(f\"classification result: {classes[paraphrase_results.index(max(paraphrase_results))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œå·²ç»å®ç°äº†çº¿ä¸Šéƒ¨ç½²çš„å…¨éƒ¨ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 transformersä½¿ç”¨æ€»ç»“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ€»ä½“æ¥è¯´ï¼ŒåŸºäºtransformersæ¡†æ¶ï¼Œå†™ä»£ç ä¼šéå¸¸ç®€å•ï¼Œæ€»ä½“æ­¥éª¤ä¹Ÿéå¸¸å°‘ã€‚\n",
    "\n",
    "1. å‚æ•°çš„ä¼ å…¥ï¼Œå‚æ•°åŒ…å«ä¸‰ç§ç±»å‹çš„å‚æ•°ï¼šæ¨¡å‹çš„å‚æ•°ï¼Œæ¨¡å‹è®­ç»ƒçš„å‚æ•°ï¼Œæ•°æ®çš„å‚æ•°ã€‚\n",
    "2. æ•°æ®çš„å¤„ç†ï¼Œä»åŸå§‹æ•°æ®åˆ°æ¨¡å‹å¯æ¥å—çš„æ•°æ®ï¼Œæœ€ç»ˆæ˜¯å°†æ•°æ®åˆ†æˆå¯ä»¥è¿­ä»£çš„ï¼Œå®šé•¿çš„ã€æ ‡å‡†çš„batchï¼Œä¾›æ¨¡å‹ä½¿ç”¨ã€‚è¿™é‡Œé¢é€šå¸¸æ ‡å‡†åŒ–ä¸ºå‡ ä¸ªæ­¥éª¤ï¼Œè¿™ä¹Ÿæ˜¯pytorchä»£ç ç¼–å†™çš„æµç¨‹ï¼š\n",
    "\n",
    "ç¬¬ä¸€æ­¥ï¼Œè®¾ç«‹processorï¼Œå®ŒæˆåŠ è½½æ•°æ®é›†ä¸ºexamplesçš„é›†å’Œï¼ˆlistï¼‰ï¼Œæ¯ä¸ªexampleå¯ä»¥ä¸ºä¸€ä¸ªInputExampleå¯¹è±¡ã€‚è¿™é‡Œé€šå¸¸è¿˜æ˜¯æ–‡æœ¬ï¼Œåªæ˜¯text_a, text_bï¼Œlabelå˜ä¸ºInputExampleçš„å±æ€§ã€‚\n",
    "\n",
    "ç¬¬äºŒæ­¥ï¼Œè½¬æ¢ä¸ºtorch.utils.data.Datasetï¼Œpytorchçš„Datasetæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œæˆ‘ä»¬éœ€è¦é‡å†™__len__,\\_\\_getitem__è¿™ä¸¤ä¸ªæ–¹æ³•ã€‚æœ€ç»ˆçš„ç›®æ ‡æ˜¯å°†æ–‡æœ¬å˜æˆä¸€ä¸ªä¸ªæ¨¡å‹å¯ä»¥ç”¨çš„ä¸€æ¡æ¡çš„æ•°æ®ã€‚åœ¨è¿™é‡Œå°†æ¯ä¸ªInputExampleè½¬æ¢æˆfeatureï¼Œè€ŒInputExampleè½¬æ¢æˆfeatureçš„æ—¶å€™ï¼Œå°±éœ€è¦tokenizerå‘æŒ¥ä½œç”¨äº†ï¼Œè¿™é‡Œå¯ä»¥è¿›è¡Œpaddingå’Œtruncationã€‚\n",
    "\n",
    "ç¬¬ä¸‰æ­¥ï¼Œåœ¨transformersé‡Œï¼Œå¯ä»¥å°†ä¸Šä¸€æ­¥å¾—åˆ°çš„datasetä¼ å…¥Trainerè¿›è¡Œä½¿ç”¨ï¼ŒTrainerä¼šè‡ªåŠ¨å¤„ç†æ­¤æ­¥éª¤ï¼Œä¸å¿…è¿‡åˆ†æ“å¿ƒé‡Œé¢çš„äº‹æƒ…ã€‚å¯¹äºé€šå¸¸çš„pytorchæ¥è¯´ï¼Œæ­¤æ­¥éª¤æ˜¯å°†datasetè½¬æ¢ä¸ºdataloaderï¼Œè¦å†³å®šbatch_sizeï¼Œæ˜¯å¦shuffleï¼ŒæŠ½æ ·æ–¹æ³•ï¼Œcollate_fnèšåˆæ–¹å¼ã€‚å¯¹äºtransformersé‡Œï¼Œcollate_fnå¯ä»¥ä½¿ç”¨é»˜è®¤æˆ–è€…è‡ªå·±å†™çš„ä¼ å…¥ï¼Œå¯ä»¥å°†batch_sizeé€šè¿‡1çš„å‚æ•°ä¼ å…¥ã€‚\n",
    "\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n",
    "\n",
    "3. å®šä¹‰metricsç”¨æ¥è¯„ä¼°é¢„æµ‹çš„æ•ˆæœã€‚å¦‚æœæœ‰éœ€è¦ï¼Œä¹Ÿéœ€è¦å†™collate_fnèšåˆå‡½æ•°ã€‚\n",
    "4. å¯¹äºå·²ç»å¤„ç†å¥½çš„æ•°æ®train_datasetã€dev_datasetã€test_datasetï¼Œè¿åŒmodelã€tokenizerã€metricã€argsä¸€åŒé€å…¥trainerï¼Œè¿›è¡Œè®­ç»ƒï¼Œä¿å­˜ã€‚è¿™é‡Œä½¿ç”¨çš„æ˜¯save_modelï¼Œå…¶å®å°±æ˜¯è°ƒç”¨çš„æ ‡å‡†çš„save_pretrainedçš„æ¥å£ã€‚\n",
    "\n",
    "é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œå·²ç»æŒæ¡äº†æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è¯¦ç»†è®­ç»ƒè¿‡ç¨‹ã€çº¿ä¸Šéƒ¨ç½²çš„ä»£ç ï¼›å¯¹äºæ–‡æœ¬åˆ†ç±»çš„æµç¨‹åŠtransformersã€pytorchä»£ç çš„ä¹¦å†™ï¼Œä½¿ç”¨å·²ç»ç†Ÿæ‚‰ï¼›å¯¹äºæ–°çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡å·²ç»å¯ä»¥ç‹¬ç«‹å®Œæˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äº”èŠ‚ GPT2è®­ç»ƒä½¿ç”¨ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 æœ¬èŠ‚è¯´æ˜\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLPé¢†åŸŸï¼Œé™¤äº†å¸¸è§çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œè¿˜æœ‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚éšç€é¢„è®­ç»ƒçš„ç«çƒ­ï¼Œå¾ˆå¤šäººå¯èƒ½æƒ³å°è¯•è‡ªå·±é¢„è®­ç»ƒä¸€ä¸ªGPT2æ¨¡å‹æˆ–è€…å› ä¸šåŠ¡éœ€è¦fine-tune GPT2æ¨¡å‹ã€‚æœ¬èŠ‚è§£å†³çš„å°±æ˜¯GPT2çš„ä»å¤´é¢„è®­ç»ƒåŠå¾®è°ƒï¼Œåˆ©ç”¨è‡ªå·±é‡å¤´é¢„è®­ç»ƒçš„æ¨¡å‹åŠå¾®è°ƒçš„æ¨¡å‹è¿›è¡Œå¥å­ç”Ÿæˆä»»åŠ¡ã€‚\n",
    "\n",
    "æœ¬ç« èŠ‚çš„ç»“æ„ä¸ç¬¬å››èŠ‚ç±»ä¼¼ï¼Œ5.2èŠ‚æ˜¯é‡å¤´è®­ç»ƒæˆ–è€…åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸Šç»§ç»­fine-tune GPT2æ¨¡å‹ï¼›5.3èŠ‚æ˜¯ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒçš„GPT2æˆ–è€…æˆ‘ä»¬è‡ªå·±è®­ç»ƒçš„GPT2æ¨¡å‹è¿›è¡Œå¥å­ç”Ÿæˆï¼Œæ­¤åœºæ™¯å¯¹åº”äºæˆ‘ä»¬çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„çº¿ä¸ŠæœåŠ¡éƒ¨åˆ†ï¼›æœ¬èŠ‚æ•´ä½“æ€è·¯å’Œæµç¨‹æ˜¯å’Œç¬¬å››èŠ‚ç›¸ä¼¼çš„ã€‚\n",
    "\n",
    "æœ¬èŠ‚æ¶‰åŠåˆ°çš„é¢„è®­ç»ƒæ¨¡å‹æ˜¯GPT2ï¼Œæ¶‰åŠåˆ°çš„æ•°æ®é›†æ˜¯WikiText-2æ•°æ®é›†[9]ï¼Œç®€å•çš„è®²ï¼ŒWikiText-2æ˜¯ä»ç»´åŸºç™¾ç§‘ä¸ŠæŠ“å»çš„å¤§é‡å¥½çš„æ–‡æœ¬ï¼Œé‡å¾ˆå¤§ï¼Œå…·ä½“çš„å¯ä»¥è‡ªè¡ŒæŸ¥çœ‹é‡Œé¢çš„æ–‡æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 é¢„è®­ç»ƒçš„è¯¦ç»†è¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢çš„æ˜¯é‡å¤´å¼€å§‹é¢„è®­ç»ƒGPT2çš„ä¾‹å­ã€‚å¦‚æœæƒ³æ”¹ä¸ºåœ¨å·²æœ‰çš„GPT2æ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒï¼Œåªéœ€è¦åœ¨ä¸‹é¢çš„è¾“å…¥å‚æ•°input_argså¢åŠ ä¸€è¡Œä»£ç '--model_name_or_path', GPT2_MODEL_NAME_OR_PATHå³å¯ï¼Œæ›´å…·ä½“çœ‹ä¸‹é¢ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "import filelock\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–æ—¥å¿—\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒä¸Šé¢çš„è®­ç»ƒä¸€æ ·ï¼Œè¿™é‡Œæ˜¯æ¨¡å‹å‚æ•°ï¼Œä¸‹é¢æ˜¯æ•°æ®å‚æ•°\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: GPT \"},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    train_data_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
    "    )\n",
    "    eval_data_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    line_by_line: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
    "    )\n",
    "\n",
    "    mlm: bool = field(\n",
    "        default=False, metadata={\"help\": \"Train with masked-language modeling loss instead of language modeling.\"}\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
    "    )\n",
    "    plm_probability: float = field(\n",
    "        default=1 / 6,\n",
    "        metadata={\n",
    "            \"help\": \"Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling.\"\n",
    "        },\n",
    "    )\n",
    "    max_span_length: int = field(\n",
    "        default=5, metadata={\"help\": \"Maximum length of a span of masked tokens for permutation language modeling.\"}\n",
    "    )\n",
    "\n",
    "    block_size: int = field(\n",
    "        default=-1,\n",
    "        metadata={\n",
    "            \"help\": \"Optional input sequence length after tokenization.\"\n",
    "            \"The training dataset will be truncated in block of this size for training.\"\n",
    "            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config, tokenizer, model\n",
    "CONFIG_NAME = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "TOKENIZER_NAME = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "# ä»å¤´è¿›è¡Œé¢„è®­ç»ƒï¼Œä¸éœ€è¦æŒ‡å®šæ­¤å‚æ•°ï¼›ç»§ç»­finetuneï¼Œéœ€è¦æŒ‡å®šåŸå§‹çš„gpt2æ¨¡å‹æ‰€åœ¨ä½ç½®\n",
    "GPT2_MODEL_NAME_OR_PATH = '/dfsdata2/yucc1_data/models/huggingface/gpt2'\n",
    "# ç”Ÿæˆçš„æ¨¡å‹\n",
    "GPT2_OUTPUT_DIR = '/dfsdata2/yucc1_data/output/gpt2-train-new-model'\n",
    "# è®­ç»ƒé›†çš„ä½ç½®\n",
    "TRAIN_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.train.raw'\n",
    "# éªŒè¯é›†çš„ä½ç½®\n",
    "EVAL_DATA_FILE = '/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.test.raw'\n",
    "\n",
    "\n",
    "input_args = [\n",
    "    '--output_dir', GPT2_OUTPUT_DIR,\n",
    "    '--model_type', 'gpt2',\n",
    "    '--config_name', CONFIG_NAME,\n",
    "    '--tokenizer_name', TOKENIZER_NAME,\n",
    "    '--do_train',\n",
    "    '--train_data_file', TRAIN_DATA_FILE,\n",
    "    '--do_eval',\n",
    "    '--eval_data_file', EVAL_DATA_FILE,\n",
    "    '--block_size', '510',\n",
    "    '--save_steps', '5000',\n",
    "    '--num_train_epochs', '10.0',\n",
    "    '--overwrite_cache',\n",
    "    '--overwrite_output_dir',\n",
    "]\n",
    "\n",
    "# å¦‚æœä¸æ˜¯é‡å¤´é¢„è®­ç»ƒï¼Œè€Œæ˜¯åœ¨å·²æœ‰çš„GPT2é¢„è®­ç»ƒæ¨¡å‹ä¸Šå¾®è°ƒï¼Œåˆ™åŠ å…¥ä¸‹é¢çš„ä¸€è¡Œä»£ç ï¼ŒæŒ‡å®šå·²æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹å³å¯ã€‚\n",
    "# input_args.extend(['--model_name_or_path', GPT2_MODEL_NAME_OR_PATH])\n",
    "\n",
    "\n",
    "parser = transformers.HfArgumentParser((ModelArguments, DataTrainingArguments, transformers.TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(input_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(model_name_or_path=None, model_type='gpt2', config_name='/dfsdata2/yucc1_data/models/huggingface/gpt2', tokenizer_name='/dfsdata2/yucc1_data/models/huggingface/gpt2', cache_dir=None)\n",
      "\n",
      "DataTrainingArguments(train_data_file='/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.train.raw', eval_data_file='/dfsdata2/yucc1_data/datasets/wikitext-2-raw/wiki.test.raw', line_by_line=False, mlm=False, mlm_probability=0.15, plm_probability=0.16666666666666666, max_span_length=5, block_size=510, overwrite_cache=True)\n",
      "\n",
      "TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/gpt2-train-new-model', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul27_16-50-40_d1f9a32623b6', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹è¿™ä¸‰ç±»å‚æ•°\n",
    "print(f'{model_args}\\n\\n{data_args}\\n\\n{training_args}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¡®ä¿è¾“å…¥å‚æ•°æ­£ç¡®\n",
    "# do_evalä¸eval_data_fileä¸¤ä¸ªå‚æ•°ç»Ÿä¸€\n",
    "# è¾“å‡ºæ–‡ä»¶å¤¹åˆç†\n",
    "if data_args.eval_data_file is None and training_args.do_eval:\n",
    "    raise ValueError(\n",
    "        \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "        \"or remove the --do_eval argument.\"\n",
    "    )\n",
    "\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 16:50:44 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/27/2020 16:50:44 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "07/27/2020 16:50:44 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/dfsdata2/yucc1_data/output/gpt2-train-new-model', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul27_16-50-40_d1f9a32623b6', logging_first_step=False, logging_steps=500, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®æ—¥å¿—æ ¼å¼ï¼Œå¹¶è®°å½•æœ¬æ¬¡è®­ç»ƒçš„é‡è¦å‚æ•°\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    training_args.local_rank,\n",
    "    training_args.device,\n",
    "    training_args.n_gpu,\n",
    "    bool(training_args.local_rank != -1),\n",
    "    training_args.fp16,\n",
    ")\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "# è®¾ç½®seed\n",
    "transformers.set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 16:50:44 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/models/huggingface/gpt2/config.json\n",
      "07/27/2020 16:50:44 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/27/2020 16:50:44 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/models/huggingface/gpt2/config.json\n",
      "07/27/2020 16:50:44 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   Model name '/dfsdata2/yucc1_data/models/huggingface/gpt2' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/dfsdata2/yucc1_data/models/huggingface/gpt2' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/added_tokens.json. We won't load it.\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/special_tokens_map.json. We won't load it.\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/tokenizer_config.json. We won't load it.\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/tokenizer.json. We won't load it.\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/models/huggingface/gpt2/vocab.json\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/models/huggingface/gpt2/merges.txt\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 16:50:44 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 16:50:44 - INFO - __main__ -   Training new model from scratch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŠ è½½config\n",
    "if model_args.config_name:\n",
    "    config = transformers.AutoConfig.from_pretrained(model_args.config_name, cache_dir=model_args.cache_dir)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = transformers.AutoConfig.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
    "else:\n",
    "    config = CONFIG_MAPPING[model_args.model_type]()\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "# åŠ è½½tokenizer\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
    "        \"and load it from here, using --tokenizer_name\"\n",
    "    )\n",
    "\n",
    "if model_args.model_name_or_path:\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = transformers.AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# å¦‚æœtokenizerä¸modelçš„embeddingä¸ªæ•°ä¸åŒï¼Œè®¾ç½®ä¸ºç›¸åŒ\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¡éªŒå‚æ•°ï¼Œå¹¶ä¸”è®¾ç½®block_size\n",
    "if data_args.block_size <= 0:\n",
    "    data_args.block_size = tokenizer.max_len\n",
    "    # Our input block size will be the max possible for the model\n",
    "else:\n",
    "    data_args.block_size = min(data_args.block_size, tokenizer.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.dataset.Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, tokenizer: transformers.PreTrainedTokenizer, file_path: str, block_size: int, overwrite_cache=False,\n",
    "    ):\n",
    "        assert os.path.isfile(file_path)\n",
    "\n",
    "        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, \"cached_lm_{}_{}_{}\".format(tokenizer.__class__.__name__, str(block_size), filename,),\n",
    "        )\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with filelock.FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "\n",
    "                self.examples = []\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "\n",
    "                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
    "                    self.examples.append(\n",
    "                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])\n",
    "                    )\n",
    "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "                # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "                # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
    "\n",
    "\n",
    "class LineByLineTextDataset(torch.utils.data.dataset.Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, file_path: str, block_size: int):\n",
    "        assert os.path.isfile(file_path)\n",
    "        # Here, we do not cache the features, operating under the assumption\n",
    "        # that we will soon use fast multithreaded tokenizers from the\n",
    "        # `tokenizers` repo everywhere =)\n",
    "        logger.info(\"Creating features from dataset file at %s\", file_path)\n",
    "\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n",
    "        self.examples = batch_encoding[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•´ç†æ–‡æœ¬è‡³åŒç­‰é•¿åº¦\n",
    "@dataclass\n",
    "class DataCollatorForLanguageModeling:\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling.\n",
    "    - collates batches of tensors, honoring their tokenizer's pad_token\n",
    "    - preprocesses batches for masked language modeling\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "\n",
    "    def __call__(self, examples: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self._tensorize_batch(examples)\n",
    "        if self.mlm:\n",
    "            inputs, labels = self.mask_tokens(batch)\n",
    "            return {\"input_ids\": inputs, \"labels\": labels}\n",
    "        else:\n",
    "            labels = batch.clone().detach()\n",
    "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            return {\"input_ids\": batch, \"labels\": labels}\n",
    "\n",
    "    def _tensorize_batch(self, examples: List[torch.Tensor]) -> torch.Tensor:\n",
    "        length_of_first = examples[0].size(0)\n",
    "        are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "        if are_tensors_same_length:\n",
    "            return torch.stack(examples, dim=0)\n",
    "        else:\n",
    "            if self.tokenizer._pad_token is None:\n",
    "                raise ValueError(\n",
    "                    \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "                    f\" ({self.tokenizer.__class__.__name__}) does not have one.\"\n",
    "                )\n",
    "            return pad_sequence(examples, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "\n",
    "    def mask_tokens(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
    "            )\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å¾—dataset\n",
    "def get_dataset(args: DataTrainingArguments, tokenizer: transformers.PreTrainedTokenizer, evaluate=False):\n",
    "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
    "    if args.line_by_line:\n",
    "        return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)\n",
    "    else:\n",
    "        return TextDataset(\n",
    "            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size, overwrite_cache=args.overwrite_cache\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 16:50:47 - INFO - filelock -   Lock 139757830182000 acquired on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.train.raw.lock\n",
      "07/27/2020 16:50:47 - INFO - __main__ -   Creating features from dataset file at /dfsdata2/yucc1_data/datasets/wikitext-2-raw\n",
      "07/27/2020 16:51:02 - INFO - __main__ -   Saving features into cached file /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.train.raw [took 0.099 s]\n",
      "07/27/2020 16:51:02 - INFO - filelock -   Lock 139757830182000 released on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.train.raw.lock\n",
      "07/27/2020 16:51:02 - INFO - filelock -   Lock 139757858257216 acquired on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw.lock\n",
      "07/27/2020 16:51:02 - INFO - __main__ -   Creating features from dataset file at /dfsdata2/yucc1_data/datasets/wikitext-2-raw\n",
      "07/27/2020 16:51:04 - INFO - __main__ -   Saving features into cached file /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw [took 0.025 s]\n",
      "07/27/2020 16:51:04 - INFO - filelock -   Lock 139757858257216 released on /dfsdata2/yucc1_data/datasets/wikitext-2-raw/cached_lm_GPT2Tokenizer_510_wiki.test.raw.lock\n"
     ]
    }
   ],
   "source": [
    "# è·å–dataset\n",
    "train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
    "eval_dataset = get_dataset(data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 16:51:08 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 16:51:08 - INFO - transformers.trainer -   ***** Running training *****\n",
      "07/27/2020 16:51:08 - INFO - transformers.trainer -     Num examples = 4740\n",
      "07/27/2020 16:51:08 - INFO - transformers.trainer -     Num Epochs = 10\n",
      "07/27/2020 16:51:08 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "07/27/2020 16:51:08 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "07/27/2020 16:51:08 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "07/27/2020 16:51:08 - INFO - transformers.trainer -     Total optimization steps = 2970\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae40802bc2048ba96fad0d766cca74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4cd143fafb43988893bde858f299a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=297.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d068c5f0997147b0b75d1be75661ed5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=297.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 16:56:37 - INFO - transformers.trainer -   {'loss': 6.876390530586242, 'learning_rate': 4.158249158249159e-05, 'epoch': 1.6835016835016834, 'step': 500}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0463e1b4d9f451984cf61fe3bcc4a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=297.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2225972370714a7fa949e5c71211ca94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=297.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:02:04 - INFO - transformers.trainer -   {'loss': 6.018788367271424, 'learning_rate': 3.3164983164983165e-05, 'epoch': 3.3670033670033668, 'step': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b310ebe5544233a23c672e6db1b705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=297.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da600d5afd7e4d97adfb4589cb1dd014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=297.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:07:31 - INFO - transformers.trainer -   {'loss': 5.70868528842926, 'learning_rate': 2.474747474747475e-05, 'epoch': 5.05050505050505, 'step': 1500}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ea6163d508406ea8eb1c26d895d732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=297.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:12:58 - INFO - transformers.trainer -   {'loss': 5.508011105537414, 'learning_rate': 1.632996632996633e-05, 'epoch': 6.7340067340067336, 'step': 2000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328d5bd4f2c747b9917e02be4ff3f588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=297.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f20e3e3cfcf4417a08672985d72f9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=297.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:18:25 - INFO - transformers.trainer -   {'loss': 5.383153617858887, 'learning_rate': 7.912457912457913e-06, 'epoch': 8.417508417508417, 'step': 2500}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2117cb825bd94c7f999ec294b5be5d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=297.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:23:32 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "07/27/2020 17:23:32 - INFO - transformers.trainer -   Saving model checkpoint to /dfsdata2/yucc1_data/output/gpt2-train-new-model\n",
      "07/27/2020 17:23:32 - INFO - transformers.configuration_utils -   Configuration saved in /dfsdata2/yucc1_data/output/gpt2-train-new-model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:23:33 - INFO - transformers.modeling_utils -   Model weights saved in /dfsdata2/yucc1_data/output/gpt2-train-new-model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒ\n",
    "if training_args.do_train:\n",
    "    model_path = (\n",
    "        model_args.model_name_or_path\n",
    "        if model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path)\n",
    "        else None\n",
    "    )\n",
    "    trainer.train(model_path=model_path)\n",
    "    trainer.save_model()\n",
    "    # For convenience, we also re-save the tokenizer to the same directory,\n",
    "    # so that you can share your model easily on huggingface.co/models =)\n",
    "    if trainer.is_world_master():\n",
    "        tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:23:34 - INFO - __main__ -   *** Evaluate ***\n",
      "07/27/2020 17:23:34 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "07/27/2020 17:23:34 - INFO - transformers.trainer -     Num examples = 561\n",
      "07/27/2020 17:23:34 - INFO - transformers.trainer -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018b9ea8ac744d348a894a513c6c202e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=36.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:23:45 - INFO - transformers.trainer -   {'eval_loss': 5.696368217468262, 'epoch': 10.0, 'step': 2970}\n",
      "07/27/2020 17:23:45 - INFO - __main__ -   ***** Eval results *****\n",
      "07/27/2020 17:23:45 - INFO - __main__ -     perplexity = 297.7839481842482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°\n",
    "results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    eval_output = trainer.evaluate()\n",
    "\n",
    "    perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
    "    if trainer.is_world_master():\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    results.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dfsdata2/yucc1_data/output/gpt2-train-new-model\n",
      "â”œâ”€â”€ config.json\n",
      "â”œâ”€â”€ eval_results_lm.txt\n",
      "â”œâ”€â”€ merges.txt\n",
      "â”œâ”€â”€ pytorch_model.bin\n",
      "â”œâ”€â”€ special_tokens_map.json\n",
      "â”œâ”€â”€ tokenizer_config.json\n",
      "â”œâ”€â”€ training_args.bin\n",
      "â””â”€â”€ vocab.json\n",
      "\n",
      "0 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹ä¿å­˜æˆ‘ä»¬è‡ªå·±ä¸è®­ç»ƒçš„æ¨¡å‹æ–‡ä»¶\n",
    "!tree {GPT2_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexity': 297.7839481842482}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢å®Œæˆäº†GPT2çš„ä»å¤´å¼€å§‹é¢„è®­ç»ƒçš„è¿‡ç¨‹ï¼Œepochä¸º10çš„æ—¶å€™ï¼ŒéªŒè¯é›†çš„pplè¾¾åˆ°äº†297ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ–‡æœ¬è§£ç çš„ç­–ç•¥æœ‰ä¸¤ç±»[7]ï¼š\n",
    "\n",
    "- Argmax Decoding: ä¸»è¦åŒ…æ‹¬beam search, class-factored softmaxç­‰\n",
    "- Stochastic Decoding: ä¸»è¦åŒ…æ‹¬temperature sampling, top-k samplingç­‰\n",
    "\n",
    "åœ¨å¤§å¤šæ•°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå¤§å®¶éƒ½ç›´æ¥é‡‡ç”¨Argmax Decodingï¼Œæœ€å¸¸è§çš„å°±æ˜¯beam searchã€‚ä½†å¦‚æœæˆ‘ä»¬çš„vocabulary sizeè¾ƒå¤§ï¼Œè¾¾åˆ°äº†50kç”šè‡³150kï¼Œåœ¨softmaxå±‚çš„è¿ç®—é‡å°±ä¼šå˜å¾—éå¸¸å¤§ï¼Œå› ä¸ºè¦ç»è¿‡softmaxã€‚æœ‰ä¸¤ç§æ•ˆç‡æ›´é«˜çš„æ–¹æ³•ï¼ŒClass-factored Softmaxå’ŒPointer-generator Networkã€‚\n",
    "\n",
    "å®é™…ä¸ŠArgmax Decodingå¸¸å¸¸ä¼šå¯¼è‡´æ¨¡å‹ç”Ÿæˆé‡å¤çš„å¥å­ï¼Œå¦‚\"I don't know. I don't know. I don't know....\"ã€‚ä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆå°±æ˜¯åœ¨decodingè¿‡ç¨‹ä¸­å¼•å…¥randomnessï¼Œï¼Œä½†æ˜¯The Curious Case of Neural Text Degenerationè¿™ç¯‡è®ºæ–‡æŒ‡å‡ºï¼Œsampling from full vocabulary distributionç”Ÿæˆçš„å¥å­ä¼šéå¸¸çš„æ‚ä¹±æ— ç« ï¼Œå› ä¸ºå½“vocabulary sizeéå¸¸å¤§æ—¶ï¼Œæ¯ä¸ªè¯çš„probabilityéƒ½ä¼šå˜å¾—å¾ˆå°ï¼Œè¿™æ—¶æ¨¡å‹ä¼šæœ‰éå¸¸é«˜çš„å¯èƒ½æ€§sampleåˆ°ä¸€ä¸ªtail distributionä¸­çš„è¯ï¼Œä¸€æ—¦sampleåˆ°äº†tail distributionä¸­ä¸€ä¸ªå’Œå‰æ–‡éå¸¸ä¸ç›¸å…³çš„è¯ï¼Œå¾ˆæœ‰å¯èƒ½æ¥ä¸‹æ¥çš„è¯éƒ½å—å…¶å½±å“ï¼Œä½¿å¾—å¥å­è„±ç¦»åŸæœ¬çš„æ„æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦sampling from truncated vocabulary distributionï¼Œæ¯”è¾ƒå¸¸è§çš„ç®—æ³•ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç§ï¼š(1) Temperature Samplingï¼Œ(2) Top-k Samplingï¼Œ(3) Top-p Sampling.\n",
    "\n",
    "æœ¬èŠ‚ä»¥æœ€å¸¸è§çš„beam searchè§£ç æ–¹æ³•å’Œtop kï¼Œtop pçš„æŠ½æ ·æ–¹æ³•è§£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:30:20 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/output/gpt2-train-new-model/config.json\n",
      "07/27/2020 17:30:20 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/27/2020 17:30:20 - INFO - transformers.modeling_utils -   loading weights file /dfsdata2/yucc1_data/output/gpt2-train-new-model/pytorch_model.bin\n",
      "07/27/2020 17:30:24 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "07/27/2020 17:30:24 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /dfsdata2/yucc1_data/output/gpt2-train-new-model.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "07/27/2020 17:30:24 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/output/gpt2-train-new-model/config.json\n",
      "07/27/2020 17:30:24 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/27/2020 17:30:24 - INFO - transformers.tokenization_utils_base -   Model name '/dfsdata2/yucc1_data/output/gpt2-train-new-model' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/dfsdata2/yucc1_data/output/gpt2-train-new-model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/27/2020 17:30:24 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/output/gpt2-train-new-model/added_tokens.json. We won't load it.\n",
      "07/27/2020 17:30:24 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/output/gpt2-train-new-model/tokenizer.json. We won't load it.\n",
      "07/27/2020 17:30:24 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/output/gpt2-train-new-model/vocab.json\n",
      "07/27/2020 17:30:24 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/output/gpt2-train-new-model/merges.txt\n",
      "07/27/2020 17:30:24 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 17:30:24 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/output/gpt2-train-new-model/special_tokens_map.json\n",
      "07/27/2020 17:30:24 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/output/gpt2-train-new-model/tokenizer_config.json\n",
      "07/27/2020 17:30:24 - INFO - transformers.tokenization_utils_base -   loading file None\n"
     ]
    }
   ],
   "source": [
    "# æœ¬æ¨¡å‹ä¸ä¸‹é¢çš„æ¨¡å‹åŠ è½½ï¼ŒäºŒé€‰ä¸€å³å¯\n",
    "# ä½¿ç”¨åˆšåˆšé¢„è®­ç»ƒçš„æ¨¡å‹\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(GPT2_OUTPUT_DIR)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(GPT2_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:32:56 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/models/huggingface/gpt2/config.json\n",
      "07/27/2020 17:32:56 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/27/2020 17:32:56 - INFO - transformers.modeling_utils -   loading weights file /dfsdata2/yucc1_data/models/huggingface/gpt2/pytorch_model.bin\n",
      "07/27/2020 17:33:00 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "07/27/2020 17:33:00 - WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /dfsdata2/yucc1_data/models/huggingface/gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "07/27/2020 17:33:00 - INFO - transformers.configuration_utils -   loading configuration file /dfsdata2/yucc1_data/models/huggingface/gpt2/config.json\n",
      "07/27/2020 17:33:00 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   Model name '/dfsdata2/yucc1_data/models/huggingface/gpt2' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/dfsdata2/yucc1_data/models/huggingface/gpt2' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/added_tokens.json. We won't load it.\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/special_tokens_map.json. We won't load it.\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/tokenizer_config.json. We won't load it.\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   Didn't find file /dfsdata2/yucc1_data/models/huggingface/gpt2/tokenizer.json. We won't load it.\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/models/huggingface/gpt2/vocab.json\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   loading file /dfsdata2/yucc1_data/models/huggingface/gpt2/merges.txt\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "07/27/2020 17:33:00 - INFO - transformers.tokenization_utils_base -   loading file None\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨GPT2å®˜æ–¹çš„é¢„è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "# ä¸ä¸Šé¢è‡ªå·±çš„è®­ç»ƒæ¨¡å‹äºŒé€‰ä¸€åŠ è½½å³å¯\n",
    "# è‡ªå·±çš„é¢„è®­ç»ƒæ¨¡å‹å¹¶ä¸å¤Ÿå¥½ï¼Œå› ä¸ºepochä¸å¤Ÿå¤šï¼Œå»ºè®®ä½¿ç”¨GPT2å®˜æ–¹æ¨¡å‹è¿›è¡Œç”Ÿæˆ\n",
    "# å¦‚æœæƒ³ä½¿ç”¨è‡ªå·±çš„GPT2ï¼Œä¸€ä¸ªæ–¹æ³•æ˜¯å¾®è°ƒGPT2å®˜æ–¹æ¨¡å‹ï¼Œå¦ä¸€ä¸ªå¢åŠ epoch\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(GPT2_MODEL_NAME_OR_PATH)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(GPT2_MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:33:55 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decode 0: Today the weather is really nice and I am planning on Â going to go out on a limb and say that I am going to go out on a limb and say that I am going to go out on a limb and say that I am going to go out on a limb and say that I am going\n",
      "decode 1: Today the weather is really nice and I am planning on Â going to go out on a limb and say that I am going to go out on a limb and say that I am going to go out on a limb and say that I am going out on a limb and say that I am going out on\n",
      "decode 2: Today the weather is really nice and I am planning on Â going to go out on a limb and say that I am going to go out on a limb and say that I am going out on a limb and say that I am going out on a limb and say that I am going out on a limb\n",
      "decode 3: Today the weather is really nice and I am planning on Â going to go out on a limb and say that I am going to go out on a limb and say that I am going to go out on a limb and say that I am going to go out on a limb and that I am going to\n",
      "decode 4: Today the weather is really nice and I am planning on Â going to go out on a limb and say that I am going to go out on a limb and say I am going to go out on a limb and say I am going to go out on a limb and say I am going to go out\n"
     ]
    }
   ],
   "source": [
    "# è§£ç ç¤ºä¾‹ä¸€ï¼š\n",
    "# åˆå§‹æ–‡æœ¬\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "# è½¬æ¢æˆtensor\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# ä½¿ç”¨beam searchç”Ÿæˆ\n",
    "# max_lengthæ˜¯æœ€é•¿é•¿åº¦ï¼Œ50è¡¨ç¤ºåœ¨è¾“å…¥æ–‡æœ¬çš„åé¢æœ€å¤šç”Ÿæˆ50ä¸ªå­—\n",
    "beam_outputs = model.generate(input_ids=inputs,\n",
    "           max_length=50+inputs.shape[-1],\n",
    "           min_length=2+inputs.shape[-1],\n",
    "           num_beams=20,\n",
    "           num_return_sequences=5,)\n",
    "for i in range(5):\n",
    "    output_ids = beam_outputs[i].tolist()\n",
    "    text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(f'decode {i}: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:46:08 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "07/27/2020 17:46:11 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on xtracting in my bedroom again. So long to be here.\n",
      "\n",
      "I've been seeing this guy regularly for about 2 weeks now. He has a nice body that allows him to stand up pretty well. Not to mention he has great eyes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:46:13 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on Â a weekend soon. But with the rain forecast for the weekend and all my other activities all well and good, then maybe this summer is going to be a nice one. My sister is taking me home from a conference here this past week. And\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:46:14 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on Â going to ride this bike home on Wednesday to cover a big field trip!<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2020 17:46:16 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on Â going to do the weekend now.\n",
      "This year's weather had very low humidity. It is very cold and humid. I would recommend wearing clothing for it. Â For the weather to be perfect in the weather, I will wear high quality\n",
      "Today the weather is really nice and I am planning on icing it up on Tuesday. Good for summer in Oregon!\n",
      "\n",
      "I had 3 dogs that arrived at the zoo about three hours after the dogs had left so the cats left me a little bit early to catch the food. My dogs are fine,\n"
     ]
    }
   ],
   "source": [
    "# è§£ç ç¤ºä¾‹äºŒï¼š\n",
    "# åˆå§‹æ–‡æœ¬\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "# è½¬æ¢æˆtensor\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# ç»™å®štensorï¼Œç”Ÿæˆæ–°çš„tensor\n",
    "# æ­¤å¤„éœ€è¦æ³¨æ„çš„æ˜¯max_lengthåŒ…å«ç€å‰é¢çš„promptçš„é•¿åº¦ï¼Œä¹Ÿå°±æ˜¯å‰é¢çš„é•¿åº¦éå¸¸é•¿ï¼Œè¶…è¿‡250ï¼Œå°±æ— æ³•ç”Ÿæˆäº†\n",
    "# top_pæ˜¯æ¦‚ç‡ä»å¤§åˆ°å°æ’åˆ—ï¼Œæœ€å°çš„ä¸ªæ•°è¾¾åˆ°0.95ï¼Œåé¢çš„ç æ‰ä¸ä½¿ç”¨ï¼›top_kæ˜¯é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„60ä¸ªï¼Œåé¢çš„ç æ‰ä¸ä½¿ç”¨ï¼›\n",
    "# ç»¼åˆèµ·æ¥å°±æ˜¯æ¦‚ç‡ç›¸åŠ ä¸è¶…è¿‡0.95ä¸”æœ€å¤š60ä¸ªçš„å•è¯å»ç”Ÿæˆã€‚\n",
    "for i in range(5):\n",
    "    outputs = model.generate(inputs, \n",
    "                             max_length=50+inputs.shape[-1], \n",
    "                             do_sample=True, \n",
    "                             top_p=0.95, \n",
    "                             top_k=60)\n",
    "    generated = tokenizer.decode(outputs[0])\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å·²ç»å®Œæˆäº†ä¸¤ç§ä¸åŒè§£ç æ–¹å¼çš„ç»“æœï¼Œå¯ä»¥çœ‹åˆ°beam searchçš„ç»“æœç›¸å¯¹é‡å¤æ€§æ¯”è¾ƒå¤šã€‚\n",
    "\n",
    "åˆ°è¿™é‡Œï¼ŒåŸºäºGPT2çš„é‡å¤´é¢„è®­ç»ƒï¼Œå¹¶ä¸”è¿›è¡Œçº¿ä¸Šé¢„æµ‹å·²ç»å®Œæˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å…­èŠ‚ æ€»ç»“ã€æ€è€ƒä¸å±•æœ›\n",
    "\n",
    "ä¸Šé¢å·²ç»å®Œæˆäº†åŸºäºhuggingface/transformersåº“åœ¨æ–‡æœ¬åˆ†ç±»çš„è®­ç»ƒä¸éƒ¨ç½²ï¼Œæ–‡æœ¬ç”Ÿæˆçš„è¯­è¨€æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²ã€‚ä¸Šé¢çš„ä»£ç ä¹Ÿæ˜¯ä¸€ç§æ ‡å‡†çš„pytorchæ¨¡å‹èŒƒå¼ã€‚\n",
    "\n",
    "ä¸€äº›å…¶ä»–çš„æ€è€ƒï¼š\n",
    "\n",
    "1. å¼€æºä»£ç åŠå·¥å…·æŠ¥çš„ä¹¦å†™æ–¹å¼ã€‚setup.pyç”¨äºpipåŒ…å†™ä½œï¼›testsæ˜¯æµ‹è¯•æ ·ä¾‹ï¼›åŒåçš„æ–‡ä»¶å¤¹ä¸‹æ˜¯ä»£ç ã€‚\n",
    "2. æ‰€æœ‰è¾“å‡ºæ—¥å¿—éå¸¸é‡è¦ä¸”æœ‰ç”¨ã€‚\n",
    "3. typing, dataclasses, classmethodç­‰æ–¹æ³•çš„ä½¿ç”¨ã€‚\n",
    "4. å¼€æºåº“çš„å­¦ä¹ ã€‚ç¬¬ä¸€æ­¥ï¼Œçœ‹æ–‡æ¡£ï¼Œè·‘å®˜æ–¹examplesï¼šç¬¬äºŒæ­¥ï¼Œä¿®æ”¹ä»£ç ï¼Œä»¿å†™è‡ªå·±çš„ä»£ç ï¼›ç¬¬ä¸‰æ­¥ï¼Œé‡å¤å‰ä¸¤æ­¥ï¼›ç¬¬å››æ­¥ï¼Œçœ‹æºç ï¼Œä¿®æ”¹æºç ã€‚\n",
    "5. é€šè¿‡å­¦ä¹ æºç ï¼Œå­¦åˆ°äº†å¾ˆå¤šï¼Œå¸Œæœ›åç»­å¯¹æºç æ›´åŠ æ¸…æ¥šï¼Œä½¿ç”¨æ›´åŠ ç†Ÿç»ƒï¼›å¸Œæœ›èƒ½åœ¨å·¥ä½œã€å­¦ä¹ ã€ç§‘ç ”ä¸Šå¯¹è‡ªå·±ã€å¯¹å¤§å®¶æ›´æœ‰å¸®åŠ©ã€‚ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸ƒèŠ‚ ç›¸å…³ç½‘å€\n",
    "1. githubä»£ç ï¼šhttps://github.com/huggingface/transformers\n",
    "2. docè¯´æ˜ï¼šhttps://huggingface.co/transformers/index.html\n",
    "3. æ¨¡å‹ä¸‹è½½åœ°å€ï¼šhttps://huggingface.co/models\n",
    "4. åˆ©ç”¨transformerså¼€å‘çš„ä¸­æ–‡chitchatç”Ÿæˆï¼šhttps://github.com/yangjianxin1/GPT2-chitchat\n",
    "5. DialoGPTï¼šhttps://github.com/microsoft/DialoGPT\n",
    "6. glueæ•°æ®é›†ä»‹ç»ï¼šhttps://zhuanlan.zhihu.com/p/135283598\n",
    "7. æ–‡æœ¬è§£ç ç­–ç•¥ï¼šhttps://zhuanlan.zhihu.com/p/68383015\n",
    "8. Trainerè¯´æ˜ï¼šhttps://huggingface.co/transformers/main_classes/trainer.html\n",
    "9. WikiTextæ•°æ®é›†ï¼šhttps://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
